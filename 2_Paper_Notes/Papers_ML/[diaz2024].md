# A Flexible Vehicle Routing Reinforcement Learning Environment for the Reusability of Trained Agents

## Metadata
- **Link to PDF**: [[[diaz2024]_A_Flexible_Vehicle_Routing_Reinforcement_Learning_Environment_for_the_Reusability_of_Trained_Agents.pdf]]
- **Tags**:  
  #ML-ReinforcementLearning  
  #RL-PolicyGradient  
  #RL-ActorCritic  
  #ProximalPolicyOptimization  
  #CVRP  
  #DeterministicVRP  
  #MultiVehicleRouting  
  #Generalization  
  #TransferLearning  
  #CentralizedController  
- **Relevant**: true  
  - *Reasoning*: The paper addresses a deterministic CVRP using reinforcement learning. No stochastic or dynamic components are focused on; thus, it is relevant for the thesis scope.
- **Fit Score**: 9
- **State of the Art (SoA) Concepts**:
  - Reinforcement Learning for CVRP
  - Proximal Policy Optimization (PPO) approach
  - Flexible action space design
  - Model reusability / transfer across different VRP instances
  - Deterministic problem setting with capacity constraints
- **Performance Evaluation**: no  
  - They conduct an internal evaluation (20 nodes and 50 nodes) but do not use standard benchmark datasets.
- **Performance Evaluation Framework**: "-"

## Abstract
"Society keeps ever-growing, and new challenges keep arising for the artificial intelligence to solve, fueled by the desire to enable global operations and optimize current deep and machine learning methodologies. One of these disciplines is reinforcement learning, which has not seen much use outside of the context of video games until recent years, where different algorithms have eased it’s transition to more real-world scenarios, such as autonomous systems or self-driving vehicles. For this paper, the aim is to develop a reinforcement learning environment that can model and solve a complex vehicle routing problem with multiple constraints, with the addendum of allowing pre-trained models to solve use cases that differ from those that they were trained on."

## Summary
- **Overview**  
  - Proposes a reinforcement learning (RL) environment for the Capacitated Vehicle Routing Problem (CVRP) under deterministic conditions.  
  - Emphasizes reusability of trained RL models: an agent trained on one instance can be directly applied (with partial action masking) to another instance of different size and demands without full retraining.  
  - Uses Proximal Policy Optimization (PPO), an actor-critic policy gradient method.

- **Motivation**  
  - Last-mile logistics and increased customer demands require more efficient and adaptive routing solutions.  
  - RL has shown potential in sequential decision-making but is less explored for real-world VRP scenarios.  
  - The authors aim for an RL environment that accommodates different VRP instances and retains learned policies beyond a single problem configuration.

- **Methodology**  
  1. **Environment & Action Space**  
     - Models a multi-vehicle CVRP (demands at nodes, capacity constraints).  
     - Creates an *abstracted* discrete action space that combines which vehicle and which node to visit in one integer index (action).  
     - Permits varying numbers of vehicles/nodes by adjusting the dimensionality of the action space.  
  2. **Observation Space**  
     - Includes arrays indicating visited nodes, vehicle positions/loads, node demands, distances, and a reward signal.  
     - Distance traveled and remaining vehicle capacity are key components of the reward.  
  3. **Rewards**  
     - Inversely proportional to the distance traveled and emptiness of the vehicle.  
     - Encourages shorter routes and higher usage of vehicle capacity.  
  4. **Episode Termination Criteria**  
     - Primary: All nodes visited and vehicles return to the depot.  
     - Alternative approaches for large instances:  
       - **Increasing**: Only a fraction of nodes visited initially, fraction gradually increases to 100%.  
       - **Decreasing**: 100% of nodes required initially, percentage decreases if no solution is found.

- **Reusability / Transfer**  
  - To enable using a trained model in smaller or similar-sized cases, an *action mask* is implemented that invalidates actions corresponding to nonexistent vehicles or nodes in the new scenario.  
  - Demonstrates immediate (zero-shot) usability of a large-instance-trained agent on a smaller instance.

- **Experiments & Results**  
  1. **Setup**  
     - PPO algorithm from Stable Baselines 3, default hyperparameters.  
     - Two main problem scales:  
       - *Small*: 20 nodes, up to 7 vehicles  
       - *Medium*: 50 nodes, up to 13 vehicles  
     - Node coordinates and demands (5 to 15) are randomly generated.  
  2. **Episode Finishing Conditions**  
     - Compared standard full-solution-based episode termination vs. the “increasing” and “decreasing” partial solutions.  
     - *Increasing approach* led to faster reward improvement early in training but less route optimality.  
     - *Decreasing approach* gave the shortest total route distance and stable final reward.  
  3. **Model Reusability**  
     - Medium-instance-trained models tested on smaller 20-node instance.  
     - Consistently produced feasible, near-optimal routes with no further training.  
     - Demonstrates potential for a single RL policy to be reused across different CVRP problem variants.

- **Key Findings**  
  - Reinforcement learning can solve deterministic CVRPs of modest size (20–50 nodes) without requiring separate training for each instance.  
  - Masking extraneous actions helps the policy adapt quickly to smaller problem instances.  
  - Adjusting termination criteria can drastically impact learning speed and final solution quality.  
  - PPO’s stability suits multi-vehicle CVRPs, but the final solutions are not strictly optimal; some route patterns can be inefficient.

- **Strengths**  
  - Flexible, general environment design for CVRP with capacity constraints.  
  - Reusability/transfer of RL models across different instance sizes.  
  - Straightforward integration of a standard policy gradient RL library.

- **Limitations**  
  - No standard VRP benchmark (e.g., Christofides, Solomon) was used, limiting external validation or comparisons.  
  - Rewards focus mainly on distance and capacity usage; more elaborate cost functions could be explored.  
  - Medium instance remains relatively small (50 nodes) compared to large industrial routing contexts.

- **Relevance to Deterministic VRP Research**  
  - Proposes a method for cross-instance policy reuse, relevant to reducing training times in deterministic settings.  
  - Highlights an approach for capacity constraints in an RL framework.  
  - Illustrates the interplay between RL exploration-exploitation and the VRP’s combinatorial complexity.

- **Future Directions**  
  - Incorporation of more advanced or custom reward shaping to enhance solution optimality.  
  - Testing on classical VRP benchmarks to compare with exact or heuristic baselines.  
  - Investigation of alternative RL algorithms (e.g., A2C, SAC) and possible combination with heuristic methods for improved scaling.

