# LEARNING A LATENT SEARCH SPACE FOR ROUTING PROBLEMS USING VARIATIONAL AUTOENCODERS

## Metadata
- **Link to PDF**: [[[hottung2021]_Learning_a_latent_search_space_for_routing_problems_using_variational_autoencoders.pdf]]
- **Tags**:
  - #ML-SupervisedFromOptimal
  - #ML-AssistedHeuristics
  - #Autoencoder
  - #AttentionMechanismAM
  - #NeuralCombinatorialOptimization
  - #PopulationBasedEvolution
  - #Symmetry
  - #TSP
  - #CVRP
  - #LKH3Solver
- **Relevant**: true
- **Fit Score**: 9
- **State of the Art (SoA) Concepts**:
  - Conditional Variational Autoencoder (CVAE) for routing
  - Supervised learning from Concorde (TSP) and LKH3 (CVRP) solutions
  - Differential Evolution in latent space
  - Symmetry breaking in solution sequences
- **Performance Evaluation**: yes
- **Performance Evaluation Framework**:
  - Used the instance generator from Kool et al. (2019) for TSP/CVRP. 
  - Compared to known methods (Concorde, LKH3, etc.)

## Abstract
"Methods for automatically learning to solve routing problems are rapidly improving in performance. While most of these methods excel at generating solutions quickly, they are unable to effectively utilize longer run times because they lack a sophisticated search component. We present a learning-based optimization approach that allows a guided search in the distribution of high-quality solutions for a problem instance. More precisely, our method uses a conditional variational autoencoder that learns to map points in a continuous (latent) search space to high-quality, instance-specific routing problem solutions. The learned space can then be searched by any unconstrained continuous optimization method. We show that even using a standard differential evolution search strategy our approach is able to outperform existing purely machine learning based approaches."

## Summary
- **Overall Objective & Problem Addressed**  
  - Proposes a method (CVAE-Opt) to learn a *continuous latent search space* for routing problems.  
  - Focuses on deterministic TSP and CVRP, leveraging solutions to these problems to train a CVAE.  
  - Addresses the challenge that many ML-based methods for routing do not effectively exploit longer run times or incorporate a robust search mechanism.

- **Machine Learning Technique & Approach**  
  - Utilizes a *Conditional Variational Autoencoder (CVAE)* to encode/decode routes:
    - Encoder maps a given solution (route) to a continuous latent vector (for a specific instance).  
    - Decoder reconstructs a routing solution from a latent vector, conditioned on the instance features.  
  - Training is supervised from high-quality solutions generated by Concorde (for TSP) or LKH3 (for CVRP).  
  - Employs *symmetry breaking* to overcome multiple equivalent representations of the same route, improving training consistency.

- **Key Contributions**  
  1. **Learning a Latent Search Space**:  
     - A CVAE is trained so that points in the latent space correspond predominantly to *high-quality* solutions.  
     - Neighboring latent vectors produce solutions of similar quality, which is crucial for an effective continuous search.  

  2. **Guided Search with Differential Evolution**:  
     - Once trained, the latent space is explored with a domain-independent *differential evolution (DE)* algorithm.  
     - Decoder transforms candidate latent vectors back to discrete routing solutions, which are then evaluated.  
     - Standard DE operators navigate the latent space, effectively searching for improved solutions over time.  

  3. **Symmetry Breaking Technique**:  
     - Routing problems often have equivalent solutions represented by different sequences.  
     - They train the CVAE to reconstruct a *randomly chosen symmetrical solution* to the input route, enforcing that only the essential route structure is learned, not the sequence permutation.  

- **Methodology & Implementation Details**  
  - **Network Architecture**:  
    - Uses attention mechanisms to encode and decode solutions step by step.  
    - Shares parameters between encoder and decoder modules for efficiency.  
  - **Data & Training**:  
    - Trains on thousands of synthetic TSP/CVRP instances, with each instance solved offline by exact or heuristic solvers.  
    - Learns to map these high-quality solutions into an n-dimensional latent space (n=100 for TSP with 100 nodes, etc.).  
    - Key hyperparameter *β* (in the VAE loss) controls the KL divergence weight, strongly influencing final search performance.  

- **Experiments & Results**  
  1. **Quality of Learned Latent Space**  
     - Neighbors in latent space generally decode to solutions of comparable quality.  
     - Shows that random samples near a known good solution still yield near-optimal tours.  

  2. **Comparison with Other ML Methods**  
     - Benchmarks against *Attention Model (AM)* from Kool et al. (2019), *NeuRewriter* (Chen & Tian, 2019), and *GCN-BS* (Joshi et al., 2019).  
     - Demonstrates that CVAE-Opt, with DE searching in the learned space, outperforms these purely ML-based methods, especially given longer run times.  
     - For TSP and CVRP with 50 and 100 nodes, CVAE-Opt finds lower cost solutions than the competing ML methods.  

  3. **Comparison with Classic Solvers**  
     - Compares with *LKH3* (CVRP) and *Concorde* (TSP).  
     - While the exact or specialized solvers can still reach best-known or optimal solutions, CVAE-Opt can approach them closely.  
     - Performance gap remains small, especially on smaller instances (20–50 nodes).  

  4. **Generalization**  
     - A model trained on 100-node instances can solve 95- or 105-node problems with minimal loss in solution quality.  
     - Degradation occurs for significantly larger (125–150 nodes) or smaller (much less than 95 nodes) problem sizes, which is expected as the distribution shifts.

- **Strengths & Limitations**  
  - **Strengths**  
    - Does not require domain-specific heuristics: the method is domain-independent, only the training dataset (from external solver) is problem-specific.  
    - Learns a well-structured search space that supports various population-based or continuous optimization methods.  
    - Achieves strong performance on standard TSP/CVRP benchmarks, outperforming many purely ML-based approaches.  
  - **Limitations**  
    - Relies on *offline solution generation* using an existing solver (Concorde/LKH3) to produce training labels.  
    - Complexity of training can be high, especially for large-scale instances.  
    - Performance advantage is most pronounced given enough runtime for the DE search; for very short runtime, simpler ML models might suffice.

- **Relevance for Deterministic VRP**  
  - Demonstrates an innovative approach combining a learned latent representation with a standard evolutionary search for two common deterministic VRP variants (TSP and CVRP).  
  - Offers a path toward domain-agnostic ML-based heuristics, beneficial for large-scale and standard VRPs in industrial settings.  
  - Illustrates how supervised learning from high-quality solutions can effectively reduce the combinatorial space to a continuous domain, supporting improved performance over purely neural or purely hand-crafted heuristics.