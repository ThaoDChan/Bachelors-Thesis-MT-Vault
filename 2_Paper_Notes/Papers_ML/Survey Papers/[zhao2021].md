# A Hybrid of Deep Reinforcement Learning and Local Search for the Vehicle Routing Problems

## Metadata
- **Link to PDF**: [[[zhao2021]_A_Hybrid_of_Deep_Reinforcement_Learning_and_Local_Search_for_the_Vehicle_Routing_Problems.pdf]]
- **Tags**:  
  #ML-ReinforcementLearning  
  #RL-PolicyGradient  
  #RL-ActorCritic  
  #AttentionMechanismAM  
  #NeuralCombinatorialOptimization  
  #DeterministicVRP  
  #LocalSearchHeuristics  
  #ML-AssistedHeuristics  
  #HybridLocalSearchRL  
  #CVRP  
  #VRPTW  
  #LargeNeighborhoodSearch  
- **Relevant**: true  
- **Fit Score**: 10  
- **State of the Art (SoA) Concepts**:  
  - Deep reinforcement learning (actor-critic methods)  
  - Attention-based neural combinatorial optimization  
  - Local search (LNS, OR-Tools) hybridization  
  - VRPTW and standard VRP with capacity constraints  
- **Performance Evaluation**: yes  
- **Performance Evaluation Framework**: They primarily train on synthetic VRP data inspired by Solomon and Augerat distributions, and compare against known heuristic algorithms (e.g., LNS, OR-Tools).

## Abstract
"Different variants of the Vehicle Routing Problem (VRP) have been studied for decades. State-of-the-art methods based on local search have been developed for VRPs, while still facing problems of slow running time and poor solution quality in the case of large problem size. To overcome these problems, we first propose a novel deep reinforcement learning (DRL) model, which is composed of an actor, an adaptive critic and a routing simulator. The actor, based on the attention mechanism, is designed to generate routing strategies. The adaptive critic is devised to change the network structure adaptively, in order to accelerate the convergence rate and improve the solution quality during training. The routing simulator is developed to provide graph information and reward with the actor and adaptive critic. Then, we combine this DRL model with a local search method to further improve the solution quality. The output of the DRL model can serve as the initial solution for the following local search method, from where the final solution of the VRP is obtained. Tested on three datasets with customer points of 20, 50 and 100 respectively, experimental results demonstrate that the DRL model alone finds better solutions compared to construction algorithms and previous DRL approaches, while enabling a 5- to 40-fold speedup. We also observe that combining the DRL model with various local search methods yields excellent solutions at a superior generation speed, comparing to that of other initial solutions."

## Summary
- **Purpose and Problem Addressed**  
  - The paper targets the Vehicle Routing Problem (VRP) and its variant with time windows (VRPTW), both under deterministic conditions.  
  - It tackles the difficulty that local search heuristics face in large-scale VRPs: slow computation and risk of poor solutions if the initial solution is weak.  
  - The authors develop a deep reinforcement learning (DRL) model with an adaptive critic, further combined with local search to improve overall solution quality and speed.

- **Core Contributions**  
  1. **DRL Actor–Critic Model**:  
     - Uses an attention-based actor network inspired by sequence-to-sequence models for combinatorial optimization.  
     - Incorporates an **adaptive critic** that changes from a standard critic (simple feedforward or RNN-based regression) to a self-critic method once the actor starts delivering more consistent solutions. This helps reduce variance and accelerate convergence.  
  2. **Routing Simulator**:  
     - Provides an artificial environment for training, generating synthetic VRP or VRPTW instances based on distributions from well-known datasets (e.g., Solomon for VRPTW).  
     - Handles mask updates to ensure feasibility (e.g., capacity constraints, time-window feasibility).  
  3. **Two-Stage Framework**:  
     - Stage 1: DRL quickly produces a route (initial solution).  
     - Stage 2: A local search method (e.g., LNS or Google OR-Tools) refines this initial solution to achieve high solution quality.

- **Methodology**  
  - **Actor Network**:  
    - Embeds each node’s static and dynamic features into a high-dimensional representation.  
    - Uses an attention mechanism to select the next customer in a sequence, ensuring capacity/time-window feasibility through masks.  
    - Trained via policy gradient, sampling solutions from the learned distribution.  
  - **Adaptive Critic**:  
    - Initially, a simple neural network approximates the expected route cost (“baseline” for the policy gradient).  
    - After a certain number of training epochs, the critic network is replaced by a copy of the actor in greedy mode (self-critic) to compute the baseline.  
    - A t-test-based criterion ensures that the switch to self-critic happens only when the actor has improved sufficiently.  
    - This approach stabilizes and speeds up the training process compared to a purely normal critic or a purely self-critic from the start.  
  - **Local Search Hybrid**:  
    - The authors feed the DRL output solution (greedy or beam-search) as the initial solution into local search heuristics (Large Neighborhood Search or Google OR-Tools).  
    - The final solution often benefits from the synergy: DRL’s strong initial solutions reduce local search’s time to find near-optimal routes.

- **Experiments and Results**  
  - Evaluated on synthetic cohorts of 20, 50, and 100 customers, for both VRP (capacity constraints) and VRPTW.  
  - **Baseline Methods**:  
    - Construction heuristics (e.g., Clarke-Wright, Nearest Insertion)  
    - Other DRL method from Nazari et al. (pointer network with attention)  
    - Ant colony optimization (ACO)  
    - Hybrid approaches: local search with different initial solutions.  
  - **Key Findings**:  
    - The proposed DRL approach alone (without local search) outperforms conventional construction heuristics and the compared DRL baseline in terms of solution quality, while also being significantly faster.  
    - Integrating the DRL-generated solutions with local search yields best performance overall. For instance, local search with DRL-based initial routes consistently surpasses local search seeded by naive heuristics, in both solution quality and speed.  
    - Larger VRP instances (100+ nodes) benefit substantially from the DRL approach due to the quick inference that provides high-quality initial routes.  
    - The adaptive critic accelerates model convergence, achieving better final performance compared to a purely normal critic or purely self-critic.

- **Strengths and Weaknesses**  
  - **Strengths**:  
    - Proposes an **adaptive critic** mechanism that improves training efficiency and stability.  
    - Demonstrates a clear synergy: DRL + local search yields solutions competitive with advanced heuristics at lower runtime.  
    - Straightforward, end-to-end approach: no domain-specific handcrafted features are strictly required; the network learns from data.  
  - **Weaknesses / Limitations**:  
    - Reliance on extensive training time: the model must be trained offline on distributions of problems.  
    - Generalization to more diverse or real-world constraints (e.g., vehicle heterogeneity) is not deeply addressed.  
    - The approach is tested mostly on synthetic cohorts referencing standard benchmarks, not directly on the original large benchmark sets.

- **Relevance to Deterministic VRP**  
  - Highly relevant: all experiments assume deterministic demands and time windows (no stochastic elements).  
  - Addresses how to scale DRL to standard VRP and VRPTW under deterministic settings, aligning precisely with the thesis scope.

- **Potential Impact and Future Directions**  
  - Showcases how combining ML-based policies (especially DRL) with classic local search can yield near-optimal solutions for deterministic VRP variants at scale.  
  - Encourages future work on combining advanced RL architectures (e.g., multi-head attention) with sophisticated large neighborhood search operators or other heuristics.  
  - Suggests that, beyond VRPTW, the same adaptive critic concept might extend to more complex variants (e.g., multi-depot or multi-vehicle settings), as long as they remain deterministic.