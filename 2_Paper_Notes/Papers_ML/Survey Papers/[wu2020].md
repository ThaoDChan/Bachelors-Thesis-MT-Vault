# Learning Improvement Heuristics for Solving Routing Problems

## Metadata
-**Link to PDF**: [[[wu2020]_Learning_improvement_heuristics_for_solving_routing_problems.pdf]]
-**Tags**:  
#ML-ReinforcementLearning  
#RL-PolicyGradient  
#RL-ActorCritic  
#TransformerModels  
#NeuralCombinatorialOptimization  
#ML-AssistedHeuristics  
#LocalSearchHeuristics  
#TSP  
#CVRP  
#RealWorldData  
-**Relevant**: true  
-**Fit Score**: 10  
-**State of the Art (SoA) Concepts**:
- Actor-critic reinforcement learning for improvement heuristics  
- Attention-based / Transformer-like policy architecture  
- TSP and CVRP as primary VRP variants  
- Neighborhood search operators (2-opt, node swap, relocation)  
- Testing on real-world benchmark datasets (TSPlib and CVRPlib)
-**Performance Evaluation**: yes  
-**Performance Evaluation Framework**:  
- Synthetic TSP and CVRP instances (uniform distributions)  
- TSPlib and CVRPlib real-world datasets  

## Abstract
"Recent studies in using deep learning to solve routing problems focus on construction heuristics, the solutions of which are still far from optimality. Improvement heuristics have great potential to narrow this gap by iteratively refining a solution. However, classic improvement heuristics are all guided by hand-crafted rules which may limit their performance. In this paper, we propose a deep reinforcement learning framework to learn the improvement heuristics for routing problems. We design a self-attention based deep architecture as the policy network to guide the selection of next solution. We apply our method to two important routing problems, i.e. travelling salesman problem (TSP) and capacitated vehicle routing problem (CVRP). Experiments show that our method outperforms state-of-the-art deep learning based approaches. The learned policies are more effective than the traditional hand-crafted ones, and can be further enhanced by simple diversifying strategies. Moreover, the policies generalize well to different problem sizes, initial solutions and even real-world dataset."


## Summary
- **Context & Motivation**  
  - Traditional approaches to TSP/CVRP rely on either exact methods (limited by problem size), approximation, or hand-crafted heuristics.  
  - Recent deep learning (DL) methods focus mainly on construction heuristics, which build solutions from scratch, but often yield suboptimal routes.  
  - This paper aims to directly learn *improvement* heuristics, refining an existing solution iteratively to reach higher-quality solutions.

- **Core Contribution**  
  - Proposes a deep reinforcement learning (RL) framework to *select local improvements* (e.g., 2-opt operations) on a partial or complete route.  
  - Uses a self-attention (Transformer-like) architecture as the RL policy network, guiding which node pairs should be operated on next.  
  - Demonstrates that this learned improvement strategy closes the gap more effectively than both classic local search rules and existing DL-based construction methods.

- **Methodology**  
  1. **RL Formulation**:
     - Each complete route is treated as a *state* in an MDP; an action corresponds to choosing a pair of nodes for a local operator (e.g., 2-opt, node swap).  
     - The transition to the next state is deterministic (applying the local operator on the route).  
     - The reward is positive only if a strictly better solution is found, encouraging the policy to find improvements over time.  
     - An actor-critic approach is used to learn the policy (actor) and state-value function (critic).

  2. **Policy Network Architecture**:
     - **Self-attention encoder**: Embeds the route (sequence of nodes) while capturing positional and neighborhood information.  
     - **Compatibility layer**: Computes pairwise “compatibility” scores for each pair of nodes in the sequence, yielding a probability matrix of which pair to select.  
     - **Sampling**: The next node pair is sampled from the computed probabilities, ensuring exploration.

  3. **Implementation Details**:
     - Trained on randomly generated TSP and CVRP instances up to 100 nodes.  
     - Used a *step limit* (e.g., T=200 or T=360, depending on problem size) during training to manage the search horizon.  
     - Primarily uses 2-opt for local search improvement, though node swap and relocation are also considered.

- **Experimental Results**  
  1. **Comparison to State-of-the-Art**:
     - Compared against strong baselines: Concorde (exact), LKH3 (efficient classical heuristic), OR-Tools, and other DL-based methods (e.g., Attention Model (AM), NeuRewriter).  
     - On synthetic 20/50/100-node TSP and CVRP, the proposed approach achieves *significantly improved solution quality*, often reducing the optimality gap more than other DL-based methods.  
     - Still slightly behind specialized solvers like LKH3, but much closer to optimal than construction-based DL approaches.

  2. **Comparison to Conventional Rules**:
     - Shows that learned policies (actor-critic) outperform standard improvement heuristics like *first-improvement* or *best-improvement*, especially on larger instances.  

  3. **Diversity & Enhancements**:
     - Introducing simple diversity strategies (e.g., multiple runs of the policy with different random seeds) further improves final solution quality with minimal additional overhead.  

  4. **Generalization**:
     - Policies trained on smaller synthetic data generalize to larger instances and different initial solutions.  
     - Demonstrates ability to handle real-world benchmark instances (TSPlib and CVRPlib) reasonably well, outperforming other DL-based methods in many cases, though still less effective than specialized metaheuristics on certain test sets.

- **Key Findings & Relevance**  
  - Illustrates the potential of *reinforcement learning* to automate and enhance improvement heuristics for deterministic routing problems.  
  - Achieves better performance than both conventional local search rules and state-of-the-art deep construction heuristics, bridging the gap to specialized OR solvers.  
  - Suggests that advanced RL frameworks can adapt to different instance sizes and data distributions without heavy manual tuning.  
  - Relevant for deterministic VRP research that seeks data-driven methods capable of generalizing across varied problem instances while maintaining high solution quality.

- **Strengths, Weaknesses, & Limitations**  
  - **Strengths**:  
    - Self-attention effectively captures route structure; the approach cleanly integrates local search operators.  
    - Demonstrates strong improvement over baseline ML methods for TSP/CVRP.  
    - Good generalization to different instance sizes and real-world datasets.  
  - **Weaknesses & Limitations**:  
    - The policy is currently specialized to pairwise local operators; more complex operators might need more intricate policy architectures.  
    - Performance can still lag behind highly optimized solvers like LKH3 for large-scale or specialized instances.  
    - Training is relatively time-consuming for large problem sizes, and no advanced metaheuristic acceptance criterion (like simulated annealing or tabu search) is integrated.

- **Implications for Thesis**  
  - Demonstrates a solid blueprint for applying *RL-based improvement heuristics* specifically for deterministic TSP/CVRP.  
  - Offers insights on how to design neural networks that choose local search moves effectively, an approach that can be extended to other VRP variants.  
  - Underscores the importance of evaluating on both synthetic and real-world data, and comparing with classical OR algorithms.  
  - Potential future work includes hybridizing with metaheuristics (tabu search, simulated annealing), exploring multiple local operators, or adopting hierarchical RL.