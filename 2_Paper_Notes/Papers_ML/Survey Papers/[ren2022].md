# A Multi-Agent Reinforcement Learning Method With Route Recorders for Vehicle Routing in Supply Chain Management

## Metadata
- **Link to PDF**: [[[ren2022]_A_multi-agent_reinforcement_learning_method_with_route_recorders_for_vehicle_routing_in_supply_chain_management.pdf]]
- **Tags**:  
  #ML-ReinforcementLearning  
  #RL-PolicyGradient  
  #MultiAgentRLCoordination  
  #MultiAgentSystem  
  #PointerNetworks  
  #AttentionMechanismAM  
  #TransformerModels  
  #NeuralCombinatorialOptimization  
  #mVRPSTW  
  #TimeWindowConstraints  
  #MultiVehicleRouting  
  #DeterministicVRP  
- **Relevant**: true  
- **Fit Score**: 9  
- **State of the Art (SoA) Concepts**:  
  - Multi-agent deep reinforcement learning  
  - Attention-based encoder-decoder for vehicle routing  
  - GRU-based “route recorders” for capturing route history  
  - Time-window constraints in deterministic multi-vehicle routing  
- **Performance Evaluation**: no (uses own generated instances)  
- **Performance Evaluation Framework**: "-"  

## Abstract
"In the modern supply chain system, large-scale transportation tasks require the collaborative work of multiple vehicles to be completed on time. Over the past few decades, multi-vehicle route planning was mainly implemented by heuristic algorithms. However, these algorithms face the dilemma of long computation time. In recent years, some machine learning-based methods are also proposed for vehicle route planning, but the existing algorithms can hardly solve multi-vehicle time-sensitive problems. To overcome this problem, we propose a novel multi-agent reinforcement learning model, which optimizes the route length and the vehicle's arrival time simultaneously. The model is based on the encoder-decoder framework. The encoder mines the relationship between the customer nodes in the problem, and the decoder generates the route of each vehicle iteratively. Specially, we design multiple route recorders to extract the route history information of vehicles and realize the communication between them. In the inferring phase, the model could immediately generate routes for all vehicles in a new instance. To further improve the performance of the model, we devise a multi-sampling strategy and obtain the balance boundary between computation time and performance improvement. In addition, we propose a simulation-based vehicle configuration method to select the optimal number of vehicles in real applications. For validation, we conduct a series of experiments on problems with different customer amounts and various vehicle numbers. The results show that the proposed model outperforms other typical algorithms in both performance and calculation time."

## Summary
- **Problem & Motivation**  
  - Addresses a multi-vehicle routing problem with soft time windows (MVRPSTW) in supply chain contexts.  
  - Emphasizes both route distance minimization and timely arrivals (i.e., reducing early or late arrival penalties).  
  - Recognizes the need for multi-agent collaboration when several vehicles operate simultaneously.

- **Approach & Core Contribution**  
  - Proposes a **multi-agent reinforcement learning (MARL)** framework, extending the encoder-decoder model common in neural combinatorial optimization.  
  - Incorporates **route recorders** (GRU modules) for each vehicle to track individual route histories, enabling vehicles to “communicate” indirectly through shared global route information.  
  - Uses a **self-attention based encoder** (inspired by the Transformer architecture) to embed customer locations, demands, time windows, and other features.  
  - A specialized **decoder** selects customers for each vehicle step-by-step, guided by both local (vehicle-specific) and global (all-vehicles) historical states.

- **Methodological Details**  
  - The **encoder** employs multi-head self-attention to capture dependencies among customers, producing an embedding for each node (customer or depot) and a global context embedding.  
  - **Local route recorders**: Each vehicle’s GRU unit processes its last chosen node and remaining load, encoding route history into a hidden state that influences the next decision.  
  - **Global route recorder**: A separate GRU integrates the positions/loads of all vehicles, ensuring each agent has partial access to the global fleet’s status.  
  - Applies **policy gradient** with a learned baseline to train the model. The reward is the negative of the total route cost (distance plus soft time window penalties).  
  - Proposes a **multi-sampling strategy** (sampling different route sequences from the probabilistic policy) to improve solution quality at inference time, then selects the route with minimum cost among the samples.

- **Experiments & Results**  
  - Compares against classic heuristics (e.g., insertion, GA) and Google OR-Tools on randomly generated MVRPSTW instances with 20, 50, and 100 customers and varying fleet sizes.  
  - Demonstrates lower average cost and much faster computational times than baselines, especially when the problem scales to more customers.  
  - Investigates how vehicle fleet size affects performance: a balance is needed to avoid both late-penalty and early-penalty extremes. Proposes a “simulation-based approach” to determine the optimal number of vehicles in practice.  
  - Shows further cost improvements by applying the multi-sampling approach at the expense of increased runtime, observing diminishing returns beyond a certain sample size.

- **Strengths & Relevance**  
  - Integrates a multi-vehicle setting with time-window constraints using a single, end-to-end trainable neural network.  
  - Provides near-instant (once trained) solutions, significantly faster than iterative metaheuristics.  
  - Allows flexible trade-off between solution quality and inference runtime using the multi-sampling approach.  
  - Thoroughly tested on varied problem sizes, showing strong potential for real-time or large-scale deterministic logistics scenarios.

- **Limitations & Future Directions**  
  - The training data and problem instances are synthetically generated; applicability to real-world distributions requires further validation.  
  - Only linear penalty functions for time-window violations are considered. Nonlinear or more complex cost structures might require new penalty formulations or additional training adjustments.  
  - The method selects the number of vehicles via trial simulations; an automatic mechanism for fleet-size selection remains for future research.  
  - Does not benchmark against widely known standard datasets (e.g., Solomon), so external comparability is limited.

- **Overall Relevance to Deterministic VRP**  
  - Offers a novel multi-agent RL solution specifically tailored for deterministic multi-vehicle routing with time windows.  
  - Demonstrates the capability of attention-based encoder-decoders combined with GRU “route recorders” for improved cooperation among vehicles.  
  - Potentially applicable to other multi-vehicle routing settings in deterministic logistics operations, aligning well with advanced RL-based VRP research.