# A novel reinforcement learning-based hyper-heuristic for heterogeneous vehicle routing problem

## Metadata
- **Link to PDF**: [[[qin2021]_A_novel_reinforcement_learning-based_hyper-heuristic_for_heterogeneous_vehicle_routing_problem.pdf]]
- **Tags**:
  - #ML-ReinforcementLearning
  - #RL-PolicyGradient
  - #RL-ActorCritic
  - #ProximalPolicyOptimization
  - #PolicySearch
  - #HyperHeuristic
  - #HCVRP
  - #DeterministicVRP
  - #HeuristicOrExactBlends
  - #PopulationBasedEvolution
  - #LargeScaleVRP
- **Relevant**: true  
- **Fit Score**: 10  
- **State of the Art (SoA) Concepts**:
  - Heterogeneous Vehicle Routing Problem (HVRP)
  - Reinforcement learning with policy gradient (DPPO / PPO variant)
  - Hyper-heuristic framework combining multiple meta-heuristics
  - Mixed-Integer Linear Programming (MILP) baseline comparison
- **Performance Evaluation**: yes  
- **Performance Evaluation Framework**: Augerat’s VRP benchmark + random HVRP instances

## Abstract
“This study investigates a practical heterogeneous vehicle routing problem that involves routing a predefined fleet with different vehicle capacities to serve a series of customers to minimize the maximum routing time of vehicles. The comprehensive utilization of different types of vehicles brings great challenges for problem modeling and solving. In this study, a mixed-integer linear programming (MILP) model is formulated to obtain optimal solutions for small-scale problems. To further improve the quality of solutions for large-scale problems, this study develops a reinforcement learning-based hyper-heuristic, which introduces several meta-heuristics with different characteristics as low-level heuristics and policy-based reinforcement learning as a high-level selection strategy. Moreover, deep learning is used to extract hidden patterns within the collected data to combine the advantages of low-level heuristics better. Numerical experiments have been conducted and results indicate that the proposed algorithm exceeds the MILP solution on large-scale problems and outperforms the existing meta-heuristic algorithms.”

## Summary
- **Purpose & Context**
  - Addresses a variant of the Heterogeneous Vehicle Routing Problem (HVRP), where multiple vehicles with different capacities must serve all customers while minimizing the maximum route distance (makespan).
  - Recognizes that standard meta-heuristics and MILP methods can either become intractable (for large problems) or yield subpar solutions. Proposes a new hyper-heuristic framework controlled by reinforcement learning.

- **Main Contributions**
  1. **Mixed-Integer Linear Programming (MILP) Model**  
     - Formulates the HVRP mathematically to solve smaller instances optimally.
     - Serves as a baseline for exact comparison on small to medium problem sizes.
  2. **Reinforcement Learning-Based Hyper-Heuristic (RLHH)**
     - Constructs a high-level controller using a policy-based RL method (Distributed Proximal Policy Optimization, DPPO), combined with an actor-critic architecture.
     - Multiple meta-heuristics—artificial bee colony, ant colony optimization, cuckoo search, genetic algorithms, particle swarm optimization, simulated annealing—act as the low-level heuristics (LLHs).  
     - RL dynamically selects the most suitable LLH at each iteration, improving solution quality and convergence speed.
  3. **Deep Learning for Policy Approximation**
     - Uses neural networks to extract hidden patterns from solution states (population distribution, best-known solutions, etc.).
     - Actor-network outputs a probability distribution over which LLH to use next; critic-network estimates the value of the current search state to guide policy improvement.

- **Methodology & Approach**
  - **Hyper-Heuristic Structure**  
    - Maintains a population of solutions (encoded as permutations with “separator” markers for different vehicles).  
    - At each decision epoch, the reinforcement learning agent observes the state of solutions (best results, population diversity) and selects one meta-heuristic to run for a certain number of iterations.
    - Rewards are computed based on improvements in best-known solutions; no immediate reward is given per step, only at certain checkpoints.
  - **RL Training**  
    - Asynchronous multi-threading (A3C-like) speeds data collection for policy updates.
    - Clipped Surrogate Objective (PPO-style) used for stable policy gradient updates.
  - **Comparison**  
    - Benchmarked against each low-level meta-heuristic run independently and a classical hyper-heuristic from the literature.
    - Validated on Augerat’s VRP instances (traditional capacity constraints) and on newly generated heterogeneous fleets with different vehicle capacities.

- **Key Results & Findings**
  - For small instances, the MILP solver (CPLEX) obtains exact solutions, giving a reference point for method validation.
  - For larger instances (up to 100 customers, multiple vehicle types):
    - RLHH surpasses each individual meta-heuristic by a notable margin (~6% better on average).
    - Outperforms an existing hyper-heuristic baseline approach.
    - Achieves near or better solutions than MILP in mid-scale cases within limited runtime; for larger instances where MILP runs out of memory or time, RLHH yields feasible high-quality results.

- **Strengths & Insights**
  - Demonstrates a flexible and adaptive approach: the RL agent can exploit the advantages of each meta-heuristic and avoid their weaknesses.
  - Addresses large-scale HVRP effectively, suggesting a practical approach for real logistics where heterogeneous fleets are common.
  - Incorporates a deep learning model for policy approximation, which can adapt to diverse problem states.

- **Limitations**
  - Requires a training phase for the RL policy, which involves computational overhead and multiple generated instance scenarios.
  - Hyper-heuristic performance depends on the diversity and effectiveness of included low-level meta-heuristics.
  - No explicit mention of how it handles advanced constraints (e.g., time windows, environmental constraints).

- **Relevance to Deterministic VRP and ML**
  - Fits within deterministic settings: no stochastic elements or dynamic customer demands are assumed.
  - Illustrates how reinforcement learning can orchestrate meta-heuristics under a hyper-heuristic framework.
  - Potentially extensible to other deterministic VRP variants.

- **Conclusion & Future Work**
  - Offers a robust RL-based hyper-heuristic method that competes with or outperforms exact MILP (on small to medium instances) and surpasses single meta-heuristics for larger cases.
  - Future directions include:
    - More advanced neural network structures for improved policy learning.
    - Incorporating additional or hybrid heuristics in the LLH pool.
    - Applying the approach to more complex real-world constraints, still in a deterministic framework.