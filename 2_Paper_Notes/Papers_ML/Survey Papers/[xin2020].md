# Multi-Decoder Attention Model with Embedding Glimpse for Solving Vehicle Routing Problems

## Metadata
- **Link to PDF**: [[[xin2020]_Multi-decoder_attention_model_with_embedding_glimpse_for_solving_vehicle_routing_problems.pdf]]
- **Tags**:
  - #ML-ReinforcementLearning
  - #RL-PolicyGradient
  - #AttentionMechanismAM
  - #TransformerModels
  - #NeuralCombinatorialOptimization
  - #TSP
  - #CVRP
  - #SDVRP
  - #Orienteering
  - #PCTSP
  - #MultiDecoder
- **Relevant**: true
- **Fit Score**: 10
- **State of the Art (SoA) Concepts**:
  - Multi-Decoder attention-based RL approach
  - Embedding Glimpse layer for incremental re-embedding
  - Application to TSP, CVRP, SDVRP, OP, PCTSP
  - Customized beam search for diverse solutions
- **Performance Evaluation**: yes
- **Performance Evaluation Framework**: Randomly generated TSP/CVRP test sets (following Kool et al. 2019); comparison to Concorde (TSP) and LKH3 (CVRP)

## Abstract
“We present a novel deep reinforcement learning method to learn construction heuristics for vehicle routing problems. In specific, we propose a Multi-Decoder Attention Model (MDAM) to train multiple diverse policies, which effectively increases the chance of finding good solutions compared with existing methods that train only one policy. A customized beam search strategy is designed to fully exploit the diversity of MDAM. In addition, we propose an Embedding Glimpse layer in MDAM based on the recursive nature of construction, which can improve the quality of each policy by providing more informative embeddings. Extensive experiments on six different routing problems show that our method significantly outperforms the state-of-the-art deep learning based models.”

## Summary
- **Context and Motivation**
  - Addresses the combinatorial complexity of Vehicle Routing Problems (VRP), focusing primarily on the deterministic TSP, CVRP, and several other routing variants (SDVRP, Orienteering, PCTSP).
  - Observes that typical single-decoder RL models (e.g., existing attention-based methods) can struggle with solution diversity, potentially limiting the best attainable solutions.
  - Aims to develop a reinforcement learning (RL) construction approach that can generate multiple complementary solution patterns while still maintaining good solution quality.

- **Core Contribution**
  1. **Multi-Decoder Attention Model (MDAM)**  
     - Introduces multiple decoders (each parameterized independently) that learn distinct construction policies under a shared Transformer-based encoder.
     - Encourages diverse solution paths through a Kullback-Leibler divergence (KL) regularization term that maximizes the dissimilarity among decoders’ node-selection distributions.
  2. **Embedding Glimpse Layer**  
     - Recognizes that once nodes are visited, they become irrelevant for future decisions.
     - Implements a “partial” re-embedding mechanism at the top layer of the encoder to update node embeddings by masking out visited nodes, thus approximating fully dynamic embeddings with lower computational overhead.
     - Improves the quality of each policy’s decisions by focusing only on unvisited, relevant nodes in each step.
  3. **Customized Beam Search**  
     - Maintains a separate beam for each decoder to preserve diversity while searching, rather than merging all decoder expansions into a single beam.
     - Merges partial solutions within the same decoder beam whenever one is provably inferior (same visited set and capacity usage, but longer route).
     - Ensures an efficient and more diverse set of candidate solutions is retained in each beam, boosting the final best-solution quality.

- **Methodology**
  - **Transformer Encoder**:  
    - Uses a multi-layer self-attention architecture to produce node embeddings.  
    - The top layer (the “Embedding Glimpse,” or EG layer) is partially recomputed every few steps (a hyperparameter p) to mask out visited nodes.
  - **Multiple Decoders**:  
    - Each decoder has the same structure but unshared parameters.  
    - Each decoder outputs a probability distribution over the next node to visit, given the updated node embeddings from the EG layer.  
    - KL divergence is applied to the decoders’ next-node distributions at training time to ensure they learn distinct policies.
  - **Training Procedure**:  
    - Uses REINFORCE (policy gradient) with a baseline constructed via a greedy rollout of the best parameters from a previous epoch.  
    - The final loss has two components: the policy gradient (to minimize route length) and the negative KL term (to force decoders’ distribution dissimilarity).
  - **Inference**:  
    - Either run each decoder in a greedy mode (pick next node with highest probability) or use the specialized beam search across all decoders.  
    - The beam search merges inferior partial solutions within each decoder’s beam, preserving diversity across decoders.

- **Experiments**
  - **Routing Variants Tested**:  
    - TSP with 20, 50, 100 nodes.  
    - CVRP with 20, 50, 100 nodes.  
    - SDVRP, Orienteering Problem (OP), Prize Collecting TSP (PCTSP), and a Stochastic Prize Collecting TSP (SPCTSP) variant.  
    - Although SPCTSP involves some uncertainty in node prizes, the overarching framework is demonstrated mostly on deterministic routing tasks.
  - **Baselines**:
    - Compares with existing deep learning–based methods (Pointer Network, Transformer-based “Attention Model” from Kool et al. 2019, RL approaches from Nazari et al. 2018, NeuRewriter from Chen and Tian 2019).  
    - Also benchmarks against traditional solvers (Concorde for TSP, LKH3 for CVRP) and several heuristics (OR-tools, Gurobi with time limits, specialized local search methods).
  - **Key Findings**:
    1. **Higher Solution Quality**:  
       - MDAM outperforms single-decoder RL baselines (e.g., Kool et al.’s method) by returning significantly better average route lengths or costs.  
       - When using beam search with all decoders, the approach finds solutions that match or closely approach those of advanced heuristic solvers like LKH3 on CVRP or Concorde on TSP, with smaller gaps than prior RL models.
    2. **Improved Diversity**:  
       - Multiple decoders reduce duplication in generated solutions; each decoder tends to favor distinct partial solutions.  
       - Customized beam search further increases the probability of discovering better routes within a limited computation time.
    3. **Ablation Studies**:  
       - Show that removing either multi-decoder (MD) or Embedding Glimpse (EG) noticeably degrades solution quality, especially for larger instances.  
       - The EG layer contributes to more informed node embeddings, while the multi-decoder structure significantly boosts coverage of solution space.
    4. **Runtime Considerations**:  
       - The method can be slower than a single-decoder approach but remains polynomial, scaling reasonably up to 100–200 nodes.  
       - Against specialized TSP solvers (Concorde), it is faster for large-scale instance generation but not necessarily state-of-the-art in minimal run time. However, it offers strong scalability across multiple VRP variants.

- **Strengths and Contributions**
  - Proposes an elegant way to address the diversity problem in RL-based construction heuristics by training multiple specialized decoders jointly.  
  - Incorporates a lightweight partial re-embedding mechanism (the EG layer) to make node embeddings more context-dependent without re-encoding from scratch each step.  
  - Delivers high-quality solutions on multiple VRP variants, improving over prior single-policy methods and occasionally matching specialized solvers in short inference times.

- **Weaknesses / Limitations**
  - The approach introduces additional complexity in training and inference due to multiple decoders and specialized beam search.  
  - It partially addresses only static or deterministic variants effectively. For the stochastic variant SPCTSP, they note it can adapt but provide limited discussion.  
  - The method is tested on random data distributions and not always on classic VRP benchmark libraries (e.g., no mention of classic CVRPLIB for large scale).  
  - Merging partial solutions in beam search can still be computationally demanding, though mitigated by keeping smaller beam sizes per decoder.

- **Relevance to Deterministic VRP and ML-based Methods**
  - Highly relevant for a deterministic VRP-focused thesis, as it illustrates:
    - A strong instance of reinforcement learning with attention-based encoders for TSP and CVRP.  
    - A solution that outperforms several established neural methods and can compete with or approximate leading OR solvers in a fraction of the time on large sets of randomly generated problems.  
    - Techniques for ensuring solution diversity, which is pivotal in VRP solution search.

- **Conclusion and Future Directions**
  - MDAM shows that multiple construction policies, when trained jointly, can significantly improve solution coverage and final solution quality.  
  - The Embedding Glimpse layer concept demonstrates how partial re-embedding can enhance state representation without incurring huge computational overhead.  
  - Future work points to more flexible numbers of decoders, potential decoder collaboration (rather than independent decisions), and broader extension to other VRP variants or large-scale instances.