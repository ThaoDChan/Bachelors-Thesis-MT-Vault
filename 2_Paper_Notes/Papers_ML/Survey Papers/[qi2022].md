# QMOEA: A Q-learning-based multiobjective evolutionary algorithm for solving time-dependent green vehicle routing problems with time windows

## Metadata
- **Link to PDF**: [[[qi2022]_QMOEA-A_Q-learning-based_multiobjective_evolutionary_algorithm_for_solving_time-dependent_green_vehicle_routing_problems_with_time_windows.pdf]]
- **Tags**:
  - #ML-ReinforcementLearning
  - #RL-Qlearning
  - #ML-AssistedLocalSearch
  - #HybridLocalSearch
  - #GeneticAlgorithm
  - #PopulationBasedEvolution
  - #TimeWindowConstraints
  - #GreenVRP
  - #TDGVRPTW
  - #MultiObjective
  - #DeterministicVRP
- **Relevant**: true  
- **Fit Score**: 10  
- **State of the Art (SoA) Concepts**:
  - Q-learning as an adaptive mechanism for neighborhood operator selection
  - Multi-objective evolutionary algorithm (NSGA-II framework)
  - Time-dependent VRPTW with green (fuel/carbon) objectives
  - Hybrid initialization heuristics for route construction
- **Performance Evaluation**: yes  
- **Performance Evaluation Framework**: Solomon-based time-dependent benchmarks  

## Abstract
"The vehicle routing problem with time windows (VRPTW) is critical in operations research and combinatorial optimization. To address realistic requirements, this paper introduces a time-dependent green VRPTW (TDGVRPTW) model that captures changing speeds over time, road slopes, and vehicle load effects on fuel consumption and emissions, while also considering customer satisfaction. Three conflicting objectives—total vehicle duration, energy consumption, and customer satisfaction—are simultaneously optimized. The authors propose a Q-learning-based multiobjective evolutionary algorithm (QMOEA) to solve this model. Key innovations include a hybrid initialization method that generates high-quality and diverse solutions, two novel Pareto-front-driven crossover strategies for solution exploration, and a Q-learning agent that adaptively selects local search operators. Numerical experiments on time-dependent Solomon-based instances demonstrate the effectiveness of QMOEA in generating a well-distributed set of non-dominated solutions with competitive convergence and diversity."

## Summary
- **Problem & Motivation**  
  - Addresses a **time-dependent green VRPTW (TDGVRPTW)** variant, a deterministic scenario where vehicle speed depends on the time of day, road slope, and vehicle load.  
  - Simultaneously optimizes **three objectives**: (1) total vehicle duration (route time), (2) energy consumption (fuel + emissions), and (3) customer satisfaction (penalties for early/late arrival).

- **Methodology**  
  1. **Model Formulation**  
     - Adapts the VRPTW to a realistic, time-dependent context: Speed is modeled as a piecewise step function over daily time zones.  
     - Fuel consumption and carbon emission rates incorporate road slope and vehicle load.  
     - A soft time-window approach is used to quantify customer satisfaction with a piecewise function.
  2. **Proposed Algorithm (QMOEA)**  
     - Built upon the NSGA-II multiobjective evolutionary framework.  
     - **Hybrid Initialization**: Combines four heuristics (random, k-nearest neighbor, improved push-forward insertion, and earliest-preferred-time insertion) to provide initial population diversity and quality.  
     - **Pareto-based Crossover**: Two novel strategies (SCOX and CBOX) that learn from partial solutions in the approximate Pareto front; plus two known crossovers (IPTL and BCBRC) to preserve route structures.  
     - **Q-learning-based Local Search**: A reinforcement learning agent adaptively chooses among five neighborhood operators. The agent updates its Q table based on the improvement (reward) across multiple objectives, balancing the solution’s total distance, emissions, and satisfaction.  

- **Experiments & Results**  
  - Tested on **time-dependent Solomon-like benchmarks** with 25 to 100 customers.  
  - QMOEA outperforms or matches several multiobjective evolutionary baselines (MOEA/D, RPDNSGAII, etc.) in terms of convergence (HV indicator) and spread (IGD indicator).  
  - The hybrid initialization accelerates finding good initial solutions, and Pareto-based crossovers improve exploration.  
  - Q-learning significantly improves local search effectiveness, indicating better solution refinement than random or uniform neighborhood selection.

- **Key Contributions & Strengths**  
  - First multiobjective approach combining green VRP with time dependencies and explicit customer satisfaction modeling.  
  - Novel integration of a **Q-learning** module to dynamically select local search operators based on multiobjective reward signals.  
  - Hybrid initialization that ensures broad coverage of different route constructs from the start.  
  - Demonstrates thorough performance comparisons using recognized VRPTW metrics (HV, IGD) and established benchmarks.

- **Weaknesses & Limitations**  
  - Main focus on a deterministic, static environment; real-world logistics often have more dynamic or uncertain parameters.  
  - Implementation complexity is relatively high, combining multiple heuristics plus a reinforcement learning layer.  
  - Potential scalability issues as the number of customers grows beyond a few hundred, though partial solutions are shown for up to 100 customers.

- **Relevance to Deterministic VRP and ML**  
  - Highly relevant: Showcases how **reinforcement learning** can be integrated into an **evolutionary multiobjective** solver for a **deterministic** VRPTW extension.  
  - Provides insights on balancing conflicting objectives (e.g., environmental impact vs. route duration vs. service quality).  
  - Offers a robust framework for future expansions and direct comparisons with other hybrid ML-based VRP approaches.