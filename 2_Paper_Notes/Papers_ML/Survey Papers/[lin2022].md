# Deep Reinforcement Learning for the Electric Vehicle Routing Problem with Time Windows

## Metadata
- **Link to PDF**: [[[lin2022]_Deep_reinforcement_learning_for_the_electric_vehicle_routing_problem_with_time_windows.pdf]]
- **Tags**:
  - #ML-ReinforcementLearning
  - #RL-PolicyGradient
  - #PointerNetworks
  - #AttentionMechanismAM
  - #NeuralCombinatorialOptimization
  - #Structure2Vec
  - #BeamSearch
  - #EVRPTW
  - #GreenVRP
  - #DeterministicVRP
  - #LargeScaleVRP
- **Relevant**: true
- **Fit Score**: 10
- **State of the Art (SoA) Concepts**:
  - Deep RL with Pointer Networks, Attention Mechanism, and Graph Embeddings
  - EVRPTW (Electric Vehicle Routing Problem with Time Windows)
  - Policy Gradient Training using Rollout Baseline
  - Comparison with Metaheuristics (VNS/TS) and CPLEX
- **Performance Evaluation**: yes
- **Performance Evaluation Framework**: “-” (custom random-generated instances)

## Abstract
"The past decade has seen a rapid penetration of electric vehicles (EVs) as more and more logistics and transportation companies start to deploy electric vehicles (EVs) for service provision. In order to model the operations of a commercial EV fleet, we utilize the EV routing problem with time windows (EVRPTW). In this paper, we propose an end-to-end deep reinforcement learning framework to solve the EVRPTW. In particular, we develop an attention model incorporating the pointer network and a graph embedding layer to parameterize a stochastic policy for solving the EVRPTW. The model is then trained using policy gradient with rollout baseline. Our numerical studies show that the proposed model is able to efficiently solve EVRPTW instances of large sizes that are not solvable with current existing approaches."


## Summary
- **Context & Motivation**
  - The authors address a **deterministic** variant of the Vehicle Routing Problem (VRP) known as the **Electric Vehicle Routing Problem with Time Windows (EVRPTW)**.
  - They focus on realistic constraints for electric vehicles: battery capacity, potential charging at dedicated stations, and time windows for serving customers.
  - Traditional methods (metaheuristics like VNS/TS, or exact methods like branch-and-cut/price) often fail to efficiently scale for medium- to large-scale EVRPTW instances.

- **Core Contribution**
  - Proposes an **end-to-end Deep Reinforcement Learning (DRL)** approach to construct EVRPTW solutions directly, bypassing the need for explicit combinatorial searches.
  - Integrates a **pointer network** and **attention mechanism** with **graph embeddings** (Structure2Vec) to handle the EVRPTW’s local (e.g., node demands, time windows) and global (e.g., number of vehicles, battery state, planning horizon) information.
  - Trains the model via a **policy gradient** algorithm with a **rollout baseline** (REINFORCE-style) to avoid requiring optimal labels, enabling the method to scale without relying on large sets of ground-truth solutions.

- **Methodology**
  1. **Problem Formulation**  
     - Defines EVRPTW on a complete graph with three types of vertices: depot, customers, charging stations.  
     - Time windows are imposed on customers (strict earliest/latest service times), with stations and depot accessible at any time in the horizon.  
     - An EV with a given capacity (cargo and battery) routes through customers; it can detour to charging stations to recharge fully before continuing.  
     - The objective is to minimize total traveled distance subject to battery and time window feasibility.

  2. **Reinforcement Learning Framework**  
     - Treats the construction of a solution as a sequence of actions (selecting the next vertex).  
     - State includes: time, current vehicle battery level, demands remaining at each node, and the number of vehicles still available.  
     - A custom reward function encourages short travel distances while penalizing excessive station visits, battery violations, and fleet-size constraint breaches.  

  3. **Attention Model**  
     - Uses a **graph embedding** step (Structure2Vec) to incorporate both local node features and global state into an embedded representation.  
     - Applies a **pointer network**-style attention mechanism to compute probabilities over which vertex to visit next.  
     - Combines these embeddings with an LSTM decoder memory state to capture partial-route history.  

  4. **Training**  
     - Employs a **policy gradient (REINFORCE)** approach; model parameters are updated based on solution outcomes (negative total distance plus penalties).  
     - A **rollout baseline** is used to reduce gradient variance by comparing the agent’s reward to a baseline solution’s reward.  
     - Trains on randomly generated small EVRPTW instances (10 customers, 3 stations) to learn generalized patterns, then tests on larger instances.

  5. **Decoding Strategies**  
     - **Stochastic sampling**: at each step, picks the next node by sampling from the learned probability distribution (used during training and test for exploration).  
     - **Greedy decoding**: selects the highest-probability node at each step.  
     - **Beam search**: maintains a fixed “beam” of multiple partial solutions to explore different route continuations, then picks the best final solution.

- **Experimental Results**
  - The proposed RL model is compared against **CPLEX** (mixed-integer programming solver) and a **VNS/TS** metaheuristic from prior literature on EVRPTW.  
  - On **small instances** (5–10 customers), the RL approach yields feasible solutions in short time but is outperformed in optimality by CPLEX or VNS/TS (the RL approach has an ~8–12% gap).  
  - For **medium-sized instances** (20–30 customers), RL runs faster than VNS/TS by a factor of ~7–10 at the cost of somewhat larger optimality gaps (~20–40%).  
  - For **larger instances** (40+ customers), RL remains feasible and efficient (minutes of runtime), while VNS/TS or CPLEX frequently fails to converge in under the tested time limits. The RL approach can handle up to 100-customer instances in ~4–8 minutes.  
  - The authors conclude that the RL approach **scales** significantly better to large instances and can produce workable, if not strictly optimal, solutions rapidly for real-time operational needs.

- **Key Insights**
  - **Strengths**:  
    - Demonstrates that **deep RL** can handle a complex set of constraints (time windows, battery recharging).  
    - Good **scalability** to larger EVRPTW instances; existing methods often struggle beyond ~30 customers.  
    - Flexible enough to adapt to different objective or constraint settings by adjusting the reward function or masking.

  - **Limitations**:  
    - On smaller instances, the solutions from RL have notable gaps compared to exact or specialized metaheuristics.  
    - Charging decisions can be short-sighted: the model often charges late, missing better early-charge opportunities under full-charge assumptions.  
    - The approach uses random instance generation for training, which might not capture all real-world distribution complexities.

  - **Relevance to Deterministic VRP**:  
    - The problem is deterministic: demands, time windows, and battery consumption are known in advance.  
    - Showcases how a policy-based RL approach can tackle a specialized VRP variant (EVRPTW) without requiring dynamic or stochastic data.  
    - Offers a potential blueprint for applying advanced neural architectures (e.g., pointer networks + attention + graph embeddings) to other **deterministic** VRP subproblems.

- **Conclusion & Potential Extensions**
  - The RL-based solution efficiently handles **large-scale** EVRPTW in a purely deterministic setting.  
  - Future directions might include partial-charging decisions (relaxing the assumption of always full charging), combining the RL solution as a warm-start for exact or metaheuristic methods, or introducing multi-criteria objectives (e.g., carbon emissions, cost).  
  - The paper thus enriches the literature on machine learning–driven vehicle routing by demonstrating a practical RL pipeline that works effectively in the deterministic EVRPTW context.