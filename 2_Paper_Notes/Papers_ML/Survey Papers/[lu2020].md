# A LEARNING-BASED ITERATIVE METHOD FOR SOLVING VEHICLE ROUTING PROBLEMS

## Metadata
- **Link to PDF**: [[[lu2020]_A_Learning-based_Iterative_Method_for_Solving_Vehicle_Routing_Problems.pdf]]
- **Tags**:  
  #ML-ReinforcementLearning  
  #RL-PolicyGradient  
  #AttentionMechanismAM  
  #NeuralCombinatorialOptimization  
  #ML-AssistedHeuristics  
  #HyperHeuristic  
  #LocalSearchHeuristics  
  #CVRP  
  #DeterministicVRP  
  #LKH3Solver  
- **Relevant**: true 
- **Fit Score**: 10  
- **State of the Art (SoA) Concepts**:
  - Iterative local-search framework guided by reinforcement learning (Learn to Improve, L2I)  
  - Outperforms LKH3 on CVRP  
  - Policy gradient (REINFORCE) with specialized improvement operators  
  - Ensemble method of multiple policies  
- **Performance Evaluation**: yes  
- **Performance Evaluation Framework**: Randomly generated CVRP instances with 20, 50, and 100 customers, consistent with Nazari et al. (2018), Kool et al. (2019), Chen & Tian (2019)

## Abstract
"This paper is concerned with solving combinatorial optimization problems, in particular, the capacitated vehicle routing problems (CVRP). Classical Operations Research (OR) algorithms such as LKH3 (Helsgaun, 2017) are inefficient and difficult to scale to larger-size problems. Machine learning based approaches have recently shown to be promising, partly because of their efficiency (once trained, they can perform solving within minutes or even seconds). However, there is still a considerable gap between the quality of a machine learned solution and what OR methods can offer (e.g., on CVRP-100, the best result of learned solutions is between 16.10-16.80, significantly worse than LKH3's 15.65). In this paper, we present “Learn to Improve” (L2I), the first learning based approach for CVRP that is efficient in solving speed and at the same time outperforms OR methods. Starting with a random initial solution, L2I learns to iteratively refine the solution with an improvement operator, selected by a reinforcement learning based controller. The improvement operator is selected from a pool of powerful operators that are customized for routing problems. By combining the strengths of the two worlds, our approach achieves the new state-of-the-art results on CVRP, e.g., an average cost of 15.57 on CVRP-100."

## Summary
- **Context & Motivation**  
  - Addresses the classic Capacitated Vehicle Routing Problem (CVRP), where vehicles must serve customer demands without exceeding capacity.  
  - Traditional algorithms like the Lin-Kernighan-based LKH3 heuristic produce high-quality solutions but may be computationally expensive, especially for large-scale instances.  
  - Recent machine learning (ML) approaches have proven fast once trained, but often yield solutions that lag behind best-known OR methods on standard benchmarks (e.g., CVRP-100).  
  - The goal is to combine strengths of powerful routing heuristics (from operations research) with the adaptivity and speed of reinforcement learning methods.

- **Core Proposal**  
  - Introduces “Learn to Improve” (L2I), an iterative local search framework guided by a reinforcement learning (RL) policy.  
  - Unlike many end-to-end neural approaches that construct solutions from scratch, L2I **iteratively** refines an existing solution using a set of improvement (local search) and perturbation (diversification) operators.  
  - Maintains feasibility (no capacity violation) throughout the search, so any intermediate solution can be considered for the final answer.

- **Methodology**  
  1. **Initial Solution**:  
     - Generate a feasible solution randomly for the first improvement iteration.  
  2. **Operators**:  
     - **Improvement Operators**: A set of known local-search operators (intra-route like 2-opt or relocate within one route, and inter-route like cross or relocate between routes).  
     - **Perturbation Operators**: Larger changes that destroy/rebuild portions of the solution to escape deep local minima.  
  3. **Hierarchical Selection**:  
     - At each iteration, a threshold-based rule decides if the algorithm should keep “improving” (apply an improvement operator) or “perturb” (escape local minima).  
     - If no improvement occurs for a certain number of steps, a perturbation operator is triggered to diversify.  
  4. **Reinforcement Learning Controller**:  
     - The improvement operator is chosen via an RL-based controller that observes the current solution state (including routes, demands, historical actions, and their effects).  
     - A policy network uses an attention-based architecture (Transformer-style multi-head attention) plus feed-forward layers to map from solution state to probabilities over possible improvement actions.  
     - Trained with policy gradient (REINFORCE).  
     - Two reward designs explored: 
       - (a) immediate improvement (reward +1 if improved, -1 otherwise), 
       - (b) advantage-based comparing each iteration’s best solution to a baseline.  
  5. **Ensemble of Policies**:  
     - To boost diversity, multiple policies (differing in how many steps of history they encode) are trained in parallel.  
     - The best solution among these policies is selected at each iteration, yielding better overall quality and faster improvement.

- **Experimental Findings**  
  - **Dataset & Setup**:  
    - Synthetic instances of CVRP with N=20, 50, 100 customers generated randomly (uniform coordinates, demands from {1..9}, capacities 20, 30, 40).  
    - Comparisons with top classical and ML-based methods: LKH3, Google OR-Tools, RL-based pointer network approaches (Nazari et al., Kool et al., Chen & Tian).  
  - **Results**:  
    - The proposed L2I approach achieves an average cost of 15.57 for CVRP-100, surpassing LKH3’s reported average of 15.65.  
    - It also improves upon prior neural methods (16.1 or worse).  
    - Outperforms baseline ML methods across N=20, 50, 100, producing new state-of-the-art solutions.  
  - **Operator Usage**:  
    - After training, the RL policy shows a preference for a handful of effective operators (2-opt, small relocation, cross-exchange, etc.).  
    - Different policies specialize in slightly different operator sequences, motivating the ensemble strategy.  
  - **Perturbation Magnitude**:  
    - Too large a perturbation (e.g., randomly reordering all routes) can degrade solution quality, as improvement operators must spend many steps regaining lost ground.  
    - Focused or partial perturbation yields smoother transitions and better final results.

- **Strengths & Contributions**  
  - **State-of-the-Art**: First published machine learning method to surpass LKH3 on the well-studied CVRP-100 benchmark in terms of solution cost.  
  - **Hybrid ML-OR**: Successfully merges classical local search operators with an RL-based selection policy. This synergy uses domain-specific heuristics while learning dynamic operator sequencing.  
  - **Efficiency**: Once trained, L2I can tackle new instances significantly faster than running LKH3 from scratch, showing promise for large-scale or real-time routing scenarios.  
  - **General Framework**: The iterative “improve-perturb” design can potentially extend to other variants of VRP or combinatorial optimization problems.

- **Limitations & Future Work**  
  - **Generality**: The approach is specifically tailored to CVRP, though the authors suggest broadening it to VRP with time windows (VRPTW) and other routing variants.  
  - **Perturbation Rules**: Currently rule-based; might be replaced or further optimized by a learned perturbation policy.  
  - **Scalability & Transfer**: Training the RL controller relies on instances drawn from a given distribution; performance on out-of-distribution or extremely large instances may require additional steps or re-training.

- **Relevance to Deterministic VRP Research**  
  - Demonstrates how reinforcement learning can guide a local search framework for deterministic CVRP.  
  - Illustrates that a well-structured hybrid of machine learning and specialized OR operators can achieve top-tier solution quality and maintain fast inference times.  
  - Provides a blueprint for bridging the gap between purely data-driven methods and classical heuristics, supporting the thesis objective of summarizing advanced ML-based heuristics for deterministic VRP.