# A reinforcement learning approach for optimizing multiple traveling salesman problems over graphs

## Metadata

- **Link to PDF**: [[[hu2020]_A_reinforcement_learning_approach_for_optimizing_multiple_traveling_salesman_problems_over_graphs.pdf]]
- **Tags**:  
  #ML-ReinforcementLearning  
  #RL-PolicyGradient  
  #GraphNeuralNetworks  
  #AttentionMechanismAM  
  #MultiAgentRLCoordination  
  #MultiVehicleRouting  
  #DeterministicVRP  
  #LargeScaleVRP  
  #ML-AssistedHeuristics  
  #PolicySearch  

- **Relevant**: true  
- **Fit Score**: 9  
- **State of the Art (SoA) Concepts**:
  - Multi-agent RL architecture (distributed policy networks) for MTSP
  - Graph Neural Networks combined with attention-based policy
  - S-sample policy gradient technique to reduce variance
  - Comparison with integer linear programming and OR-Tools heuristics
- **Performance Evaluation**: yes  
- **Performance Evaluation Framework**: "-" (uses randomly generated datasets, no standard benchmark)  

## Abstract
"This paper proposes a learning-based approach to optimize the multiple traveling salesman problem (MTSP), which is one classic representative of cooperative combinatorial optimization problems. The MTSP is interesting to study, because the problem arises from numerous practical applications and efficient approaches to optimize the MTSP can potentially be adapted for other cooperative optimization problems. However, the MTSP is rarely researched in the deep learning domain because of certain difficulties, including the huge search space, the lack of training data that is labeled with optimal solutions and the lack of architectures that extract interactive behaviors among agents. This paper constructs an architecture consisting of a shared graph neural network and distributed policy networks to learn a common policy representation to produce near-optimal solutions for the MTSP. We use a reinforcement learning approach to train the model, overcoming the requirement data labeled with ground truth. We use a two-stage approach, where reinforcement learning is used to learn an allocation of agents to vertices, and a regular optimization method is used to solve the single-agent traveling salesman problems associated with each agent. We introduce a S-samples batch training method to reduce the variance of the gradient, improving the performance significantly. Experiments demonstrate our approach successfully learns a strong policy representation that outperforms integer linear programming and heuristic algorithms, especially on large scale problems."

## Summary
- **Context & Motivation**
  - The paper addresses a **deterministic multi-traveling salesman problem (MTSP)** setting, a variant of multi-vehicle route planning where each city is visited exactly once by a single agent.
  - MTSP arises in applications requiring multiple vehicles or salesmen to cooperatively visit sets of locations, analogous to multi-vehicle VRP.
  - Traditional approaches (integer linear programming, heuristics) struggle at large scale due to the combinatorial explosion.

- **Core Contribution**
  - Proposes a **reinforcement learning (RL)** approach to allocate cities among multiple agents and thereby decompose MTSP into smaller single-TSP subproblems.
  - Develops a **two-stage solution**:
    1. Train a **graph neural network (GNN)** + **distributed policy networks** that assign nodes to agents.
    2. Solve each agentâ€™s sub-TSP with a conventional TSP solver or heuristic (they use OR-Tools).
  - Employs a **S-sample batch approximation** in policy gradient to reduce variance, accelerate convergence, and avoid the need for labeled optimal solutions.

- **Methodology**
  - Uses a **Compositional Message Passing Neural Network (CMPNN)** to embed graph nodes into latent features.
  - Each agent has an attention-based embedding mechanism to capture its importance of the node set.
  - A final softmax step assigns each city to exactly one agent (probabilistic assignment).
  - The global reward is defined as the negative of the longest TSP route among all agents (the objective is to minimize this maximum route length).
  - Training relies on **REINFORCE**-style policy gradient, but with an S-sample strategy to reduce gradient noise.  
  - Performance is evaluated by comparing with Gurobi (ILP) and OR-Tools (meta-heuristics) under time constraints.

- **Key Results**
  - Outperforms ILP and heuristic methods (when both are time-limited), especially on larger instances with up to 1000 nodes.
  - Significantly faster inference compared to Gurobi or OR-Tools searching for an improved solution.
  - The learned policy generalizes well from smaller training instances (e.g., 50 or 100 nodes) to much larger test instances.
  - Ablation studies confirm both the graph embedding and the distributed policy network are essential to the final performance.

- **Strengths & Limitations**
  - **Strengths**:  
    - Demonstrates an effective RL-based pipeline that scales to large MTSP instances.  
    - Leverages GNN-based embeddings to handle variable graph sizes.  
    - Achieves near real-time solution generation after training.
  - **Limitations**:  
    - Limited to the MTSP objective of minimizing the maximum route length; no capacity or time-window constraints are considered.  
    - Training requires iterative calls to a TSP solver for rewards, which can be expensive for extremely large datasets.

- **Relevance for Deterministic VRP**
  - The MTSP is effectively a multi-vehicle VRP with no capacity constraints, fully deterministic.
  - Illustrates a successful combination of RL policy learning + GNN embedding + standard solvers for sub-routes.
  - Offers insight into how multi-agent RL and graph neural networks can be leveraged for partitioning large route-planning tasks.