## An Overview and Experimental Study of LearningBased Optimization Algorithms for the Vehicle Routing Problem

Bingjie Li, Guohua Wu, Yongming He, Mingfeng Fan, and Witold Pedrycz, Fellow, IEEE

AbstractThe  vehicle  routing  problem  (VRP)  is  a  typical discrete  combinatorial  optimization  problem,  and  many  models and  algorithms  have  been  proposed  to  solve  the  VRP  and  its variants. Although existing approaches have contributed significantly  to  the  development  of  this  field,  these  approaches either are limited in problem size or need manual intervention in choosing  parameters.  To  solve  these  difficulties,  many  studies have considered learning-based optimization (LBO) algorithms to solve  the  VRP.  This  paper  reviews  recent  advances  in  this  field and divides relevant approaches into end-to-end approaches and step-by-step  approaches.  We  performed  a  statistical  analysis  of the  reviewed  articles  from  various  aspects  and  designed  three experiments  to  evaluate  the  performance  of  four  representative LBO  algorithms.  Finally,  we  conclude  the  applicable  types  of problems for different LBO algorithms and suggest directions in which researchers can improve LBO algorithms.

significant  practicality  in  the  real  world.  VRP  variants  have appeared  in  many  fields  [2],  [3],  particularly  in  the  logistic industry  that  is flourishing  with  the  development  of  the globalization. Effective routing optimization can save a lot of cost (generally ranging from 5 % to 20 % ) [4], which is of great significance for improving distribution efficiency [15]-[8] and increasing economic benefits of enterprises [9]-[11]. The VRP is still continuously attracting attention from researchers [12], [13]. However, it is still a challenging optimization task due  to the NP-hard  characteristic [14] and its different complex variants.

Index TermsEnd-to-end approaches, learning-based optimization (LBO)  algorithms,  reinforcement  learning,  step-by-step  approaches, vehicle routing problem (VRP).

## I.  Introduction

Ttransportation  and  operations  research  areas  [1].  The HE VRP is one of the most widely researched problems in canonical  VRP  has  a  simple  structure  and  can  be  seen  as  a basic discrete combinatorial optimization problem. Many optimization problems can be transformed into the form of the VRP.  Hence,  the  VRP  can  be  used  to  demonstrate  the optimization  performance  of  different  algorithms,  which  can help  to  design  efficient  algorithms.  Moreover,  the  VRP  has

Manuscript received November  12, 2021; revised January 1, 2022; accepted February 16, 2022. Recommended by Associate Editor Long Chen. (Corresponding author: Guohua Wu.)

Citation: B. J. Li, G. H. Wu, Y. M. He, M. F. Fan, and W. Pedrycz, 'An overview and experimental  study  of  learning-based  optimization  algorithms for the vehicle routing problem,' IEEE/CAA J. Autom. Sinica ,  vol.  9,  no.  7, pp. 1115-1138, Jul. 2022.

- B.  J.  Li,  G.  H.  Wu,  and  M.  F.  Fan  are  with  the  School  of  Traffic  and Transportation  Engineering,  Central  South  University,  Changsha  410075, China  (e-mail:  csulbj@csu.edu.cn;  guohuawu@csu.edu.cn;  mingfan@csu. edu.cn).
- Y. M. He is with the College of Systems Engineering, National University of Defense Technology, Changsha 410073, China (e-mail: heyongming10@ hotmail.com).
- W.  Pedrycz is with the Department of Electrical and Computer Engineering,  University  of  Alberta,  Edmonton,  AB  T6G  2V4,  Canada,  the Department of Electrical and Computer Engineering, Faculty of Engineering, King Abdulaziz University,  Jeddah  21589,  Saudi  Arabia,  and  also  with  the Systems  Research  Institute,  Polish  Academy  of  Sciences,  Warsaw  01447, Poland (e-mail: wpedrycz@ualberta.ca).

Color  versions  of  one  or  more  of  the  figures  in  this  paper  are  available online at http://ieeexplore.ieee.org.

Digital Object Identifier 10.1109/JAS.2022.105677

Optimization algorithms for solving the VRP can be crudely divided  into three categories: exact, heuristic, and LBO algorithms. Fig. 1 shows the overview of referred algorithms. Heuristic algorithms and  exact algorithms are traditional algorithms for combinatorial optimization problems, and much of the literature  has  reviewed  their  applications  in  the VRP  [15].  Mańdziuk  [16]  reviewed  papers  using  heuristic algorithms to solve the VRP  between  2015  and 2017. Adewumi  and  Adeleke  [17]  emphasized  recent  advances  of traditional algorithms in 2018, while Dixit et al. [18] reviewed some of the recent advancements of meta-heuristic techniques in solving the VRP with time windows (VRPTW). Exact and heuristic  algorithms  have  made  significant  progress  in  the VRP,  but  the  recent  research  trend  has  been  to  design  a flexible  model  that  can  solve  the  VRP  with  large  scale  and complex  constraints  more  quickly.  Exact  algorithms  need adequate time to get an optimal solution when solving a VRP with a large scale [19]. The optimality of heuristic algorithms cannot  be  guaranteed  and  their  computational  complexity  is not satisfactory [20]. What's  more, these two kinds of algorithms  usually  need  a  specific  design  for  solving  the concrete  problem.  In  recent  years,  the  LBO  algorithms  have been applied to different combinatorial optimization problems [21], [22], and many remarkable research breakthroughs have been  achieved.  Hence,  the  LBO  algorithms  have  attracted significant attention in the field of the VRP.

The LBO algorithms can learn a model from training sets to obtain optimization  strategies [23], [24], which  could automatically  produce  solutions  of  online  tasks  by  end-toend  or  step-by-step  approaches.  In  end-to-end  approaches, the  model  is  trained  to  approximate  a  mapping  function between the input and the solution, and the model can directly output  a  feasible  solution  when  given  an  unknown  task  in application. While step-by-step approaches learn optimization

Fig. 1.     Overall classification of the referred algorithms of the VRP.

![Image](image_000000_bb3676f47eefbd83fe8f2940861d4715ad01b4385ecf6cd1fb345038e2688e67.png)

strategies that could iteratively improve a solution rather than outputting  a  final  solution  directly.  Both  approaches  have strong learning ability and generalization. They can overcome the deficiencies of the tedious parameter tuning of exact and heuristic  algorithms  and  rapidly  solve  online  instances  with the advantage of offline training. However, the LBO algorithms  still  have  some  technological  bottlenecks  to  be settled, such as training data limitation, generality limitation, and so on.

Overall, although the application of the LBO algorithms to the  VRP  have  become  popular  and  significant  achievements have been made [25], [26] in recent years, up to now there are no comprehensive assessment experiments and in-depth analyses on the characteristics of different LBO algorithms in solving  the  VRP.  Therefore,  we  aim  to  present  an  elaborate overview and experimental study of related works to fill the gap in this field. The contributions of the paper are mainly on the following aspects:

- 1)  The  paper  briefly  introduces  the  applications  of  LBO algorithms to the VRP to aid beginners in understanding the development of this field.
- 2) The paper discusses the advantages and disadvantages of different LBO algorithms based on extensive experiments on different datasets.
- 3)  The  paper  suggests  promising  research  trends  on  using LBO algorithms to solve the VRP in the future.

The  remainder  of  this  paper  is  structured  as  follows.  In Section II, we introduce the background of the VRP, including the mathematical model and classical optimization algorithms. In Section III, we review the related literatures and provide a summative  analysis  of  the  references.  In  Section  IV,  the experimental  comparisons  among  different  algorithms  are presented.  The  final  section  summarizes  the  full  text  and suggests several research trends of using LBO algorithms to solve the VRP.

## II.  Background of the VRP

As  routing optimization problems become  increasingly crucial  in  industrial  engineering,  logistics,  etc,  a  series  of research  breakthroughs  have  been  achieved  in  the  VRP  and many  algorithms  have  been  proposed.  In  this  section,  we briefly introduce the background of the VRP.

## A.  Variants of the VRP

The  mathematical  model  of  the  VRP  is  first  proposed  by Dantzig and Ramser [27] in 1959. A few years later, Clarke and  Wright  [28]  proposed  an  effective  greedy  algorithm  to solve the VRP,  which also launched a research boom involving the VRP. The VRP can be defined as follows: for a series  of  nodes  with  different  demands,  the  aim  is  to  plan routes with the lowest total cost under various side constraints [29],  [30]  for  vehicles  to  serve  these  nodes  sequentially according to the planning routes. With its wide applications in the  real  world,  the  VRP  must  consider  more  constraints, which  results  in the derivation of many  variants. Fig. 2 presents the hierarchy of VRP variants.

The  basic  version  of  the  VRP  is  the  capacitated  vehicle routing  problem  (CVRP)  [16],  [31].  In  the  CVRP,  each customer is served only once and the sum of goods carried by a  vehicle  should  not  exceed  its  capacity  [32].  This  type  of problem  has  the  following  characteristics:  1)  vehicles  leave from the depot  and  return  to  it;  2)  vehicles  are  permitted  to visit  each  customer  on  a  set  of  routes  exactly  once;  3)  each customer has a non-negative demand of delivery. Fig. 3 is the schematic diagram of the CVRP.

## B.  Mathematical Model of the VRP

V = N [f g 0 x k i ; j x k i ; j u k i qj The  VRP  can  be  described  by  an  integer  programming model [19], and we mainly present the mathematical model of the CVRP  in this subsection. According to [33], the mathematical  model  of  the CVRP  can  be  expressed  as follows, where N represents the set of customers, the number 0 represents the depot, K represents the number of vehicles, A represents  the  set  of  arcs  and V represents  the  set  of  nodes, with .  We define a binary decision variable to be 1 if the pair of nodes   and   are adjacent in the route of the i j vehicle k ;  otherwise, equals  0.  Each  vehicle's  maximum capacity is C . represents the surplus capacity of the vehicle k after it serves the customer  , and i indicates the demand of customer  . j

$$\underset { \underset { \underset { \underset { \text{$f$d}$} } { \omega } } { \omega } } { \circ } \cdots \cdots \cdots \cdots \cdots \cdots \\ \underset { \underset { \text{$f$d}$} } { \circ } \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \colon } { \circ } \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdelta } { \circ } \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cd� \colon } { \circ } \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdjs \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \ddots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \\ \underset { \text{$f$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$f$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$f$f$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$e$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$e$f$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$l$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$l$e$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$f$e$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$e$e$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$l$f$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$g$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$g$f$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$D$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$d$$$

$$\stackrel { \nu } { \text{id} } ^ { \nu } & & \text{s.t.} \sum _ { k \in K } \sum _ { j \in V, j \neq i } x _ { i, j } ^ { k } = 1, i \in N & & ( 2 )$$

Fig. 2.     Different variants of the VRP.

![Image](image_000001_5395dc291beed2065efdcfc7112e660311e221a916d5a6c81d61222f0874f1b7.png)

Fig. 3.     Schematic diagram of the CVRP.

![Image](image_000002_026c6fa2ccddf6a909dea32c817852ada6fa44158c70b6a49a17c24c85b17047.png)

$$\begin{array} { c }. 3. & \text{Schematic diagram of the CVR} \, \\ \sum _ { k \in K } \sum _ { i \in V, i \neq j } x _ { i, j } ^ { k } = 1, j \in N \\ \longrightarrow \, \longrightarrow \, \end{array}$$

x k i ; j maximum capacity C . Constraint (8) is used to ensure decision variable is a binary.

## C.  Traditional Optimization Algorithms for the VRP

Traditional  optimization  algorithms  for  solving  the  VRP include exact algorithms and heuristic algorithms. Fisher [34] divided  the  development  of  traditional  algorithms  applied  to the  VRP  into  three  stages.  The  first  stage  is  from  1959  to 1970, and simple heuristic algorithms were mainly used, such as the local heuristic and greedy algorithms; the second stage is  from  1970 to 1980, and this stage primarily applied exact algorithms;  the  third  stage  is  after  1980  and  meta-heuristic algorithms  began  to  be  applied  to  VRPs.  These  traditional algorithms have been successfully used to solve different VRP variants [35].

1)  Exact  Algorithms:

According  to  [36],  exact  algorithms for  solving  the  VRP  can  be  divided  into  three  categories:

direct  tree  search  (such  as  the  branch-and-bound  algorithm

[37]), dynamic

programming

[38], and

integer linear

$$\kappa \in \mathfrak { A } \, & \, \imath \in \mathfrak { A } \\ \sum _ { k \in K } \sum _ { j \in N } x _ { 0, j } ^ { k } = K \\ \quad \,.$$

$$\sum _ { k \in K } & \sum _ { i \in N } x _ { i, 0 } ^ { k } = K \\ \sigma & \sigma \ \.$$

(5)

- (4) programming (such as the column generation algorithm [39]). Many breakthroughs were achieved using exact algorithms in the 1970s. Christofides and Eilon [40] studied the CVRP and

dynamic  VRP  (DVRP)  by  designing  three  approaches:  a

$$u _ { j } ^ { k } = \begin{cases} u _ { i } ^ { k } - q _ { j }, & \text{if $x_{i,j}^{k}=1$, $\{i,j\}in A\colon j\neq 0$} \\ C & \text{if $j=0$} \\ \end{cases} & \text{for $\frac{partial}{\text{optim}$} \\ \text{are} \ e a } \\ \text{$\dots\colon \colon$}$$

$$q _ { j } \leq u _ { i } ^ { k } \leq C, \text{ if } x _ { i, j } ^ { k } = 1 \, \colon i \in V, \ j \in N \quad \quad \quad ( 7 ) \quad \text{prohibi} \\ 2 ) \, \ 1$$

$$x _ { i, j } ^ { k } \in \{ 0, 1 \}, \{ i, j \} \in A. \quad \quad \quad ( 8 ) \text{ \ \widehat { \text{divide} } } \text{ \ \text{algorit} l }$$

ci j ; In  this  formulation,  the  objective  is  to  minimize  the  total cost of transportation and represents the cost of arc { ,  }. i j Equations  (2)  and  (3)  indicate  that  each  customer  can  be served only once. Constraints (4) and (5) ensure that both the number  of  vehicles  leaving  the  depot  and  returning  to  the depot should be equal to the total number of vehicles, which means  that  each  vehicle  should  start  from  and  end  at  the depot.  Equation  (6)  shows  the  update  process  of  the  surplus capacity  of  the  vehicle k ,  and  constraint  (7)  ensures  that  the surplus  capacity  of  the  vehicle  must  not  be  less  than  the demand  of  its  next  customer j while  does  not  exceed  the branch-and-bound  approach,  a  saving  approach,  and  a  3-

optimal tour method. Christofides et al.

[41] used tree search algorithms to solve the VRP. Although these exact algorithms

are easy

to understand,

their computational

expense is

prohibitive in large-scale problems [42], [43].

2) Heuristic Algorithms: Heuristic algorithms can be divided into traditional heuristic algorithms and meta-heuristic algorithms. Referring to [44], traditional heuristic algorithms can  be  divided  into constructive and  two-stage  heuristic algorithms. The former gradually generates feasible solutions to minimize costs (e.g., the savings algorithm [28]). The latter first clusters nodes and then constructs multiple feasible routes to satisfy constraints. The quality of the solution is improved by changing the positions of nodes between or within routes (e.g.,  [45]).  Constructing the neighborhood of a solution is a significant  part  of  heuristic  algorithms.  However,  traditional heuristic  algorithms  perform  an  entire  search  without  focus; therefore, they only explore neighborhoods  in a limited manner.  A  meta-heuristic  algorithm  can  focus  on  potential

(3)

areas  by  combining  intelligently  complex  search  rules  [46] and  memory  structures.  As  reported  in  [47],  meta-heuristic algorithms generally produce better solutions than traditional heuristic algorithms (typically between 3 % and 7 % ).

Although exact and heuristic algorithms have been developed for many years, they all have their own limitations. Both  algorithms  frequently  require  optimization  expertise  to model the problem and design effective search rules. Moreover,  algorithms  based  on  search  frequently  encounter the challenge of a trade-off between searching efficiency and solution accuracy when solving complex combinatorial optimization  problems.  This  is  apparent  because  finding  an optimal solution comes at the expense of searching for a larger neighborhood and longer computational time. With the development  of  computer  technology  since  2010,  the  LBO algorithms  have  become  a  popular  research  topic  and  many breakthroughs  have  been  obtained  in  using  them  to  solve optimization problems. Along with the deepening of research on the VRP, it is necessary to design algorithms that can more rapidly and  efficiently solve problems.  Hence,  the  LBO algorithms are beginning to be applied to solve the VRP.

## III.  Learning-Based Optimization Algorithms for the VRP

From a technical point of view, the LBO algorithms usually include three kinds of learning modes [48], [49]. If the agent learns on  data with  labels, this training mode  is  called supervised learning (SL). In contrast, learning from unlabeled data  is  named  unsupervised  learning  (UL).  Note  that  UL  is commonly used for the parameter optimization of continuous problems; therefore, it is rarely used in the literature reviewed in our paper. The  final type of learning framework  is reinforcement learning (RL), which requires the agent to learn from  interacting  with  the  environment.  RL  involves  agents that sense the environment and learn to select optimal actions through trial  and  error  [50],  [51].  Compared  with  traditional optimization algorithms, the LBO  algorithms  have  three advantages:

- 1)  The  LBO  algorithms  do  not  require  substantial  domain knowledge  [52]  for  mathematical  modeling  and  parameter tuning,  which  enables  the  LBO  algorithms  to  model  routing problems in a real-world more flexibly.
- 2)  The  LBO  algorithms  can  automatically  construct  an empirical formula to approximate the mapping function between  the  inputs  and  solutions  through  pre-training  on  a dataset.

3) The LBO algorithms can automatically extract optimization knowledge from training data and give computers the ability to learn without being explicitly programmed [53].

The  ability  of  autonomous  and  offline  learning  results  in LBO models not requiring any hand-engineered reasoning and rapidly provides a promising solution for unknown data [52]. The  LBO  algorithms  have  been  applied  to  different  fields [24], such as video games [54], Go [55], robotics control [56] and  image  identification  [57].  Many  studies  have  used  the LBO  algorithms for the VRP, and we divide related frameworks  into two types: end-to-end and step-by-step approaches.

## A.  Step-by-Step Approaches

Step-by-step  approaches  learn  optimization  strategies  that could  iteratively  improve  a  solution  rather  than  outputting  a final solution directly. These approaches can find a promising solution, but they have low time efficiency for training since their search space is more extensive. We subdivide this type of  approaches  into  learning  assisting  heuristic  and  heuristically accelerated  LBO  algorithms  according  to  different solving frameworks.

- 1) Learning Assisting Heuristic Algorithms: Heuristic algorithms  use  a  series  of  operators  to  find  the  optimal solution [58]. During the search process, heuristic algorithms would generate considerable information about how to evolve and search in different stages [59], but this useful information is  not  comprehensively well utilized by heuristic algorithms. Hence,  many  studies  have  used  the  LBO  algorithms  as assistors to learn from this information and define the optimal setting  of  heuristic  algorithms.  Hence,  we  call  algorithms  of these  studies learning  assisting  heuristic  algorithms because they still search based on heuristic frameworks.

In their book in 1995, Gambardella and Dorigo [60] used Qlearning to construct the initial path for the ant system (AS). The  LBO  models  in  both  papers  of  Lima et  al. [61 ]  and Alipour et  al. [62 ]  were  also  used  as  constructors  of  initial solutions for heuristic algorithms. Subsequently, many scholars used LBO models to assist heuristic algorithms from different  aspects.  Liu  and  Zeng  [63]  proposed  an  improved generic algorithm (GA) with RL called RMGA. They used RL model to determine whether to break the connection relation of the initial tour and used the GA to reselect the next city for the  current  city.  Later,  Phiboonbanakit et  al. [64 ]  used  a transfer learning algorithm to aid the GA  in dividing customers into regional clusters beforehand. Ding et  al. [65] used  the  LBO  models  to  predict  the  values  of  nodes  for accelerating the convergence of solvers, and recently, Sun et al. [66] used the LBO model to quantify the likelihood of each edge  belonging  to  an  optimal  route  by  the  support  vector machine  (SVM).  The  LBO  models  can  reduce  the  search space to simplify problems for heuristics, and all experiments demonstrated that these learning assisting heuristic approaches can significantly improve the performance of original heuristic algorithms and speed up the solving process.

In addition to using the LBO algorithms to assist heuristic algorithms  in  constructing  the  solution,  the  LBO  algorithms can  set  parameters  of  heuristic  algorithms.  In  2017,  Cooray and Rupasinghe [67] used the LBO model to set the mutation rate  of  the  GA  for  the  energy-minimized  VRP  (EMVRP). They experimented with GA with different mutation rates on instances and selected the optimal parameter settings that can produce minimal energy consumption. It has been proven that parameter  tuning  according  to data characteristics has a significant  effect  on  improving  the  applicability  of  the  GA. Al-Duoli et  al. [68 ]  also  used  the  LBO  model  to  define  the parameters  values  of  a  heuristic  algorithm.  Moreover,  they used association  rules  trained  using  UL  to  provide  an  initial solution for the search.

The  large  neighborhood  search  (LNS)  algorithm  is  a  new

heuristic  algorithm  that  was  first  proposed  by  Shaw  [69]  in 1997. Compared with previous heuristics, this algorithm was based  on  the  ruin-and-recreate  principle [70] and  would explore  more  complex  neighborhoods.  The  LNS  algorithms have  outstanding  performance  in  solving  various  transportation and scheduling problems [71]. Therefore, some scholars have  considered  using  the  LBO  algorithms  to  improve  the LNS algorithms. The learned LNS algorithm for the VRP was first used by Hottung and Tierney [72] for the CVRP and split delivery  vehicle  routing  problem  (SDVRP)  in  2020  They adopted  a  LBO  model  as  the  repair  operator  for  the  LNS algorithm. Their model is limited by the number of destroyed fragments; hence, it ineffectively solves large-scale problems. Later, both Chen et al. [73] and Gao et al. [74] used a LBO model as a destroy operator of the LNS algorithm to solve the VRP. Chen et al. [73] used the proximate policy optimization (PPO)  algorithm to train a hierarchical recursive graph convolution  network  (GCN)  as  the  destroy  operator.  Then, they  simply  inserted  nodes  removed  by  the  destroy  operator into  the  infeasible  solution  according  to  the  principle  of minimum cost. Experiments on Solomon benchmarks [75] and synthetic datasets demonstrated that the model of Chen et al. [73] performed better than the adaptive LNS (ALNS) algorithm  in  terms  of  solution  quality  and  computational efficiency.  Gao et  al. [74 ]  considered  the  effect  of  graph topology on the solution, and they used an element-wise graph attention network with edge-embedding  (EGATE)  as  an encoder in which both the node set and arc set contribute to the attention weights. They used a principle similar to that in [73]  to  repair  the  destroyed  solution.  Overall,  using  a  LBO model  as  a  destroy  operator  in  the  LNS  algorithm  is  more beneficial for generating potential neighbor solutions than as a repair operator.

2)  Heuristically  Accelerated  LBO  Algorithms: Although using the LBO models as assistors can effectively improve the performance of heuristic algorithms, these heuristics still need to  elicit knowledge  from  experts  and  encode  it  into  the program,  which  is  far  from  being  easy  in  any  applications. Hence, some studies proposed to replace knowledge-modeling in conventional algorithms and used heuristics as assistors of the  LBO  algorithms  to  search  for  solutions.  We  call  these approaches heuristically accelerated LBO algorithms .

Reinaldo et  al. [76 ]  proposed  a  heuristically  accelerated distributed deep Q-learning (HADQL) algorithm and compared their model with the ant colony optimization (ACO) algorithm and deep Q network (DQN) on TSP instances. The results indicated that the convergence of their model was fast, but their model required more computation time. Yang et al. [77]  used  the  LBO  algorithm  to  approximate  the  dynamic programming  function, and their algorithm outperformed other  well-known  approximation  algorithms  on  large-scale TSPs. Joe and Lau [78] used the LBO algorithm to compute the serving cost of each node, then they applied a simulated annealing (SA) algorithm to plan routes to minimize the total cost  of  the  dynamic  VRP  with  time  windows  and  random requirements.  Later,  Delarue et  al. [79 ]  also  used  a  LBO algorithm to compute the cost of nodes beforehand and their experiments demonstrated that their model achieved an average gap against the well-known solver OR-Tools of 1.7 % on  CVRPs.  Yao et al. [80 ] considered  a  real-life route planning  problem.  They  regarded  the  safety  of  the  planned route as one of the objectives and used RL to train the LBO model. They compared their model with EMLS and NSGA-II on a map of New York. Although their framework has good efficiency  and  optimality,  it  requires  a  large  amount  of  time during training.

Chen  and  Tian  [81]  proposed  a  LBO  model  with  a  2-opt algorithm as the search operator. The model selects a fragment of a solution to be improved according to the region-picking policy  and  uses  the  rule-picking  policy  to  select  a  rewriting rule  applicable  to  the  region.  Both  policies  are  learnt  by  the LBO  model, and the solution is iteratively rewritten continuously until it converges. Because  this framework partially improves the solution, it is significantly affected by the  initial  solution.  Lu et  al. [82 ]  proposed  an  RL  model incorporating several heuristic operators, called the L2I. If the cost reduction after improvement by heuristic operators does not  reach  the  threshold,  a  random  perturbation  operator  is applied  and  the  operation  begins  again.  Generally,  [81]  and [82] are similar because both include improvement  and breaking during search process. Although they both obtained good  experimental  results,  [82] generated  more  potential solutions  than  [81]  because  [82]  designed  a  rich  set  of improvement  and  perturbation  operators.  Similar  to  [81], Costa et al. [83] and Wu et al. [84] used an LBO model as a policy network for the 2-opt operator. Reference [83] built an encoder-decoder framework based on the pointing mechanism [85]. Their greatest contribution was that they used different neural  networks  (NN)  to  embed  node  and  edge  information respectively, and proved that the consideration of edges has an important  effect  on  solving  the  VRP.  The  difference  in  [84] from [81] is that they did not construct another LBO model to decide  the  rewritten  region,  and  in  [84]  the  nodes  were embedded  based  on  the  self-attention mechanism  before selecting  node  pairs  to  apply  the  2-opt  heuristic.  Vlastelica et al. [86] turned solvers into a component of the LBO model to search the optimal solution. They used the LBO model to embed a sequence of nodes into a matrix of pairwise distances to input the solver. Then they used the gap between the output of  the  solver  and  data  label  as  a  loss  to  optimize  the  LBO model. Later, Ma et al. [87] modified the model of Wu et al. [84] by separating sequence embedding of the current solution from node embedding. Experimental results showed that their LBO  model  could  capture  the  circularity  and  symmetry  of VRP solutions more effectively. Although the above models could obtain better solutions for complex VRPs by making the use of an extensive heuristic search, they need a longer time to search.

In addition to incorporating heuristics as components of the LBO models, scholars have recently proposed to the use of the LBO algorithm to automatically select or generate heuristics, which can be considered as a type of hyper-heuristic algorithm. Hyper-heuristic algorithms have been used to solve the VRP in many studies [88]-[90]. This kind of algorithms does  not  search  randomly  and  is  guided  by  other  high-level algorithms.  Meignan et al. [91 ] first used  a  multi-agent

modular  model  based  on  the  agent  metaheuristic  framework (AMF) to build a selective hyper-heuristic framework for the CVRP and SDVRP. In their model, each agent builds an AMF model and selects low-level heuristics. RL and SL mechanisms are used by agents to learn from experience and other agents respectively. Later, Asta and Özcan [92] designed a hyper-heuristic  algorithm  based  on  apprenticeship  learning to produce a new heuristic method for the VRPTW. The same LBO algorithm was used by Tyasnurita et al. [93] to train the time delay neural network (TDNN) to solve the open vehicle routing problem (OVRP), which does not require vehicles to return  to  the  depot.  Moreover,  some  studies  used  the  LBO algorithms to select heuristics. Both Kerschke et al. [94] and Zhao et al. [95] trained the LBO model as a selector to select the  best  algorithms  for  a  given  TSP  instance.  GutierrezRodríguez et  al. [96 ]  constructed  a  multi-layer  perceptron (MLP)  to  select the best meta-heuristic for solving the VRPTW, and  Martin et  al. [97 ]  used  LBO  as  a  selector  to select the best parameter  settings  for  randomized  Clarke Wright savings for CVRP. Using LBO models as selector of heuristics can leverage complementarity  within a set of heuristics to achieve better performance.

## B.  End-to-End Approaches

End-to-end  approaches  learn  a  model  to  approximate  a mapping  function  between  the  input  (the  features  of  the problem) and the output (the solution of the problem), and the model can  directly  output  a  feasible  solution  on  unseen  test tasks. The time efficiency for training end-to-end approaches is  generally  better  compared  to  step-by-step  approaches,  but they  currently  encounter  challenges  in  finding  high  quality solutions and scalability. According  to the characteristic (whether  the  vehicle  visits the depot  multiple times) of problems which the approach is applied to, end-to-end approaches  can  be  divided  into  single-path  and  multi-path planning approaches.

1) Single-Path Planning: In some VRP variants, the vehicle is not required to return to the depot, such as the TSP, and the LBO algorithms applied to this type of VRPs output a single path that connect all of nodes. Hence, we refer to these LBO algorithms as single-path planning approaches .

Standard  NNs  rely  on  the  assumption  of  independence among data points and fixed-length inputs and outputs, which is unacceptable for sequence-to-sequence problems [98]. States of sequence-to-sequence problems are related in time or space, such as words from sentences and frames from a video. To  overcome  this  shortcoming,  recurrent  neural  networks (RNNs)  were  designed  [99],  and  other  RNN  architectures, long  short-term  memory (LSTM)  [100]  and bidirectional recurrent neural networks (BRNNs) [101] were also introduced for sequence learning in 1997. Later, many studies used  these  architectures  in  different  sequence-to-sequence problems,  such  as  natural  language  translation  [102]  and image captioning [103]. However, these models compress all the  information  of  the  encoder  into  a  fixed-length  context vector,  which  prevents  the  decoder  from  focusing  on  more important  information  when  decoding.  Hence,  Vinyals et  al. [85] introduced the attention mechanism  [104] into the sequence  model  of  [102]  to  enable  the  decoder  to  focus  on important embeddings and called their model Pointer Net . The attention  mechanism  enables  the  LBO  model  to  select  the contexts  that  are  most  closely  related  to  the  current  state  as input.  They  trained  an  RNN  with  a  non-parametric  softmax layer using SL to predict the sequence of the cities. Although their end-to-end model can directly output the target point of the  next  step,  it  is  not  effective  for  a  large-scale  TSP.  The experimental results also demonstrated that Pointer Net produced better solutions when the number of nodes was less than 30, but performed poorly on TSP40 and TSP50.

Vinyals et  al. [85 ]  first  proposed  an  end-to-end  model  for solving combinatorial optimization problems, but their research needs further improvement. Because SL is undesirable for NP-hard problems, Bello et al. [105] combined the RL algorithm with Pointer Net to solve the TSP in 2016. They proved that their method was superior to that of Vinyals et al. [85 ] on TSP100.  Levy  and Wolf  [106] transformed Pointer Net to  solve other sequence problems in addition  to  the  TSP.  They  inputted  two  sequences  to  two Pointer Nets and used a convolution neural network (CNN) to output the alignment scores, which were subsequently converted  into  distribution  vectors  using  a  softmax  layer. Later, Deudon et al. [107] used multiple attention layers and a feedforward layer as an encoder to simplify Pointer Net . Some studies  have  also  extended  the  structure  of Pointer  Net to other single-path problems. Li et al. [108] used Pointer Net to solve the multi-objective TSPs (MOTSP). They first decomposed the multi-objective problem into a series of subproblems  and  then  used Pointer  Net to  solve  each  subproblem.  All  sub-problems  can  be  solved  sequentially  by transferring weights of the network, using the neighborhoodbased parameter-transfer  strategy.  Kaempfer  and  Wolf  [109] added a leave-one-out pooling to extend Pointer Net to solve the multiple TSP (MTSP), and Le et al. [110] trained Pointer Net using  RL  to  plan  a  route  for  cleaning  robots.  Ma et  al. [111] extended Pointer Net to solve the TSP. Reference [111] used  a  graph  neural  network  (GNN)  to  encode  distances  of cities as context vectors of the attention mechanism, which is beneficial for solving a large-scale TSP. They also improved their model to a hierarchical framework to solve the TSP with time windows (TSPTW), in which they solved the vanilla TSP in the first level and solved the constraint of time windows in the high level.

Although Pointer Net has been widely accepted to solve the TSP  since  its  pioneering  contribution,  many  scholars  have proposed other end-to-end models to solve the TSP. As stated in  [112],  the  mentioned  neural  architectures  of Pointer  Net cannot yet effectively reflect the graph structure of the TSP; therefore,  Dai et  al. [112] proposed incorporating a GNN to construct solutions incrementally for the TSP, which is named as Structure2vec (S2V) [113], where the embeddings of their model  are  updated  continuously  by  adding  new  nodes  into currently infeasible solutions. Later, Ottoni et al. [114] used a set of statistical techniques called the response surface model (RSM) to determine values of the learning rate and discount factor of the LBO  algorithm.  Although  the  experiments proved that using RSM  can significantly improve the

performance  of  the  LBO  model,  their  model  needed  to calculate  optimal  parameters  per  instance  which  limited  the generalization of the model. To solve large-scale TSPs (up to 10000  nodes)  within  an  acceptable  runtime,  Fu et  al. [115] innovatively  introduced  a  graphic  transformation  and  heat map  technique  into  an  end-to-end  model.  They  used  graph sampling to abstract  sub-graphs  from  the  initial  large  graph, and the trained LBO model output the corresponding heat map (probability matrix over the edges). Finally, all the heat maps were merged into  the  final  heat  map,  and  the  RL  algorithm was used to output the optimal solution. The only limitation is that the model only experimented on simulation data but not on other benchmarks or real data. Zhang et al. [116] proposed an LBO approach for a kind of TSP that allows the vehicle to rejects orders, which was named the TSPTWR. They modified the model of Kool et al. [117] to output an initial solution for the TSP and then used a greedy algorithm to post-process the solution  by  rejecting  the  nodes  that  violate  time  windows. Compared  with  tabu  search  (TS),  this  method  has  a  shorter solving time and better results. Because the decisions of endto-end  approaches  cannot  be  reversed,  Xing  and  Tu  [118] combined  a  GNN  and  Monte  Carlo  tree  search  (MCTS)  to solve  the  TSP.  In  contrast  to  previous  LBO  models  that directly made a decision by using a prior probability generated by training, they used MCTS to further improve the decision. Although  their  model  outperforms  previous  LBO  models  in test sets of any size, the solving time is much longer.

Groshev et al. [119] and Joshi et al. [120] trained a GCN by SL to  solve  the  TSP.  What's  more,  [119]  further  expanded their model  by  using  a  trained  GCN  to  guide  heuristic algorithms to output solutions and then used these solutions as labels  to  retrain  the  GCN  for  large-scale  TSPs.  Prates et  al. [121] also used SL to train an LBO model. They considered the edge  weights  as  features per instance, and  that the deviation of their solutions from the optimal can be less than 2 % .  Previous  LBO  models  are  usually  trained  and  tested  on Euclidean  space,  which  makes  these  models  unavailable  in instances where nodes are not uniformly distributed. Consequently, Sultana et al. [122] tested their model in a nonEuclidean space. Although above LBO models trained by SL require  fewer  samples  compared  with  those  LBO  models trained by RL, the optimal solutions that are selected as labels have a significant influence on the performance of the models. Joshi et  al. [123] performed controlled experiments between the RL model [117] and SL model [120], and they also proved that the RL model has better generalization.

2) Multi-Path Planning Approaches: Most routing problems are limited by different constraints due to the complex environment.  In  these  VRPs,  the  LBO  algorithms  need  to output  multiple  path  loops  since  vehicles  need  return  to  the depot  more  than  once.  We  termed  these  LBO  algorithms  as multi-path planning approaches .

To  the  best  of  our  knowledge,  Nazari et  al. [124 ]  first proposed  an  end-to-end  approach  to  solve  the  CVRP.  They believed  that  the  order  of  inputs  is  meaningless;  thus,  they expanded Pointer Net by utilizing element-wise projections to map  static  elements  (coordinates  of  nodes)  and  dynamic elements  (demands  of  nodes)  into  a  high-dimensional  input.

The dynamic input is directly fed into the attention mechanism and not complexly computed using an RNN; thus, their model is easier to update embeddings while constructing the solution compared with Pointer Net . The optimality of the model  was  proved  by  Ibrahim et  al. [125 ]  by  comparing  it with column generation and OR-Tools on different instances. Later, Kool et al. [117 ] introduced a multi-attention mechanism  to  pass  weighted  information  among  different nodes out of consideration for the influence of adjacent structures on solutions. This attention mechanism enables nodes to capture  more  information  from  their  own  neighborhoods; hence,  their  encoder  can  learn  more  useful  knowledge  to obtain better solutions.

Both  Nazari et  al. [124 ]  and  Kool et  al. [117 ]  laid  the research  foundation  for  the  follow-up  development  of  the VRP. Therefore, many scholars either extended their models to  solve  VRP  variants  or  modified  them  to  obtain  better solutions.  Peng et  al. [126 ]  considered  that  node  features should  be  constantly  updated  during  the  solving  process. Therefore,  they  built  a  dynamic  attention  mechanism  model (AM-D) by recoding embeddings at each step. Compared with Kool et al. [117 ],  the  performance  of  AM-D  is  notably improved for VRP20 (2.02 % ),  VRP50 (2.01 % )  and  VRP100 (2.55 % ).  Based  on  the  model  of  Nazari et  al. [124 ],  Duan et  al. [127] added a classification decoder based on MLP to classify edges. The decoder of Nazari et al. [124] was used to output a sequence of nodes, while Duan et al. [127] used this sequence as labels to train  another  classification  decoder  for the final solutions. This type of joint training approach outperforms other existing LBO models in solving large-scale CVRP by approximately 5 % .  Vera and Abad [128] used one more  encoder  than  Kool et al. [117 ] to embed  vehicle information for the capacitated multi-vehicle routing problem (CMVRP) with a fixed fleet size. The VRPTW has also been widely studied, and the work of Falkner and Schmidt-Thieme [129] can be considered the first extension of Kool et al. [117] to solve the VRPTW. They employed two additional encoders to  embed  current  tours  and  vehicles  for  solving  large-scale CVRP and produce a comprehensive context for the decoder. Their decoder would concurrently select the visiting node and the  serving  vehicle.  Their  model  exhibits  strong  results  on solving VRPTWs, even outperforming OR-Tools.

In  the  past  two  years,  there  has  been  an  increase  in  the publication of papers that design end-to-end models for multipath problems. Similar to Peng et al. [126], in 2020, Xin et al. [130]  proposed  the  dynamic  update  of  embeddings  before decoding. However, they considered the computational complexity of the model; they changed the attention weight of the visited nodes in the top layer of the encoder instead of reembedding.  In  2021,  Xin et al. [131 ] proposed  another approach  to  expand  the  model  of  [117]  by  using  a  multidecoder  with  different  parameters  to  train  multiple  policies and select  the  best  one  to  decode  at  each  step.  Experiments indicated  that  these  innovations  are  useful  in  improving  the original  LBO  model.  Zhang et  al. [132 ]  used  the  model  of Kool et al. [117] to solve multi-vehicle routing problems with soft time windows (MVRPSTW). They considered vehicles as multi-agents,  where  all  agents  share  one  encoder  but  use

different  decoders.  Zhao et  al. [133 ]  modified  the  model proposed by Nazari et al. [124] by using a routing simulator to generate data and update both the dynamic state and mask for the CVRP and the VRPTW. Furthermore, they applied a local search  to  improve  solutions  of  the  LBO  model,  and  they demonstrated that combining the LBO algorithms and heuristic search can be a general method of solving combinatorial problems. In contrast to previous LBO models, which  are  based  on  the  standard  policy  gradient  method, Sultana et al. [134 ] first proposed adding an entropy regularization term to encourage exploration in solving VRPs, thereby avoiding the limitation that the LBO model can easily converge too rapidly to a poor solution. They experimentally demonstrated that the policy with the highest entropy found a satisfactory solution more easily. Drori et al. [135] transformed  the  structure  of  the  VRP  into  a  line  graph  by defining  each  edge  in  the  primal  graph  corresponding  to  a node in  the  line  graph,  and  computed  edge  weights  as  node features.  To  effectively  solve  dynamic  and  stochastic  VRP (DS-VRP), Bono et al. [136] proposed a multi-agent model by adding  two  attention  networks  based  on  [117]  to  encode vehicle's  features  and  make  a  last  decision.  Their  model presented favorable results for small-scale DS-CVRP and DSCVRPTW  compared  with  OR-Tools.  However,  the  model exhibited  poor  generalization  when  testing  on  deterministic CVRPs  and  CVRPTWs.  Lin et  al. [137 ]  incorporated  the model of Nazari et al. [124] with a graph embedding layer to solve the electric vehicle routing problem with time windows (EVRPTW).  They  trained  the  model  using  the  modified REINFORCE algorithm proposed by Kool et al. [117], but the maximum number of nodes in the test instance only went up to 100. Recently, Li et al. [138] modified the model proposed by Kool et al. [117] to solve the pickup and delivery problem (PDP),  which  has  priority  constraints  and  specific  pairing relations.  To  learn  complex  relations  and  precedence  among nodes  of  different  roles,  they  added  another  six  types  of attention mechanisms based on the original attention mechanism.  Later,  focused  on  the  fact  that  most  end-to-end approaches are designed for homogeneous vehicle fleets, Li et al. [139] proposed adding a vehicle decoder to minimize the travel  time  among  all  heterogeneous  vehicles  based  on  the model  of  Kool et  al. [117 ].  Their  LBO  model  would  select both a serving vehicle and a visiting node rather than solely selecting the next node to visit at each step.

Many  scholars have proposed other novel end-to-end frameworks for VRPs with different objectives. Li et al. [140] and James et al. [141] proposed a model to plan online vehicle routes to minimize computation time. The differences between the two papers lie in the architecture and problem background. Li et al. [140] first used an LSTM network to predict future traffic conditions and then used a double-reward value iterative  network  to  make  decisions.  However,  James et  al. [141]  planned  routes  by  improving Pointer  Net [85 ]. In addition, Li et al. [140] trained their model on the data of 400 000  taxi  trajectories  in  Beijing,  whereas  James et  al. [141] applied to the green logistics system and trained their model on the  traffic  data  of  Cologne,  Germany.  Balaji et  al. [142] combined a distributed prioritized experience reply [143] with

DQN  to  maximize  the  total  reward  of  the  VRP.  Although DQN is widely used in LBO models, it requires a significant amount of  time  to  converge  because  the  information  update lag  of  the  experience  replay.  With  respect  to  this  limitation, Mukhutdinov et  al. [144 ]  proposed  using  SL  to  generate preliminary Q values for nodes and they applied this approach to minimize the cost of the packet routing problem. Ramachandranpillai et al. [145] designed an adaptive extended spiking neural  P  system  with  potentials  (ATSNPS)  to  determine  the shortest  solutions  for  the  VRPTW.  They  experimented  on supermarket  chain instances and demonstrated that their model can obtain better solutions. Sheng et al. [146] used the principle  of maximizing  the  total  benefit  and  introduced global attention [147] to modify Pointer Net [85] to solve the VRP  with  task  priority  and  limited  resources  (VRPTPLR). Cao et al. [148] first used the RL model to solve the stochastic shortest  path  (SSP)  problem  requiring  an  on-time  arrival. They used the Q-value to represent the probability of arriving on time and set the discount factor of reward as 1 to maximize the probability of arriving on time. Chen et al. [149] built an LBO  model  for  an  autonomous  vehicle  fleet and postprocessed routes for minimizing the energy cost of the entire fleet. However,  the  environment  of  their  model  was  too idealistic, and they did not consider the dynamics in the real world.

In addition to designing models for the VRP with a single objective,  some  studies  were  aimed  at  achieving  multiple objectives  simultaneously,  which  is  difficult  for  traditional algorithms. To  minimize  driving time and route length, Kalakanti et al. [150] proposed a framework with two stages, which included clustering by heuristics and route planning by Q-learning.  However,  experiments  on  three  VRP  variants demonstrated that their model performed poorly in a stochastic setting. To minimize the tour length and cost of the ride-sharing field, Holler et al. [151] used a MLP to compute the pooling weights and selected action based on the pooling mechanism. They used the DQN and PPO algorithm to train the  LBO  model  respectively,  and  experiments  demonstrated that DQN is more efficient.

## C.  Analysis of Literatures

As  we  indicated  previously,  many  papers  using  the  LBO algorithms to solve different VRP variants have been published (Fig. 4). 14 and 25 relevant papers were published, respectively, in 2019 and 2020. As we can observe, there has been a rapid  expansion  in  this  field  over  the  last  two  years. This  is  primarily  because  of  the  following  reasons:  1)  An increasing  number  of  scholars  have  proven  that  the  LBO algorithm is competitive in solving combinatorial optimization problems;  2)  With  the  development  of  economic  globalization, transportation efficiency has become  a key factor affecting company profits; 3) Recent VRP studies have been characterized  by  large-scale  online  planning  and  complex constraints. These  factors  significantly promote  the  LBO algorithms in solving the VRP. Another aspect worth noting in Fig. 4 is that scholars tended to use step-by-step approaches in the initial research phase of the field. However, they preferred end-to-end approaches after 2017, which is primarily owed to

the success achieved by Alpha GO. However, with recent indepth research using the LBO algorithms for the VRP, much of  the  literature  has  demonstrated  that  combining  the  LBO model  with  other  algorithms  is  more  effective  for  complex optimization  problems.  Hence,  the  research  on  step-by-step methods has been revived.

Fig. 4.     Distribution of published papers per year for the VRP (the deadline for statistical data in 2021 is September).

![Image](image_000003_4782a6c5cebe3e696eff648104206201338b7432c0d69182e6dd2cf9c8efe196.png)

As shown in  the  pie  chart  on  the  right  of Fig. 5 ,  most  of studies  focus  on  the  TSP  (approximately  41 % ).  As  a  widely studied variants of the VRP, the TSP is a basic graph problem; therefore, scholars tend to test a new model on the TSP and then  extend  the  model  to  other  VRP  variants.  The  second widely  studied  variant  is  the  CVRP  (approximately  27 % ), which has a simple mathematical model but strong flexibility. Both the CVRP and TSP primarily focus on minimizing the tour length; hence, length is the most studied objective among the distribution in the objectives of current literature (approximately 75.7 % , Fig. 6 ). Note that the sum of percentages  in Fig. 6   is  greater  than  1  because  some  studies are multi-objective.

Fig. 5.     Distribution of different variants.

![Image](image_000004_0133ea6db898ebdffe7e21c0e045315174b022743d9a2908d7ca863cd89cac4a.png)

We  summarize the characteristics of different VRP problems by synthesizing and analyzing the referenced papers. The four evaluation indicators are described as follows.

i)  High-complexity: High-complexity  refers  to  VRPs  with multiple constraints or objectives, and we quantifiably represent the practicability of an LBO  model by the complexity of its application problem because the VRP in the real world often has multiple constraints or objectives. In this paper, we consider that problems with two or more objectives or constraints are high complexity. For example, the TSPTW with rejection solved by Zhang et al. [116] needs to minimize the tour length and the rejection rate; the VRP solved by Bono et al. [136] requires time windows and stochastic demands to be  satisfied  simultaneously.  We  considered  that  the  above problems  are  complex,  and  we  compared  the  number  of studies of two class approaches applied to complex problems. We observed that step-by-step approaches are more likely to be selected for problems with high-complexity.

Fig. 6.     Distribution of objective functions.

![Image](image_000005_63f70ac2ef4526cdb2d5e05b5a04712e215f1f2fcaa72dbc71f1c8229b85a862.png)

ii) Stochastic: We  define  the  VRPs  whose  customer's demands,  time  windows  or  other  uncertain  elements  follow some probability  distribution  models  as  stochastic  problems. For example, Balaji et al. [142] considered a VRP variant of on-demand  delivery,  whose  orders  were  generated  with  a constant probability; Joe and Lau [78] considered a real-time VRP in urban logistics  in  which  customers  and  orders  were randomly  added  or  cancelled.  Step-by-step  approaches  have limitations in solving stochastic VRPs since this type of LBO algorithms  needs  customers  and  travel  costs  to  be  known  in advance and has a longer solving time. Compared with stepby-step approaches, end-to-end approaches have the advantages of flexibility and time efficiency under stochastic environments,  which  can  quickly  respond  to  changes  by utilizing knowledge extracted from past training experience.

iii) Timeliness: Timeliness refers to those VRPs that require planning  a  tour  with  the  least  time,  which  is  a  significant characteristic of real-world VRPs. We consider problems that use LBO approaches to minimize tour time, such as in [141]. Overall,  end-to-end  approaches  are  easily  used  for  problems with timeliness because of their quick solution speed.

iv)  Fuzzy: With  the  development  of  the  VRP,  many  new VRP variants have been proposed. We use fuzzy methods to define  these  new  problems  because  researchers  do  not  have much expertise in these problems. The more likely it is that the LBO approaches is used to solve new problems indicates, the  more  likely  it  has  a  more  generic  modeling  framework. More  fuzzy  problems  are  solved  by  end-to-end  approaches among the included literature as the learning process of these approaches does not require abundant domain knowledge.

Generally,  end-to-end  approaches  are  suitable  for  VRPs with stochastic or time requirements; step-by-step approaches can solve more complex problems effectively.

In  addition  to  analyzing  the  applied  problems  of  the  LBO algorithms,  it  is  interesting  to  compare  their  models.  The previous review clearly  indicates  that  many  studies  used  the encoder-decoder  framework;  therefore,  we  compared  these encoder-decoder  models  (see Table I ,  where  MHA  refers  to the multi-head attention layers and FF refers to the feedforward network). We observed that the encoder-decoder framework is more commonly used in end-to-end approaches,

TABLE I

## Comparison of Encoder-Decoder Frameworks Among Referred Models

| Approaches              | Literature   | Problem             | Encoder           | Embedding features              | Decoder                             | Learning manner   |
|-------------------------|--------------|---------------------|-------------------|---------------------------------|-------------------------------------|-------------------|
|                         | [127]        | CVRP                | GCN               | nodes &distance matrix          | RNN + PM                            | RL + SL           |
|                         | [117]        | CVRP                | MHA               | nodes                           | MHA                                 | RL                |
|                         | [85]         | TSP                 | RNN               | nodes                           | RNN + PM                            | SL                |
|                         | [105]        | TSP                 | RNN               | nodes                           | RNN + PM                            | RL                |
|                         | [106]        | TSP                 | CNN               | nodes                           | RNN                                 | SL                |
|                         | [108]        | TSP                 | RNN               | nodes                           | RNN + PM                            | RL                |
|                         | [110]        | TSP                 | GNN               | nodes                           | RNN + PM                            | RL                |
|                         | [111]        | TSP                 | GNN               | nodes                           | RNN + PM                            | RL                |
|                         | [116]        | TSPTWR              | MHA               | nodes                           | MHA                                 | RL                |
|                         | [122]        | TSP                 | CNN               | nodes                           | RNN + PM                            | SL                |
|                         | [124]        | CVRP                | GCN               | nodes                           | RNN + PM                            | RL                |
|                         | [126]        | CVRP                | MHA               | nodes                           | MHA                                 | RL                |
| End-to-end approaches   | [128]        | CMVRP               | GCN               | nodes                           | RNN + PM                            | RL                |
|                         | [129]        | CVRPTW              | MHA               | nodes &tour & vehicles          | MHA                                 | RL                |
|                         | [130]        | CVRP, TSP           | MHA               | nodes                           | MHA                                 | RL                |
|                         | [132]        | MVRPSTW             | MHA               | nodes                           | MHA                                 | RL                |
|                         | [133]        | CVRP, VRPTW         | GCN               | nodes                           | RNN + PM                            | RL                |
|                         | [135]        | CVRP, TSP           | MHA               | edges                           | MHA                                 | RL                |
|                         | [136]        | DS-CVRP, DS- CVRPTW | MHA               | nodes &vehicles                 | MHA                                 | RL                |
|                         | [137]        | EVRPTW              | GCN               | nodes &battery & time &vehicles | RNN + PM                            | RL                |
|                         | [138]        | PDP                 | MHA               | nodes                           | MHA                                 | RL                |
|                         | [141]        | Online VRP          | GNN               | nodes                           | RNN + PM                            | RL                |
|                         | [146]        | VRPTPLR             | RNN               | nodes                           | RNN + PM                            | RL                |
|                         | [137]        | EVRPTW              | GCN               | nodes                           | RNN + PM                            | RL                |
|                         | [72]         | CVRP, SDVRP         | Two linear layers | tour                            | FF                                  | RL                |
| Step-by-step approaches | [74]         | CVRP, CVRPTW        | EGATE             | nodes &edges                    | RNN + PM                            | RL                |
| Step-by-step approaches | [83]         | TSP                 | GCN               | tour                            | PM + Max-pooling &FF + Mean-pooling | RL                |

and most differences of frameworks are in the encoder. This is probably  due  to  the  fact  that  if  scholars  require  different information,  they  must  use  different  NNs  to  extract  related features  from  the  input.  If  authors  seek  to  incorporate  more features in addition to the node coordinates and demands, they often consider the GCN as a good option. Conversely, there are two  main  types  of  decoders: RNN  with  a  pointing mechanism  (PM)  (Vinyals et  al. [85 ])  and  a  decoder  with several  multi-head  attention  sublayers  (MHA)  (Kool et  al. [117]). In terms of learning manners, Vinyals et al. [85] used SL to train the model, whereas all the others used RL. This is expected  because  the  VRP  is  an  NP-hard  problem,  and  it  is difficult to obtain labels for training data. We also present an overview of the LBO architectures in Fig. 7. We first classify NN models according to training algorithms. Then, we make a list  where  we  solve  problems  of  different  structures  together with the corresponding literature and their publication time.

To clearly compare the literature, we list the main content of the referenced papers, including model features, baselines, and benchmarks in experiments at the end of the paper. For convenience,  we  abbreviate  end-to-end  approaches  as  E2E

and step-by-step approaches as SbS in this table. Benchmarks can frequently be divided into simulation data and data from the literature or the real world; we  refer to the latter collectively as real data.

## IV.  Experimental Study

## A.  Experimental Setting

1) Experimental Algorithms: As described in Section III, the research  of  Lu et  al. [82 ]  (L2I)  and  Chen  and  Tian  [81] (Rewriter)  are the recent research trends of step-by-step approaches,  and  both  papers  are  also  among  the  most  cited step-by-step  approaches.  Kool et  al. [117 ]  (AM)  made  a significant contribution to solving the VRP by using an endto-end framework. Many studies have modified the model of Kool et al. [117 ], and the model  of Xin et al. [130] (ASWTAM), proposed in 2020, is one of the representatives. ASWTAM designs a step-wise update embedding mechanism, which is  beneficial  for  the  original  AM  model  as  it  helps  it focus on useful inputs and determine the optimal solution. To

Fig. 7.     Overview of LBO architecture of overviewed papers ([152]).

![Image](image_000006_ffe250f1ae51efe5ac7d43322534fb506361f99be61b468d21609b1d520c7d2d.png)

analyze  the  characteristics  and  limitations  of  different  LBO models, we test these four representative LBO algorithms on different sizes of instances, and we compare them with other algorithms  in  this  section.  We  select  three  classical  metaheuristics (ACO, TS and LNS), and two well-known solvers (Gurobi  and  OR-Tools)  as  baseline  algorithms.  The  best parameters of different scales of the problems are determined through multiple experiments. To limit the total time expended when solving the problem, we modify the time limit parameter of Gurobi to 1800 seconds.

- 2)  Problem  Details: We  evaluate  four  LBO  models  and baseline algorithms on the CVRP of different scales, and we do not distinguish the CVRP and VRP in this section.
- 3) Data Generation:
- i)  Training  set: We  consider  three  training  instances,  the Euclidean VRP with 20, 50, and 100 nodes, named VRP20, VRP50, and VRP100, respectively. For all tasks, the location

of each customer and the depot are uniformly sampled in the unit square [0, 1] 2 , and the demand of each customer is also uniformly  sampled  from  the  discrete  set  {1,  2,   …,  9}.  The capacities of a vehicle are 30, 40, and 50 for N = 20, 50, and 100, respectively.

ii) Test set: We test different algorithms using three types of test data: a) Set 1, following the same rules as the training set, we newly generate 300 instances for the three-scale VRP. b) Set 2 also contains 300 VRP instances with n =  20,  50,  and 100,  but  the  locations  of  the  nodes  are  sampled  from  the gamma  distribution  ( α =  1, β =  0).  c)  Set  3  contains  nine benchmarks  from  Subramanian et  al. [153 ],  whose  nodes ranged from 100 to 200.

- 4)  Hyperparameters  Setting: The  hyperparameters  of  the selected LBO models are the same as those in the literature to ensure  the  validity  of  the  experimental  results  as  much  as possible.  We  set  the  random  seed  at 1234   to  ensure  the

consistency of training data, and we set the batch size during testing to 1 to better compare the running time. Table II lists the other hyperparameters of the training model. We conduct all the experiments on Python software on a computer with a Core i7-9800x 3.8-GHz CPU, 16 GB memory, Windows 10 operation system, and a single 2080Ti GPU.

TABLE II The Hyperparameters of LBO Models

| Parameters               | AM        | Rewriter   | L2I    | ASWTAM    |
|--------------------------|-----------|------------|--------|-----------|
| The number of epochs     | 100       | 10         | 40 000 | 100       |
| The size of training set | 1 280 000 | 100 000    | 2000   | 1 280 000 |
| Batch size               | 512       | 128        | 1000   | 512       |
| Learning rate            | 0.001     | 5.00E-05   | 0.001  | 1.00E-04  |

- 5) Evaluation Metrics: To better evaluate LBO models, we use multiple evaluation metrics that are widely used in the RL research community [25], [142]. In addition, since each type of VRP in sets 1 and 2 only contains 300 instances, we use the Wilcoxon signed-rank test and Friedman test on test results to infer the holistic performance of different LBO models.
- i) Training time and occupy memory.
- ii) Length of solutions, optimal gap, and solving time in perinstance.
- iii)  Rank  obtained  by  Friedman  test  and p -value  obtained from the Wilcoxon test.
- 6) Experimental Design:
- To fully evaluate the effect of LBO models, we compare the experimental  algorithms  from  three  aspects:  time  efficiency, scalability,  and  optimality.  The  comparison  experiments  can be divided into three parts:

Part  I: To  validate  the  learning  effectiveness  of  the  LBO models,  we  train  the  LBO  models  on  the  training  set  and compare  the  training  time,  occupied  memory,  and  learning curves among the LBO models.

Part II: To validate the time efficiency and optimality of the LBO models, we test algorithms on sets 1 and 2 and compare the  solution  length,  solving  time,  and  standard  deviation.  In addition, we use the Wilcoxon signed-rank test and Friedman test to analyze statistical results in depth.

Part III: To validate the scalability of LBO models to larger problems,  we  test  LBO  models  trained  on  VRP20  on  set  3. We use the solution length, solving time, and optimal gap as evaluation indicators.

## B.  Results and Analysis

1) Comparison and Discussion of Part I: We compared the training time (in hours) of the four LBO models on VRP20, VRP50, and VRP100. The left bar chart of Fig. 8 shows that end-to-end  approaches  frequently  requires  less  training  time than  step-by-step  approaches.  AM  requires  the  least  training time, whereas Rewriter requires the most training time. This is primarily  because  these  step-by-step  approaches  must  be combined with heuristic search to determine optimal solutions;  therefore,  they  require  more  time  to  constantly experience the circulation of generation, evaluation, and evolution  during  training.  While  end-to-end  approaches  use NNs to  replace  this  complex  circulation,  they  can  cost  less time to learn. Comparing Rewriter with L2I, although Rewriter uses only 2-opt to generate neighborhood solutions and  L2I  uses  six  heuristics,  the  training  size  of  Rewriter  is approximately 50 times than that of L2I. Therefore, Rewriter requires  more  time  than  L2I.  Comparing  two  end-to-end approaches,  ASWTAM  costs  longer  training  time  since  it needs to update embeddings at each step.

We also recorded the occupied memory of the LBO models during  training  (right  bar  chart  of Fig. 8 ),  and  we  observed that step-by-step approaches require less memory than end-toend approaches; e.g., L2I requires only approximately 1.2892 GB on VRP100 and ASWTAM requires 9.07 GB. This result reveals that end-to-end approaches can fully utilize advanced computing hardware to reduce training time but requires the computer to have a large amount of memory to store sizable data generated by parallel computing. In addition, we observed that LBO models require more memory as the size of  the  problem  increases.  This  is  apparent  because  a  largescale VRP has high input dimensions, and solving it requires deeper NNs with more parameters. However, the problem size remains a challenge for the LBO algorithms to settle because of the curse of dimensionality and the limitations in computational resources. Comparing L2I with Rewriter, Rewriter  requires  more  memory  during  training  because  it uses another policy network to select segments to be rewritten in  addition  to  a  rule-defining  policy  network.  Similarly,  we observed  that  ASWTAM  requires  more  memory  than  AM. This is because ASWTAM  requires step-wise update embeddings but the embeddings of AM are fixed.

We illustrated the comparison of learning curves in Fig. 9, and  we  defined  the  average  solution  length  of  samples  of  a batch as distance. The learning curve is an important index for evaluating  the  learning  ability  of  an  LBO  model,  and  it  can provide much information about the LBO model. The earlier the  turning  point  of  the  curve  tends  to  be  stable,  the  fewer samples  are  required  for  model  training.  In  addition,  the distance  in  the  training  process  predicts  the  optimization  of the model  on  the  training data, whereas  a  lower  value indicates a better performance of the LBO model. Comparing the two approaches, we can conclude that step-by-step approaches  can  converge  faster  than  end-to-end  approaches. Among them, L2I has the best learning performance because its  learning  curve  reaches  the  turning  point  earlier,  which proves that using heuristics as search operators can effectively improve the learning effectiveness of LBO models. This also explains  why  step-by-step  approaches  require  less  training data than end-to-end approaches. In addition, we can speculate that  there  is  a  mutual  promotion  between  LBO  models  and heuristic algorithms, but exceedingly  few  heuristics may reduce the learning ability of LBO  models, similar to Rewriter. Additionally, note that the learning curve of ASWTAM is below the AM as the training progressed. This proves that the input features have an important influence on the learning ability of the model because ASWTAM modifies AM by dynamically updating the embeddings.

Fig. 8.     Training time and occupy memory of LBO models on VRP20, VRP50, and VRP100.

![Image](image_000007_0c158f8db17acd12ca60339b67224b992921db8dc7b7a2779b0d9fbfca28dc7e.png)

![Image](image_000008_a7bf81151cdcf444e245887aee93433c52d3544f5d10e359ea5b1f843231ca0f.png)

Fig. 9.     Learning curves of LBO models on VRP20 and VRP50.

![Image](image_000009_68b0da7ba66e9bed5c7a231b17124ac9e8560e48640d166612fa85528500b31f.png)

![Image](image_000010_87ed6096517ef0762cd1a072bd7fc80aee6fcd05eab52a8111ac67cbdb20ccae.png)

2) Comparison and Discussion of Part II: Tables III and IV show the optimization  of  the  LBO  algorithms  on  test  sets  1 and  2,  respectively.  For  the  columns  in Tables III  and  IV, column 1 shows the algorithms, and columns 2-4 respectively lists the average tour length (mean), standard deviation (std), and  average  solving  time  (time)  used  by  each  algorithm  for instances with n = 20. Columns 5-7 and 8-10 respectively list the  same  information  of  experimental  algorithms  on  the instances with n = 50 and 100. Note that we tested two types of search policies for AM for a comprehensive comparison.

First,  the  tables  show  that  L2I  has  the  minimum  mean  on testing  instances  except  for  VRP20,  and  its  results  are  even better than those of OR-Tools. Furthermore, we observed that the  standard  deviations  of L2I  and  Rewriter  are  always smaller than those of OR-Tools, LNS and ACO. These results indicate  that  incorporating  the  LBO  algorithms  within  the heuristic search can result in a stable performance and strong generalization  of  the  models.  Several  studies  [107],  [122], [133] have indicated this positive effect. Second, we observed that TS and LNS perform below ASWTAM and AM (greedy) in VRP50 and VRP100 on set 1, and ASWTAM is better than OR-Tools in VRP100. However, end-to-end approaches have disappointing  performance  on  set  2,  although  their  solutions are still better than that of ACO. Therefore, we can conclude that  end-to-end approaches have a strong dependence on the distribution of data. In addition, comparing L2I to end-to-end approaches, although L2I outperforms ASWTAM and AM in terms  of  the  quality  of  the  solution,  end-to-end  approaches have an advantage in computational time, e.g., AM (greedy) requires  only 0.93  s and  ASWTAM  requires 1.82  s on VRP100  from  set  1,  but  L2I  requires  25.25  s.  Third,  we observed  that  different search  methods  are  applicable  to different types of test data. Comparing Tables III and IV, we can conclude that the sampling search method outperforms the greedy search method on data obeying the gamma distribution,  but  the  greedy  search  method  can  obtain  better solutions for data obeying the same distribution as the training data. This is because the greedy search selects the best action at  each  step  according  to  training  experience,  whereas  the sampling search selects the best from many sampled solutions [111]. Hence, a greedy search is more dependent on the data distribution. Finally, comparing L2I with Rewriter, we observed that L2I has better optimization than Rewriter. Thus, more search operators are beneficial to searching for a larger solution space, and the optimal solution is more likely to be determined.  However,  note  that  an  excessive  number  of heuristic  operators  result  in  the  solving  time  of  L2I  being twice as that of Rewriter. We also compared two LBO models of  end-to-end  approaches  and  concluded  that  ASWTAM  is better  than  AM  in  quality  of  solutions.  ASWTAM  adopts  a dynamic  embedding  mechanism,  and  dynamic  embedding aids  the  network  in  capturing  the  real-time  characteristics  of the environment.

To further illustrate the overall performance of the experimental  algorithms  across  all  test  cases,  we  performed nonparametric  tests  on  VRP20  from  set  1  and  2,  and  the results  are  shown  in Fig. 10  and  Table V.  Fig. 10   shows  the results  of  the  Freidman  test  and  uses  Freidman  Rank  as  the abscissa. The  smaller the rank value is, the better the algorithm performs in all instances. Table V shows the results of  the  Wilcoxon  test  and  column  1  represents  the  tested algorithms.  The  Wilcoxon  test  first  computes  the  difference between  the  two  algorithms  on  a  set  of  instances  and  then obtains the corresponding rank by sorting the absolute values of the differences. Column 2 of Table V summarizes the rank of the positive differences, and column 3 summarizes the rank

TABLE III Comparison of Average Value and Solving Time (In Seconds) of Different Algorithms on Dataset 1

| Baseline     | VRP20, Cap30   | VRP20, Cap30   | VRP20, Cap30   | VRP50, Cap40   | VRP50, Cap40   | VRP50, Cap40   | VRP100, Cap50   | VRP100, Cap50   | VRP100, Cap50   |
|--------------|----------------|----------------|----------------|----------------|----------------|----------------|-----------------|-----------------|-----------------|
| Baseline     | mean           | std            | time           | mean           | std            | time           | mean            | std             | time            |
| Gurobi       | 5.74           | 0.62           | 1800           |                | -              |                |                 | -               |                 |
| OR-Tools     | 6.12           | 1.06           | 1.25           | 10.55          | 1.56           | 2.32           | 16.55           | 2.06            | 3.12            |
| TS           | 6.27           | 0.69           | 23.57          | 11.4           | 0.96           | 54.53          | 18.61           | 1.86            | 113.4           |
| ACO          | 12.29          | 1.38           | 7.42           | 19.86          | 1.97           | 34.81          | 37.03           | 4.57            | 126.6           |
| LNS          | 6.48           | 0.96           | 4.96           | 12.85          | 1.62           | 94.32          | 18.86           | 2.03            | 771.6           |
| AM(greedy)   | 6.41           | 0.83           | 0.25           | 10.79          | 0.82           | 0.53           | 16.66           | 1.78            | 0.93            |
| AM(sampling) | 6.42           | 0.83           | 0.3            | 10.9           | 0.92           | 0.53           | 19.35           | 1.65            | 0.91            |
| ASWTAM       | 6.36           | 0.67           | 1.41           | 11.03          | 0.76           | 1.65           | 16.36           | 1.58            | 1.82            |
| Rewriter     | 6.83           | 0.92           | 2.15           | 12.45          | 1.03           | 5.14           | 19.98           | 1.9             | 10.56           |
| L2I          | 6.1            | 0.73           | 5.93           | 10.34          | 0.86           | 13.33          | 16.11           | 1.8             | 25.25           |

TABLE IV Comparing Average Value and Solving Time (In Seconds) of Different Algorithms on Data Set 2

| Baseline     | VRP20, Cap30   | VRP20, Cap30   | VRP20, Cap30   | VRP50, Cap40   | VRP50, Cap40   | VRP50, Cap40   | VRP100, Cap50   | VRP100, Cap50   | VRP100, Cap50   |
|--------------|----------------|----------------|----------------|----------------|----------------|----------------|-----------------|-----------------|-----------------|
| Baseline     | mean           | std            | time           | mean           | std            | time           | mean            | std             | time            |
| Gurobi       | 20.85          | 5.47           | 1800           |                | -              |                |                 | -               |                 |
| OR-Tools     | 21.83          | 5.58           | 1.03           | 38.16          | 9.01           | 2.12           | 58.28           | 12.31           | 2.56            |
| TS           | 22.51          | 5.73           | 58.86          | 38.47          | 9.03           | 76.26          | 63.85           | 12.38           | 113.93          |
| ACO          | 35.13          | 8.61           | 23.51          | 87.03          | 13.99          | 45.15          | 165.79          | 13.95           | 79.08           |
| LNS          | 21.09          | 5.80           | 20.15          | 41.85          | 10.56          | 103.84         | 62.12           | 13.51           | 804.10          |
| AM(greedy)   | 40.33          | 13.9           | 0.42           | 149.17         | 73.03          | 1.03           | 241.25          | 138.26          | 0.98            |
| AM(sampling) | 37.83          | 9.54           | 0.35           | 95.23          | 21.27          | 0.62           | 169.13          | 30.31           | 0.8             |
| ASWTAM       | 28.07          | 7.54           | 1.41           | 58.02          | 16.49          | 1.94           | 110.79          | 22.85           | 2.2             |
| Rewriter     | 24.45          | 6.63           | 2.26           | 44.05          | 9.95           | 5.83           | 69.46           | 12.71           | 12.93           |
| L2I          | 20.88          | 5.41           | 6.01           | 35.43          | 8.75           | 13.6           | 54.44           | 12.35           | 25.13           |

of  negative  differences.  The  gap  between  the  two  sums represents the difference between the two algorithms, and the p -value in the final column is used to evaluate this difference quantitatively.  Under  the  confidence  degree α =  0.05,  a p -value  greater  than  0.05  means  that  there  is  no  significant difference between the two algorithms.

models  on  VRP20  and  tested  them  on  set  3  to  verify  the scalability of the LBO models, and we show the performance of  the  experimental  algorithms  in Table VI  and  Fig. 11 .  We selected only AM with a sampling search policy for testing.

&gt; 0 05 : &lt; 0 05 : From the Friedman test (in Fig. 10) on VRP20, we observed that L2I is second only to Gurobi in VRP20, and the results of the Wilcoxon test of the two algorithms in Table V indicates that L2I is not significantly different from Gurobi in VRP20 at a confidence coefficient of 0.05. Moreover, we observed that the  performance  of  ASWTAM  is  not  significantly  different from AM on set 1, but AM is distinctly inferior to ASWTAM on dataset 2. This proves that AM is effectively improved by ASWTAM. It is worthy noting that the p -value of OR-Tools and L2I is 1 in set 2, although this value is 0.000002 in set 1. Similar  results  are  observed  in  the  comparison  of  the  other three  LBO  models  with  OR-Tools,  in  which  LBO  models achieve p -values in  set  1  but in  set  2.  This indicates  that  LBO  models  have  limitations  when  applied  to the  problem  whose  data  distribution  is  different  from  the training set.

3) Comparison and Discussion of Part III: We trained LBO

Note that the solving time of all the algorithms increases as the  scale  of  the  problem  increased,  but  the  solving  time  of LBO is always  significantly  less  than  that  of  ACO  and  TS. The most time-consuming L2I is 33.08 s in instances with 195 nodes,  whereas  TS  requires  319.98  s;  the  longest  computational time of the end-to-end model is less than 7 s , which is significantly less than that of OR-Tools. Second, L2I has the best scalability on benchmarks, and the gap between L2I and the optimal does not exceed 0.1 . However, L2I is still inferior to TS in X-n101-k25 and X-n186-k15. Moreover, as shown in Table VI and  Fig. 11 ,  models  of  end-to-end  approaches  are more unstable than the step-by-step approaches for large-scale problems.  The  maximum  gap  of  AM  is  up  to  5.99,  but  the minimum gap is only 0.83. However, compared with AM, the ASWTAM exhibits significant improvement, and it is better than  Rewriter  in  X-n186-k15.  In  the  other  instances,  the solutions of ASWTAM are always better than ACO and close to  Rewriter.  This  indicates  that  end-to-end  approaches  are promising,  and  further  research  is  required  to  achieve  better

![Image](image_000011_a490100b4e55838e4743b11aa98708d7ef0a06d92e774c7b69e03193f6c260ba.png)

4

6

Freidman rank

Fig. 10.     Friedman test on VRP20 from different datasets (left from set 1, right from set 2).

TABLE V Wilcoxon Test Results Among Different Algorithms for α = 0.05 on VRP 20 From Different Date Set

| Comparing algorithms       | Data set 1   | Data set 1   | Data set 1   | Data set 2   | Data set 2   | Data set 2   |
|----------------------------|--------------|--------------|--------------|--------------|--------------|--------------|
|                            | R+           | R-           | p - value    | R+           | R-           | p - value    |
| L2I vs OR-Tools            | 465          | 0            | 0.000002     | 267          | 198          | 0.471592     |
| L2I vs Gurobi              | 168          | 267          | 1.000000     | 82.5         | 382.5        | 1.000000     |
| L2I vs TS                  | 465          | 0            | 0.000002     | 465          | 0            | 0.000002     |
| Rewriter vs ACO            | 465          | 0            | 0.000002     | 465          | 0            | 0.000002     |
| Rewriter vs TS             | 109          | 356          | 1.000000     | 3            | 462          | 0.000002     |
| Rewriter vs OR-Tools       | 181          | 284          | 1.000000     | 53           | 412          | 0.000053     |
| Rewriter vs AM(sampling)   | 29           | 436          | 1.000000     | 465          | 0            | 0.000002     |
| Rewriter vs AM(greedy)     | 24           | 441          | 1.000000     | 465          | 0            | 0.000002     |
| ASWTAM vs Rewriter         | 434          | 31           | 0.000033     | 3            | 462          | 0.000002     |
| ASWTAM vs ACO              | 465          | 0            | 0.000002     | 452          | 13           | 0.000006     |
| ASWTAM vs AM(greedy)       | 285          | 150          | 0.130731     | 464          | 1            | 0.000002     |
| ASWTAM vs OR-Tools         | 316          | 149          | 0.084035     | 0            | 465          | 0.000002     |
| AM(sampling) vs AM(greedy) | 219          | 246          | 1.000000     | 443          | 22           | 0.000014     |
| AM(greedy) vs ACO          | 465          | 0            | 0.000002     | 15           | 450          | 0.000012     |
| AM(greedy) vs TS           | 393          | 72           | 0.000928     | 0            | 465          | 0.000002     |
| AM(greedy) vs OR-Tools     | 313          | 152          | 0.095706     | 0            | 465          | 0.000002     |

improvement.

4)  Experiments  Conclusion: To  demonstrate  the  effectiveness of the LBO models, we tested AM, ASWTAM, Rewriter, and L2I on three datasets and compared the LBO approaches with ACO, TS, LNS, Gurobi, and OR-Tools. According to the results of the three experiments, the following conclusions can be drawn: i) Step-by-step approaches have faster convergence and better generalization than end-to-end approaches, but they require  more  computational  time;  ii)  End-to-end  approaches frequently have less computational time both during training and  testing,  but  they  require  more  computational  resources; iii) Both approaches have limitations in the scale of problems and  their  performance  is  affected  by  data  distribution;  iv) Compared  with  conventional  algorithms,  LBO  approaches require further improvements.

## V.  Conclusion and Research Trend

The  LBO  algorithms  have  been  successfully  applied  to  a series  of  optimization  problems,  and  studies  on  using  these algorithms  to  solve  the  VRP  can  be  divided  into  two  types:

end-to-end  and  step-by-step  approaches.  Exhaustive  experiments  demonstrate  that  step-by-step  approaches  have  strong scalability but are not suitable for problems requiring solving time, whereas end-to-end approaches can rapidly solve problems but are more dependent on data distribution.

Using the LBO algorithms to solve the VRP is still under research. There are several challenges in the LBO algorithms that  need  to  be  settled  in  the  future  and  we  suggest  several potential research directions of applying the LBO algorithms in the VRP from these limitations.

1) Solving sizable or more complex VRPs based on the decomposition framework. In 2014, Yao and Liu [154] had proposed that scaling up learning algorithms is an important problem. LBO models have difficulty in solving combinatorial optimization  problems  with  high  complexity  or  large  scale, because these problems result in a curse of dimensionality and easy to overfit [155]. Li et al. [108] proposed to decompose a multi-objective  VRP  into  a  set  of  subproblems,  and  their results decomposing  that their framework  is better than NSGA-II in problems with five objectives; Fu et al. [115] also

OR-Tools

ASWTAM

Gurobi

TS

ACO

L2I

AM (sampling)

AM (greedy)

Rewriter

2

8

10

TABLE VI

|          | Gap   | 0.06       | 0.08       | 0.07      | 0.08       | 0.12       | 0.07       | 0.09       | 0.09       | 0.07       |
|----------|-------|------------|------------|-----------|------------|------------|------------|------------|------------|------------|
| OR-Tools | Time  | 0.53       | 3.43       | 4.76      | 10.02      | 11.28      | 23.4       | 22.85      | 44.38      | 54.37      |
| OR-Tools | Value | 29405      | 16149      | 14243     | 31362      | 15223      | 22650      | 22477      | 52111      | 26017      |
|          | Gap   | -0.29      | 0.18       | 0.17      | 0.15       | 0.21       | 0.16       | 0.24       | 0.01       | 0.29       |
| TS       | Time  | 188.89     | 211        | 208.53    | 309.08     | 315        | 444.8      | 348.16     | 341.64     | 319.98     |
| TS       | Value | 19504      | 17745      | 15591     | 33196      | 16479      | 24678      | 25582      | 48386      | 31176      |
|          | Gap   | 0.5        | 1.1        | 1.5       | 0.7        | 1.6        | 0.7        | 1.4        | 0.8        | 1.2        |
| ACO      | Time  | 61.21      | 50.03      | 57.26     | 51.47      | 70.86      | 94.01      | 85.54      | 142.9      | 136.3      |
| ACO      | Value | 39913      | 31192      | 33517     | 48745      | 35071      | 35577      | 49431      | 84249      | 54150      |
|          | Gap   | 0.02       | 0.02       | 0.05      | 0.05       | 0.04       | 0.06       | 0.08       | 0.03       | 0.09       |
|          | Time  | 24.23      | 24.1       | 25.53     | 30.19      | 28.06      | 24.06      | 40.31      | 31.93      | 33.08      |
|          | Value | 28247      | 15526      | 14095     | 30093      | 14394      | 22364      | 22225      | 49412      | 26402      |
|          | Gap   | 0.83       | 2.91       | 5.99      | 2.55       | 4.39       | 2.09       | 3.18       | 1.08       | 3.21       |
| AM       | Time  | 3.95       | 3.45       | 3.89      | 4.34       | 4.31       | 4.12       | 4.69       | 5.53       | 6          |
| AM       | Value | 73578      | 52410      | 60056     | 74536      | 70266      | 70046      | 86097      | 99533      | 101705     |
|          | Gap   | 0.30       | 0.26       | 0.23      | 0.17       | 0.23       | 0.28       | 0.25       | 0.33       | 0.18       |
|          | Time  | 11.24      | 11.13      | 10.97     | 12.49      | 12.54      | 14.61      | 15.08      | 17.47      | 16.96      |
|          | Value | 35879      | 18948      | 15848     | 35966      | 16579      | 27183      | 25716      | 63848      | 28430      |
|          | Gap   | 0.84       | 0.75       | 0.5       | 0.79       | 1.08       | 0.4        | 0.35       | 0.28       | 0.82       |
|          | Time  | 4.81       | 4.83       | 4.32      | 4.68       | 5.48       | 5.15       | 5.26       | 6.1        | 6.19       |
|          | Value | 50934      | 26176      | 20020     | 51904      | 28382      | 29631      | 27697      | 61107      | 43872      |
|          |       | 27591      | 14971      | 13332     | 28940      | 13590      | 21220      | 20557      | 47812      | 24145      |
|          |       | X-n101-k25 | X-n110-k13 | X-n120-k6 | X-n129-k18 | X-n139-k10 | X-n153-k22 | X-n176-k26 | X-n186-k15 | X-n195-k51 |

## TABLE VII Reviewed Papers About Learning-Based Optimization Algorithms

![Image](image_000012_bfafb10a1ebd1f934a5f9139a6f48d0bdf4988bc81cef463c395864f64e62db9.png)

|                 | LBO                  | 45.7 %                   | p                            | p                       | p p        | p p         |                                | p                         | p                     | p            | p              | p                   | p           |             | p                       | p            | p               |                          |             | p    |    |
|-----------------|----------------------|--------------------------|------------------------------|-------------------------|------------|-------------|--------------------------------|---------------------------|-----------------------|--------------|----------------|---------------------|-------------|-------------|-------------------------|--------------|-----------------|--------------------------|-------------|------|----|
| Baselines       | Heuristics           | 75.70 %                  | p p p                        | p                       | p          | p           | p p                            | p                         |                       |              | p              | p                   | p p         | p           | p                       | p            | p               |                          |             | p    |    |
|                 | Solver               | 52.90 %                  | p                            |                         | p          | p           |                                | p                         | p                     | p            |                |                     |             | p           | p                       |              | p               |                          | p           |      |    |
| Learning manner | RL                   | 70.00 %                  | p p                          | p                       |            | p           | p p                            |                           |                       | p            | p              |                     | p           | p           |                         | p p          |                 | p                        | p           |      |    |
|                 | SL                   | 32.90 %                  |                              | p                       | p          |             |                                | p p                       | p                     | p            | p              | p                   |             |             | p                       |              |                 |                          | p           |      |    |
| Benchmarks      | Simulation Real data | % 60.00 %                | p p p p                      | p                       | p p        | p           | p                              | p p p                     |                       | p p          |                | p                   | p p         | p           | p p                     | p            |                 | p                        | p           | p    |    |
| Objective       | Length Time Cost     | % 8.60 % 21.40 % 51.40 p | p p                          | p p                     | p          | p           | p p p                          | p p                       |                       | p p          | p p            | p p                 |             | p p p       | p p                     | p            | p               |                          | p           | p    |    |
| Approaches      | E2E SbS              | 65.70 % 34.30 % 75.70    | p p                          | p p                     | p          | p p         | p p                            | p p                       | p                     | p            | p p            | p p                 | p           | p           | p p                     | p            | p               |                          | p           | p    |    |
| Reference types | Conference Journal   | % 67.10 %                | p                            | p                       |            | p p         | p p                            | p p                       | p                     |              | p              | p                   | p           |             |                         | p p          |                 | p                        | p           | p    |    |
| Problem         |                      | 32.90 p                  | TSP TSP p                    | TSP CVRP, DVRP          | p          |             | TSP                            | TSP p                     | OVRP                  | TSP          | MTSP p         | TSP ATSP p          |             | TSP p       |                         | p            | CVRP TSP Multi- | objective route planning | TSP         | CVRP |    |
|                 |                      |                          |                              |                         | VRPTW      | TSP TSP     | CVRP                           | EMVRP                     |                       |              |                | [114] TSP,          | CVRP        | CVRP        | TSP                     | CVRP         |                 | [80]                     | et al. [77] | [81] |    |
| Reference       |                      | of papers                | Junior et al. [61] Zeng [63] | et al. [76] et al. [91] | Özcan [92] | et al. [85] | Bello et al. [105] et al. [97] | and Wolf [106] Cooray and | [67] et al. [93]      | et al. [107] | and Wolf       | et al. [119] et al. | et al. [68] | et al. [62] | Phiboonbanakit al. [64] | et al. [94]  | [117]           |                          |             | Tian |    |
| Year            |                      | %                        | Lima Liu and                 | Rianchi Meignan         | Asta and   | Vinyals     | Martin Dai                     | et al. [112] Levy         | Rupasinghe Tyasnurita |              | Kaempfer [109] |                     |             |             |                         | et al. [124] | et al.          | et al.                   |             | and  |    |
|                 |                      |                          | 2007                         |                         | 2010 2014  | 2015        | 2016                           |                           |                       | Deudon       |                | Groshev Ottoni      | Al-Duoli    | Alipour     | et Kerschke             | Nazari       | Kool            | Yao                      | Yang        | Chen |    |
|                 |                      |                          |                              | 2009                    |            |             |                                | 2017                      |                       |              |                |                     | 2018        |             |                         |              |                 |                          |             |      |    |

TABLE VII (continued)

## Reviewed Papers About Learning-Based Optimization Algorithms

![Image](image_000013_1c5b6b98168f0bd3fae5a19bf335742250ceb90eac8785df967a453487ee2362.png)

| Baselines       | LBO                  | 45.7 %                 | p p                                                 |                       | p            | p                    | p                   | p p                        | p                  | p               | p p          | p         | p            |           | p p   |       | p   |     |     |    |             |       |       |    | p   |       |         |       |    |         |    | p   |    |    |
|-----------------|----------------------|------------------------|-----------------------------------------------------|-----------------------|--------------|----------------------|---------------------|----------------------------|--------------------|-----------------|--------------|-----------|--------------|-----------|-------|-------|-----|-----|-----|----|-------------|-------|-------|----|-----|-------|---------|-------|----|---------|----|-----|----|----|
| Learning manner | Solver Heuristics    | 52.90 % 75.70 %        | p p p                                               |                       | p p p        | p p p                | p p p               | p p                        |                    | p p             | p            | p         |              |           | p     | p     |     |     |     | p  |             |       | p     | p  | p   |       |         |       |    |         |    | p   |    |    |
|                 | SL RL                | 32.90 % 70.00 %        | p p                                                 | p                     | p            | p p                  | p p p p             | p p                        | p p                | p p             | p p p        | p         | p            |           | p     | p     |     |     |     |    |             |       |       |    |     |       |         |       |    |         |    | p   |    |    |
| Benchmarks      | Real data            | 60.00 %                | p p p p                                             | p p                   | p            | p p p                | p p                 | p p                        | p                  | p               |              |           | p            |           |       | p     |     | p p |     |    |             |       |       | p  |     |       |         |       |    |         |    |     |    |    |
| Objective       | Time Cost Simulation | 51.40 %                | p p                                                 | p p                   | p            | p p                  | p p                 | p p                        | p p p              | p p             | p p          | p p       |              |           |       | p     |     | p   |     |    |             |       |       |    | p   |       |         |       |    |         |    |     |    |    |
|                 | E2E SbS Length       | 75.70 % 8.60 % 21.40 % | p p p p p                                           | p                     | p            | p                    | p p                 | p p                        | p p p              | p p             |              |           | p            |           |       |       |     | p   |     |    |             | p     |       |    |     |       |         | p     |    |         |    |     |    |    |
| Approaches      |                      | % 34.30 %              | p p p p                                             | p                     | p p p p      | p                    | p p                 |                            | p p                | p               | p            |           | p p          |           |       |       |     |     |     |    |             |       | p     | p  |     |       |         |       |    |         |    |     |    |    |
| Reference types | Conference Journal   | 32.90 % 67.10 %        | p p                                                 | p                     |              | TSPTW route planning | SVRP Online route p |                            |                    | VRPTW           | TSP          |           | DS-CVRP,     |           |       |       |     |     |     |    |             |       |       |    |     |       |         |       |    |         |    |     |    | p  |
| Problem         |                      |                        | TSP TSP SDVRP, VRPTW                                | MDVDRP packet problem | routing TSP, | Online               | planning CMVRP      | VRPTW CVRP                 | TSPTWR             | CVRP, TSP CVRP, | CVRP,        | CVRP, TSP | DS-CVRPTW    | TSP       | CVRP, |       |     |     | TSP |    |             | CVRP, |       |    |     | CVRP  |         | SDVRP |    |         |    |     |    |    |
| Reference       |                      | % of papers            | et al. [120] et al. [121] et al. [150] et al. [151] | et al. [144]          | et al. [111] | et al. [140] Abad    | et al. [142]        | [96] Lu et al. [82]        | et al. [116] [130] | et al. [133]    | et al. [134] | al. [135] | et al. [136] | al. [122] |       |       |     |     |     |    |             | and   | [141] |    |     | [126] |         | [72]  |    |         |    |     |    |    |
|                 |                      |                        | Joshi Prates Kalakanti                              | Holler Mukhutdinov    | Ma           | Li                   |                     | al.                        | al. [115]          | and Tu [118]    |              |           | Bono         |           |       |       |     |     |     |    |             |       |       |    |     |       |         |       |    |         |    |     |    |    |
| Year            |                      |                        |                                                     |                       |              |                      | Balaji James et al. |                            |                    |                 |              |           |              |           |       |       |     |     |     |    |             |       |       |    |     |       |         |       |    |         |    |     |    |    |
|                 |                      |                        |                                                     |                       |              | 2019                 |                     |                            |                    | 2020            |              |           |              |           |       |       |     |     |     |    |             |       |       |    |     |       |         |       |    |         |    |     |    |    |
|                 |                      |                        |                                                     |                       |              |                      |                     |                            | Xing               |                 |              |           |              |           |       |       |     |     |     |    |             |       |       |    |     |       |         |       |    |         |    |     |    |    |
|                 |                      |                        |                                                     |                       |              |                      |                     |                            | Zhang              |                 | Zhao         |           |              |           |       |       |     |     |     |    |             |       |       |    |     |       |         |       |    |         |    |     |    |    |
|                 |                      |                        |                                                     |                       |              |                      | Vera                | Peng                       | Xin et             |                 |              |           |              |           |       |       |     |     |     |    |             |       |       |    |     |       |         |       |    |         |    |     |    |    |
|                 |                      |                        |                                                     |                       |              |                      |                     |                            | Fu et              |                 |              |           |              |           |       |       |     |     |     |    |             |       |       |    |     |       |         |       |    |         |    |     |    |    |
|                 |                      |                        |                                                     |                       |              |                      |                     | et                         |                    |                 |              |           |              |           |       |       |     |     |     |    |             |       |       |    |     |       |         |       |    |         |    |     |    |    |
|                 |                      |                        |                                                     |                       |              |                      |                     | Hottung Tierney            |                    |                 | Sultana      |           |              |           |       |       |     |     |     |    |             |       |       |    |     |       |         |       |    |         |    |     |    |    |
|                 |                      |                        |                                                     |                       |              |                      |                     | al.                        | Sultana et         |                 |              |           |              |           |       |       |     |     |     |    |             |       |       |    |     |       |         |       |    |         |    |     |    |    |
|                 |                      |                        |                                                     |                       |              |                      |                     |                            |                    | al.             |              |           |              | Drori et  |       |       |     |     |     |    |             |       |       |    |     |       |         |       |    |         |    |     |    |    |
|                 |                      |                        |                                                     |                       |              |                      | and [128]           | Gutierrez-Rodriguez et al. |                    |                 |              |           |              |           |       |       |     |     |     |    |             |       |       |    |     |       |         |       |    |         |    |     |    |    |
|                 |                      |                        |                                                     |                       |              |                      |                     |                            |                    |                 |              |           |              |           |       | Li et |     |     |     |    |             |       |       |    |     |       |         |       |    |         |    |     |    |    |
|                 |                      |                        |                                                     |                       |              |                      |                     |                            |                    |                 |              |           |              |           | TSP   |       |     |     |     |    | [108] MOTSP |       |       |    |     |       | p p p p |       | p  | p p p p | p  | p   | p  | p  |
|                 |                      | 65.70                  |                                                     | p p                   |              | p                    |                     |                            | p p                | p               |              |           |              |           |       |       |     |     |     | p  |             |       |       |    |     |       |         |       |    |         |    |     |    |    |

TABLE VII (continued)

## Reviewed Papers About Learning-Based Optimization Algorithms

![Image](image_000014_4825013d479b5d1f5d4a9cc8718f3f9f04f88d2fcd00dc02985bcd1b5f2e9b30.png)

Fig. 11.     Comparation of solution value of different algorithms on dataset 3.

![Image](image_000015_76320e9ad97f2bffc1feb8f459c706e72adda7c9bb7d0848b1bedc29e2eb5f99.png)

decomposed a sizable TSP into multiple small-scale subproblems  and  solved  each  of  them  using  the  SL  model,  and effectively solved the TSP with 10 000 nodes. Hence, decomposing complex  or sizable problems to multiple simple subproblems and solving them seems to be a feasible approach.

- 2) Combining with conventional algorithms to improve the generality of LBO models. Nickel et al. [156] illustrated that  the  relation  learned  by  an  LBO  model  in  knowledge graphs only makes sense when applied to entities of the right type. We also observed in our experiments that LBO models are  affected  by  data  distribution  while  combining  heuristics, as  search  operators  can  improve  the  generality  of  the  LBO algorithms. Hence, considering different forms of combination might be beneficial to guaranteeing a better generalization of LBO models. Using a solver to post-process solutions of LBO models [86] or using solutions generated by heuristic algorithms to pretrain LBO models [119] are all good research directions.
- 3)  Embedding  other  algorithms  to  increase  training efficiency of LBO models. Butler et al. [157] indicated that the  data  limitations  of  the  LBO  algorithms  must  be  solved. The  LBO  algorithms  frequently  require  sufficient  data  as  a training set, but it is difficult to obtain sufficient raw data of combinatorial optimization problems in the real world. What's more,  the  value  function  of  learning  models  is  randomly initialized,  and  models  would  select  a  random  action  with  a certain  probability  to  balance  exploration  and  exploitation. LBO models might require a large amount of time and data to train  but  still converge  to  a  suboptimal  solution.  Hence, embedding other algorithms to accelerate the convergence of the LBO  model  is  necessary.  For  example,  using  other algorithms computes the initial value function [125], [144] or generates the prior knowledge for the next action [118].
- 4) Building a generic framework for the LBO algorithms to  solve  VRPs. Cappart et  al. [158 ]  proposed  building  a generic  modeling  framework  for  LBO  algorithms  as  a  new direction; Wagstaff [159] also indicated that the LBO algorithms  have  not  yet  matured  such  that  researchers  from other  areas  can  simply  apply  them.  Although  many  LBO algorithms  have  been  proposed  to  solve  different  VRPs, encapsulating  different  LBO  algorithms  as  a  solver  is  still difficult  for  researchers.  Many  technical  difficulties  must  be solved  and  the  most  critical  step  is  to  define  a  generic modeling framework to provide upper-application with uniform interface.

## References

- T.  Pinto,  C.  Alves,  and  J.  V.  de  Carvalho, ' Models  and  advanced optimization  algorithms  for  the  integrated  management  of  logistics operations,' in Proc. IO2017 , Valença, Portugal, pp. 313-324. [1]
- R.  S.  Khan,  J.  B.  Yang,  and  J.  Handl, ' Interactive  multi-objective vehicle  routing  via  GA-based  dynamic  programming,' in Proc.  Int. Conf. Transportation Information and Safety , Wuhan, China, 2015, pp. 318-322. [2]
- K. Dorling, J. Heinrichs, G. G. Messier, and S. Magierowski, 'Vehicle routing problems for drone delivery,' IEEE Trans. Syst. Man Cybern. Syst. , vol. 47, no. 1, pp. 70-85, Jan. 2017. [3]
- P. Toth and D. Vigo, 'An overview of vehicle routing problems,' in The Vehicle Routing Problem , P. Toth and D. Vigo, Eds. Philadelphia, USA: Society for Industrial and Applied Mathematics, 2002, pp. 1-26. [4]
- J.  F.  Cordeau,  G.  Laporte,  M.  W.  P.  Savelsbergh,  and  D.  Vigo, 'Vehicle routing,' Handb. Oper. Res. Manage. Sci. ,  vol. 14,  pp. 367428, 2007. [5]
- D.  Pisinger  and  S.  Ropke, ' A  general  heuristic  for  vehicle  routing problems,' Comput. Oper. Res. , vol. 34,  no. 8,  pp. 2403-2435,  Aug. 2007. [6]
- J.  Euchi  and  H.  Chabchoub, ' A  hybrid  tabu  search  to  solve  the heterogeneous fixed fleet vehicle routing problem,' Logist. Res. , vol. 2, no. 1, pp. 3-11, Jun. 2010. [7]
- M.  B.  Guzairov,  N.  I.  Yusupova,  O.  N.  Smetanina,  and  E.  Y. Rassadnikova, 'Models and algorithms for the vehicle routing problem with time windows and other conditions,' in Proc. 13th Int. ScientificTechnical Conf. Actual Problems of Electronics Instrument Engineering , Novosibirsk, Russia, 2016, pp. 412-416. [8]
- H.  Afaq  and  S.  Saini, ' A  novel  approach  to  solve  graph  based travelling salesman problem using particle swarm optimization technique,' in Proc. IEEE Int. Conf. Computational Intelligence and Computing Research , Coimbatore, India, 2012, pp. 1-4. [9]
- M. A. Mohammed, M. S. Ahmad, and S. A. Mostafa, 'Using genetic algorithm  in  implementing  capacitated  vehicle  routing  problem,' in Proc.  Int.  Conf.  Computer  &amp;  Information  Science , Kuala  Lumpur, Malaysia, 2012, pp. 257-262. [10]
- J.  H.  Wilck IV and T. M. Cavalier, 'A construction heuristic for the split delivery vehicle routing problem,' Am. J. Oper. Res. , vol. 2, no. 2, pp. 153-162, Jun. 2012. [11]
- U. Ritzinger, J. Puchinger, and R. F. Hartl, 'A survey on dynamic and stochastic  vehicle  routing  problems,' Int. J. Prod. Res. ,  vol. 54,  no. 1, pp. 215-231, Jan. 2016. [12]
- T. Jastrzab and A. Buchcik, 'Practical applications of smart delivery systems:  Mine  evacuation  as  an  example  of  rich  vehicle  routing problem,' Smart  Delivery  Systems ,  J.  Nalepa,  Ed.  Amsterdam,  The Netherlands: Elsevier, 2020, pp. 249-268. [13]
- J. K. Lenstra and A. H. G. R. Kan, 'Complexity of vehicle routing and scheduling problems,' Networks , vol. 11, no. 2, pp. 221-227, 1981. [14]
- B. Golden, S. Raghavan, and E. Wasil, The Vehicle Routing Problem: Latest  Advances  and  New  Challenges . New  York,  USA:  Springer, 2008. [15]
- J.  Mańdziuk, 'New shades of the vehicle routing problem: Emerging problem formulations and computational intelligence solution methods,' IEEE Trans. Emerg. Top. Comput. Intell. , vol. 3, no. 3, pp. 230-244, Jun. 2019. [16]
- A. O. Adewumi and O. J. Adeleke, 'A survey of recent advances in vehicle  routing problems,' Int. J. Syst. Assur. Eng. Manage. , vol. 9, no. 1, pp. 155-172, Feb. 2018. [17]
- A.  Dixit,  A.  Mishra,  and  A.  Shukla, ' Vehicle  routing  problem  with time windows using meta-heuristic algorithms: A survey,' in Harmony Search  and  Nature  Inspired  Optimization  Algorithms ,  N.  Yadav,  A. Yadav,  J.  C.  Bansal,  K.  Deep,  and  J.  H.  Kim,  Eds.  Singapore: Springer, 2019, pp. 539-546. [18]
- W. J.  Cao  and  W.  S.  Yang, ' A  survey  of  vehicle  routing  problem,' MATEC Web  Conf. ,  vol.  100,  p.  01006.  Mar.  2017.  DOI: 10.1051/ matecconf/201710001006. [19]
- R.  Goel  and  R.  Maini, ' Vehicle  routing  problem  and  its  solution [20]

- methodologies: A survey,' Int. J. Logist. Syst. Manage. , vol. 28, no. 4, pp. 419-435, Nov. 2017.
- K.  A.  Smith, ' Neural  networks  for  combinatorial  optimization:  A review  of  more  than  a  decade  of  research,' INFORMS J. Comput. , vol. 11, no. 1, pp. 15-34, Feb. 1999. [21]
- O. I. Abiodun, A. Jantan, A. E. Omolara, K. V. Dada, N. A. Mohamed, and H. Arshad, ' State-of-the-art in artificial neural network applications: A survey,' Heliyon , vol. 4, no. 11, p. E00938, Nov. 2018. DOI: 10.1016/j.heliyon.2018.e00938. [22]
- X. D. Zhang, 'Machine learning,' in A Matrix Algebra Approach to Artificial Intelligence , X. D. Zhang, Ed. Singapore: Springer, 2020, pp. 223-440. [23]
- I. El Naqa and M. J. Murphy, 'What is machine learning?' in Machine Learning in Radiation Oncology ,  I.  El  Naqa, R. J. Li, M. J. Murphy, Eds. Cham, Germany: Springer, 2015, pp. 3-11. [24]
- A.  François,  Q.  Cappart,  and  L.  M.  Rousseau, ' How  to  evaluate machine learning approaches for combinatorial optimization: Application to the travelling salesman problem,' arXiv preprint arXiv: 1909.13121, 2019. [25]
- R. B. Bai, X. N. Chen, Z. L. Chen, T. X. Cui, S. H. Gong, W. T. He, X. P. Jiang, H. Jin, J. H. Jin, G. Kendall, J. W. Li, Z. Lu, J. F. Ren, P. Weng, N. Xue, and H. Y. Zhang, 'Analytics and machine learning in vehicle routing research,' arXiv preprint arXiv: 2102.10012, 2021 [26]
- G.  B.  Dantzig  and  J.  H.  Ramser, ' The  truck  dispatching  problem,' Manage Sci , vol. 6, no. 1, pp. 80-91, Oct. 1959. [27]
- G. Clarke  and  J.  W.  Wright, ' Scheduling  of  vehicles  from  a  central depot  to  a  number  of  delivery  points,' Oper. Res. , vol. 12,  no. 4, pp. 568-581, Aug. 1964. [28]
- G.  Laporte, ' What  you  should  know  about  the  vehicle  routing problem,' Naval Res. Logist. , vol. 54, no. 8, pp. 811-819, Dec. 2007. [29]
- G.  Laporte, ' Fifty  years  of  vehicle  routing,' Transport Sci ,  vol. 43, no. 4, pp. 408-416, Oct. 2009. [30]
- R.  Fukasawa,  H.  Longo,  J.  Lysgaard,  M.  P.  de  Aragão,  M.  Reis,  E. Uchoa, and R. F. Werneck, 'Robust branch-and-cut-and-price for the capacitated vehicle routing problem,' Math. Program. ,  vol. 106, no. 3, pp. 491-511, May 2006. [31]
- M.  Junger,  G.  Reinelt,  and  G.  Rinaldi, ' The  traveling  salesman problem,' Handb. Oper. Res. Manage. Sci. , vol. 7, pp. 225-330, 1995. [32]
- P.  Toth  and  D.  Vigo, ' Models,  relaxations  and  exact  approaches  for the capacitated vehicle routing problem,' Discrete Appl. Math. , vol. 123, no. 1-3, pp. 487-512, Nov. 2002. [33]
- M. Fisher, 'Vehicle routing,' Handb. Oper. Res. Manage. Sci. , vol. 8, pp. 1-33, 1995. [34]
- J.  Caceres-Cruz,  P.  Arias,  D.  Guimarans,  D.  Riera,  and  A.  A.  Juan, 'Rich vehicle routing problem: Survey,' ACM Comput. Surv. , vol. 47, no. 2, Article No. 32, Jan. 2015. [35]
- G. Laporte and Y. Nobert, 'Exact algorithms for the vehicle routing problem,' North-Holland Math. Stud. , vol. 132, pp. 147-184, 1987. [36]
- G. Laporte, H. Mercure, and Y. Nobert, 'An exact algorithm for the asymmetrical capacitated vehicle routing problem,' Networks , vol. 16, no. 1, pp. 33-46, Mar. 1986. [37]
- S. Eilon, C. D. T. Watson-Gandy, N. Christofides, and R. de Neufville, 'Distribution management-mathematical modelling and practical analysis,' IEEE Trans. Syst. Man Cybern. , vol. SMC-4, no. 6, pp. 589-589, Nov. 1974. [38]
- M.  R.  Rao  and  S.  Zionts, ' Allocation  of  transportation  units  to alternative trips-a column  generation scheme  with out-of-kilter subproblems,' Oper. Res. , vol. 16, no. 1, pp. 52-63, Feb. 1968. [39]
- N. Christofides and S. Eilon, 'An algorithm for the vehicle-dispatching problem,' J. Oper. Res. Soc. , vol. 20, no. 3, pp. 309-318, Sep. 1969. [40]
- N. Christofides, A. Mingozzi, and P. Toth, 'Exact algorithms for the vehicle  routing  problem,  based  on  spanning  tree  and  shortest  path relaxations,' Math. Program. , vol. 20, no. 1, pp. 255-282, Dec. 1981. [41]
- B. L. Golden, E. A. Wasil, J. P. Kelly, and I. M. Chao, 'The impact of metaheuristics  on  solving  the  vehicle  routing  problem:  Algorithms, problem  sets,  and  computational  results,' in Fleet  Management  and Logistics , T.  G.  Crainic  and  G.  Laporte,  Eds.  New  York,  USA: [42]
- Springer, 1998, pp. 33-56.
- S. N. Kumar and R. Panneerselvam, 'A survey on the vehicle routing problem and its variants,' Intell. Inf. Manage. , vol. 4, no. 3, pp. 66-74, May 2012. [43]
- J. F. Cordeau and G. Laporte, 'Tabu search heuristics for the vehicle routing problem,' Metaheuristic Optimization via Memory and Evolution ,  R.  Sharda, S. Voß, C. Rego, and B. Alidaee, Eds. Boston, USA: Springer, 2005, pp. 145-163. [44]
- G.  A.  P.  Kindervater  and  M.  W.  P.  Savelsbergh, ' Vehicle  routing: Handling edge exchanges,' Local Search in Combinatorial Optimization , E. Aarts and J. K. Lenstra, Eds. New York, USA: Wiley, 1997. [45]
- S. Desale, A. Rasool, S. Andhale, and P. Rane, 'Heuristic and metaheuristic algorithms and their relevance to the real world: A survey,' Int. J. Comput. Eng. Res. Trends , vol. 351, no. 5, pp. 2349-7084, 2015. [46]
- G. Laporte and F. Semet, 'Classical heuristics for the capacitated vrp,' in The  Vehicle Routing Problem , P. Toth and D. Vigo, Eds. Philadelphia,  USA:  Society  for  Industrial  and  Applied  Mathematics, 2002, pp. 109-128. [47]
- T.  O.  Ayodele, ' Types  of  machine  learning  algorithms,' in New Advances in Machine Learning ,  Y.  G.  Zhang, Ed. IntechOpen, 2010, pp. 19-48. [48]
- C. M. Bishop, Pattern Recognition and Machine Learning . New York, USA: Springer, 2006. [49]
- L.  P.  Kaelbling,  M.  L.  Littman,  and  A.  W.  Moore, 'Reinforcement learning: A survey,' J. Artif. Intell. Res. , vol. 4, no. 1, pp. 237-285, Jun. 1996. [50]
- R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction . 2nd ed. Cambridge, USA: MIT Press, 2018. [51]
- J. C. Yan, S. Yang, and E. Hancock, 'Learning for graph matching and related combinatorial optimization problems,' in Proc. 29th Int. Joint Conf. Artificial Intelligence , Yokohama, Japan, 2020, pp. 4988-4996. [52]
- B. Mahesh, 'Machine learning algorithms-a review,' Int. J. Sci. Res. , vol. 9, no. 1, pp. 381-386, Jan. 2020. [53]
- M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, 'The arcade learning environment: An evaluation platform for general agents,' J. Artif. Intell. Res. , vol. 47, pp. 253-279, Jun. 2013. [54]
- D. Silver,  A.  Huang,  C.  J.  Maddison,  A.  Guez,  L.  Sifre,  G.  van  den Driessche,  J.  Schrittwieser,  I.  Antonoglou,  V.  Panneershelvam,  M. Lanctot,  S. Dieleman,  D.  Grewe,  J.  Nham,  N.  Kalchbrenner,  I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis, 'Mastering the game of go with deep neural networks and tree search,' Nature , vol. 529, no. 7587, pp. 484-489, Jan. 2016. [55]
- S. Levine, C. Finn, T. Darrell, and P. Abbeel, 'End-to-end training of deep  visuomotor  policies,' J. Mach.  Learn.  Res. , vol. 17, no. 1, pp. 1334-1373, Jan. 2016. [56]
- K. Xu, J. L. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. S. Zemel, and Y. Bengio, 'Show, attend and tell: Neural image caption generation with visual  attention,' in Proc. 32nd Int. Conf. Int.  Conf. Machine Learning , Lille, France, 2015, pp. 2048-2057. [57]
- H. D. Song, I. Triguero, and E. Özcan, 'A review on the self and dual interactions between machine learning and optimisation,' Prog. Artif. Intell. , vol. 8, no. 2, pp. 143-165, Jun. 2019. [58]
- M.  Karimi-Mamaghan,  M.  Mohammadi,  P.  Meyer,  A.  M.  KarimiMamaghan, and E. G. Talbi, 'Machine learning at the service of metaheuristics for solving combinatorial optimization problems: A state-ofthe-art,' Eur. J. Oper. Res. , vol. 296, no. 2, pp. 393-422, Jan. 2022. [59]
- L. M. Gambardella and M. Dorigo, ' Ant-Q: A  Reinforcement Learning  approach  to  the  traveling  salesman  problem,' in Machine Learning Proceedings 1995 , A. Prieditis and S. Russell, Eds. Amsterdam, The Netherlands: Elsevier, 1995, pp. 252-260. [60]
- F. C. de Lima Junior, J. D. de Melo, and A. D. D. Neto, 'Using Qlearning algorithm for initialization  of  the  GRASP  metaheuristic  and genetic algorithm,' in Proc. Int. Joint Conf. Neural Networks , Orlando, USA, 2007, pp. 1243-1248. [61]
- M. M. Alipour, S. N. Razavi, M. R. F. Derakhshi, and M. A. Balafar, 'A  hybrid algorithm using a genetic algorithm and multiagent reinforcement learning heuristic to solve the traveling salesman [62]

- problem,' Neural Comput. Appl. ,  vol. 30,  no. 9,  pp. 2935-2951,  Nov. 2018.
- F. Liu and G. Z. Zeng, 'Study of genetic algorithm with reinforcement learning to solve the TSP,' Exp. Syst. Appl. , vol. 36, no. 3, pp. 6995-7001, Apr. 2009. [63]
- T.  Phiboonbanakit,  T.  Horanont,  T.  Supnithi,  and  V.  N.  Huynh, 'Knowledge-based  learning  for  solving  vehicle  routing  problem,' in Proc.  ACM  Int.  Joint  Conf.  and  2018  Int.  Symp.  Pervasive  and Ubiquitous Computing and Wearable Computers , Singapore, 2018, pp. 1103-1111. [64]
- J. Y. Ding, C. Zhang, L. Shen, S. Y. Li, B. Wang, Y. H. Xu, and L. Song, ' Accelerating primal solution findings for mixed integer programs  based  on  solution  prediction,' in Proc.  AAAI  Conf.  Artif. Intell. , vol. 34, no. 2, pp. 1452-1459, Apr. 2020. [65]
- Y. Sun, A. Ernst, X. D. Li, and J. Weiner, 'Generalization of machine learning  for  problem  reduction:  A  case  study  on  travelling  salesman problems,' OR Spectrum , vol. 43, no. 3, pp. 607-633, Sep. 2021. [66]
- P.  L.  N.  U.  Cooray  and  T.  D.  Rupasinghe, ' Machine  learning-based parameter  tuned  genetic  algorithm  for  energy  minimizing  vehicle routing problem,' J. Ind. Eng. , vol. 2017, Jan. 2017. [67]
- F.  Al-Duoli,  G.  Rabadi,  M.  Seck,  and  H.  A.  Handley, 'Hybridizing meta-raps with machine learning algorithms,' in Proc. IEEE Technology  and  Engineering  Management  Conf. , Evanston,  USA, 2018, pp. 1-6. [68]
- P.  Shaw, ' A  new  local  search  algorithm  providing  high  quality solutions  to  vehicle  routing  problems,'  University  of  Strathclyde, Glasgow, Scotland, 1997. [69]
- G.  Schrimpf,  J.  Schneider,  H.  Stamm-Wilbrandt,  and  G.  Dueck, 'Record  breaking  optimization  results  using  the  ruin  and  recreate principle,' J. Comput. Phys. , vol. 159, no. 2, pp. 139-171, Apr. 2000. [70]
- D. Pisinger and S. Ropke, 'Large neighborhood search,' in Handbook of  Metaheuristics ,  M.  Gendreau  and  J.  Y.  Potvin,  Eds.  New  York, USA: Springer, 2010, pp. 399-419. [71]
- A. Hottung and K. Tierney, 'Neural large neighborhood search for the capacitated  vehicle  routing  problem,' Proc.  24th  European  Conf. Artificial Intelligence , Santiago de Compostela, Spain, 2020. [72]
- M. X.  Chen,  L.  Gao,  Q.  C.  Chen,  and  Z.  X.  Liu, ' Dynamic  partial removal:  A  neural  network  heuristic  for  large  neighborhood  search,' arXiv preprint arXiv: 2005.09330, 2020. [73]
- L. Gao, M. X. Chen, Q. C. Chen, G. Z. Luo, N. Y. Zhu, and Z. X. Liu, 'Learn  to  design  the  heuristics  for  vehicle  routing  problem,' arXiv preprint arXiv: 2002.08539, 2020. [74]
- M. M. Solomon, 'Algorithms for the vehicle routing and scheduling problems  with  time  window  constraints,' Oper. Res. , vol. 35,  no. 2, pp. 254-265, Apr. 1987. [75]
- R.  A.  C.  Bianchi,  C.  H.  C.  Ribeiro,  and  A.  H.  R.  Costa, ' On  the relation between ant colony optimization and heuristically accelerated reinforcement learning,' in Proc. 1st Int. Workshop on Hybrid Control of Autonomous System , Pasadena, USA, 2009, pp. 49-55. [76]
- F.  D.  Yang,  T.  C.  Jin,  T.  Y.  Liu,  X.  M.  Sun,  and  J.  L.  Zhang, 'Boosting  dynamic  programming  with  neural  networks  for  solving NP-hard  problems,' in Proc.  10th  Asian  Conf.  Machine  Learning , Beijing, China, 2018, pp. 726-739. [77]
- W. Joe and H. C. Lau, 'Deep reinforcement learning approach to solve dynamic vehicle routing problem with stochastic customers,' in Proc. Int.  Conf.  Autom.  Plann.  Sched. ,  vol.  30,  no.  1,  pp.  394-402,  Jun. 2020. [78]
- A.  Delarue,  R.  Anderson,  and  C.  Tjandraatmadja, 'Reinforcement learning with combinatorial actions: An application to vehicle routing,' arXiv preprint arXiv: 2010.12001, 2020. [79]
- Y. Yao, Z. Peng, and B. Xiao, 'Parallel hyper-heuristic algorithm for multi-objective  route  planning  in  a  smart  city,' IEEE Trans. Veh. Technol. , vol. 67, no. 11, pp. 10307-10318, Nov. 2018. [80]
- X. Y. Chen and Y. D. Tian, 'Learning to perform local rewriting for combinatorial optimization,' in Proc. 33rd Int. Conf. Neural Information Processing Systems , Vancouver, Canada, 2019, pp. 6281-6292. [81]
- H. Lu, X. W. Zhang, and S. Yang, 'A learning-based iterative method [82]
- for solving vehicle routing problems,' in Proc. 8th Int. Conf Learning Representations , Addis Ababa, Ethiopia, 2020.
- P.  R.  de  O.  da  Costa,  J.  Rhuggenaath,  Y.  Q.  Zhang,  and  A.  Akcay, 'Learning 2-opt heuristics for the traveling salesman problem via deep reinforcement learning,' in Proc. 12th Asian Conf. Machine Learning , Bangkok, Thailand, 2020, pp. 465-480. [83]
- Y.  X.  Wu,  W.  Song,  Z.  G.  Cao,  J.  Zhang,  and  A.  Lim, 'Learning improvement  heuristics  for  solving  routing  problems,' IEEE  Trans. Neural Netw. Learn. Syst. , 2021, DOI: 10.1109/TNNLS.2021.3068828 [84]
- O. Vinyals, M. Fortunato, and N. Jaitly, 'Pointer networks,' in Proc. 28th  Int.  Conf.  Neural  Information  Processing  Systems , Montreal, Canada, 2015, pp. 692-2700. [85]
- M. V. Vlastelica,  A.  Paulus,  V.  Musil,  G.  Martius,  and  M.  Rolínek, 'Differentiation of blackbox combinatorial solvers,' in Proc. 8th Int. Conf. Learning Representations , Addis Ababa, Ethiopia, 2020. [86]
- Y. N. Ma, J. W. Li, Z. G. Cao, W. Song, L. Zhang, Z. H. Chen, and J. Tang, 'Learning to iteratively solve routing problems with dual-aspect collaborative  transformer,' in Proc.  35th  Conf.  Neural  Information Processing Systems , Sydney, Australia, 2021. [87]
- W.  Qin,  Z.  L.  Zhuang,  Z.  Z.  Huang,  and  H.  Z.  Huang, ' A  novel reinforcement learning-based hyper-heuristic for heterogeneous vehicle routing problem,' Comput. Ind. Eng , vol. 156, p. 107252, Jun. 2021. DOI: 10.1016/j.cie.2021.107252. [88]
- J. Mlejnek and J. Kubalik, ' Evolutionary hyperheuristic for capacitated  vehicle  routing  problem,' in Proc.  15th  Annu.  Conf. Companion  on  Genetic  and  Evolutionary  Computation ,  Amsterdam, The Netherlands, 2013, pp. 219-220. [89]
- M.  Okulewicz  and  J.  Mańdziuk, ' A  particle  swarm  optimization hyper-heuristic for the dynamic  vehicle  routing  problem,' arXiv preprint arXiv: 2006.08809, 2020. [90]
- D. Meignan, A. Koukam, and J. C. Créput, 'Coalition-based metaheuristic: A self-adaptive metaheuristic using reinforcement learning and mimetism,' J. Heuristics , vol. 16, no. 6, pp. 859-879, Dec. 2010. [91]
- S. Asta and E. Özcan, 'An apprenticeship learning hyper-heuristic for vehicle  routing  in  HyFlex,' in Proc.  IEEE  Symp.  Evolving  and Autonomous Learning Systems , Orlando, USA, 2014, pp. 65-72. [92]
- R.  Tyasnurita,  E.  Özcan,  and  R.  John, ' Learning  heuristic  selection using a time delay neural network for open vehicle routing,' in Proc. IEEE Congr.  Evolutionary  Computation ,  Donostia,  Spain,  2017,  pp. 1474-1481. [93]
- P. Kerschke, L. Kotthoff, J. Bossek, H. H. Hoos, and H. Trautmann, 'Leveraging TSP solver complementarity through machine learning,' Evol. Comput. , vol. 26, no. 4, pp. 597-620, Dec. 2018. [94]
- K. F. Zhao, S. C. Liu, J. X. Yu, and Y. Rong, 'Towards feature-free TSP solver selection:  A  deep  learning  approach,' in Proc.  Int.  Joint Conf. Neural Networks , Shenzhen, China, 2021, pp. 1-8. [95]
- A. E.  Gutierrez-Rodríguez,  S.  E.  Conant-Pablos,  J.  C.  Ortiz-Bayliss, and H. Terashima-Marín, ' Selecting meta-heuristics for solving vehicle routing problems  with  time  windows  via  meta-learning,' Expert Syst. Appl. , vol. 118, pp. 470-481, Mar. 2019. [96]
- S. Martin, D. Ouelhadj, P. Beullens, E. Ozcan, A. A. Juan, and E. K. Burke, 'A multi-agent based cooperative approach to scheduling and routing,' Eur. J. Oper. Res. , vol. 254, no. 1, pp. 169-178, Oct. 2016. [97]
- Z.  C.  Lipton,  J.  Berkowitz,  and  C.  Elkan, ' A  critical  review  of recurrent  neural networks  for  sequence  learning,'  arXiv  preprint arXiv: 1506.00019, 2015. [98]
- J.  J.  Hopfield, 'Neural networks and physical systems with emergent collective computational abilities,' Proc. Natl. Acad. Sci. USA , vol. 79, no. 8, pp. 2554-2558, Apr. 1982. [99]
- S. Hochreiter and J. Schmidhuber, 'Long short-term memory,' Neural Comput. , vol. 9, no. 8, pp. 1735-1780, Nov. 1997. [100]
- M.  Schuster and K. K. Paliwal, ' Bidirectional recurrent neural networks,' IEEE Trans. Signal Process. , vol. 45, no. 11, pp. 26732681, Nov. 1997. [101]
- I. Sutskever, O. Vinyals, and Q. V. Le, 'Sequence to sequence learning with  neural  networks,' in Proc.  27th  Int.  Conf.  Neural  Information Processing Systems , Montreal, Canada, 2014, pp. 3104-3112. [102]

- A.  Karpathy  and  L.  Fei-Fei, ' Deep  visual-semantic  alignments  for generating image descriptions,' in Proc. IEEE Conf. Computer Vision and Pattern Recognition , Boston, USA: IEEE, 2015, pp. 3128-3137. [103]
- D. Bahdanau, K. Cho, and Y. Bengio, 'Neural machine translation by jointly learning to align and translate,' arXiv preprint arXiv: 1409.0473, 2014. [104]
- I.  Bello,  H.  Pham,  Q.  V.  Le,  M.  Norouzi,  and  S.  Bengio, 'Neural combinatorial optimization with reinforcement learning,' arXiv preprint arXiv: 1611.09940, 2016. [105]
- D.  Levy  and  L.  Wolf, ' Learning  to  align  the  source  code  to  the compiled  object  code,' in Proc.  34th  Int.  Conf.  Machine  Learning , Sydney, Australia, 2017, pp. 2043-2051. [106]
- M.  Deudon,  P.  Cournut,  A.  Lacoste,  Y.  Adulyasak,  and  L.  M. Rousseau, ' Learning  heuristics  for  the  TSP  by  policy  gradient,' in Proc. 15th Int. Conf. Integration of Constraint Programming, Artificial Intelligence, and Operations Research ,  Delft, The Netherlands, 2018, pp. 170-181. [107]
- K. W. Li, T. Zhang, and R. Wang, 'Deep reinforcement learning for multiobjective optimization,' IEEE Trans. Cybern. , vol. 51, no. 6, pp. 3103-3114, Jun. 2021. [108]
- Y. Kaempfer and L. Wolf, 'Learning the multiple traveling salesmen problem with permutation invariant pooling networks,' arXiv preprint arXiv: 1803.09621, 2018. [109]
- A. V. Le, P. Veerajagadheswar, P. Thiha Kyaw, M. R. Elara, and N. H. K. Nhan, 'Coverage path planning using reinforcement learning-based TSP for htetran-a polyabolo-inspired self-reconfigurable tiling robot,' Sensors ,  vol.  21,  no.  8,  p.  2577,  Apr.  2021.  DOI: 10.3390/ s21082577. [110]
- Q. Ma, S. W. Ge, D. Y. He, D. Thaker, and I. Drori, 'Combinatorial optimization by graph pointer networks and hierarchical reinforcement learning,' arXiv preprint arXiv: 1911.04936, 2019. [111]
- H.  J.  Dai,  E.  B.  Khalil,  Y.  Y.  Zhang,  B.  Dilkina,  and  L.  Song, 'Learning  combinatorial  optimization  algorithms  over  graphs,' in Proc.  31st  Int.  Conf.  Neural  Information  Processing  Systems ,  Long Beach, USA, 2017, pp. 6351-6361. [112]
- H. J. Dai, B. Dai, and L. Song, 'Discriminative embeddings of latent variable models for structured data,' in Proc. 33rd Int. Conf. Int. Conf. Machine Learning , New York, USA, 2016, pp. 2702-2711. [113]
- A.  L.  C.  Ottoni,  E.  G.  Nepomuceno,  and  M.  S.  de  Oliveira, 'A response surface model approach to parameter estimation of reinforcement learning for the travelling salesman problem,' J. Control Autom. Electr. Syst. , vol. 29, no. 3, pp. 350-359, Jun. 2018. [114]
- Z. H. Fu, K. B. Qiu, and H. Y. Zha, 'Generalize a small pre-trained model  to  arbitrarily  large  tsp  instances,' in Proc.  AAAI  Conf.  Artif. Intell. , vo. 5, no. 8, pp. 7474-7482, May 2020. [115]
- R. K. Zhang, A. Prokhorchuk, and J. Dauwels, 'Deep reinforcement learning  for  traveling  salesman  problem  with  time  windows  and rejections,' in Proc. Int. Joint Conf. Neural Networks ,  Glasgow, UK, 2020, pp. 1-8. [116]
- W.  Kool,  H.  van  Hoof,  and  M.  Welling, ' Attention,  learn  to  solve routing problems!' arXiv preprint arXiv: 1803.08475, 2018. [117]
- Z. H. Xing and S. K. Tu, 'A graph neural network assisted monte carlo tree  search  approach  to  traveling  salesman  problem,' IEEE Access , vol. 8, pp. 108418-108428, Jan. 2020. [118]
- E.  Groshev,  A.  Tamar,  M.  Goldstein,  S.  Srivastava,  and  P.  Abbeel, 'Learning  generalized  reactive  policies  using  deep  neural  networks,' in Proc. 28th AAAI Spring Symposia, Stanford University ,  Palo Alto, USA, 2018. [119]
- C.  K. Joshi, T. Laurent, and X. Bresson, ' An  efficient graph convolutional network technique for the travelling salesman problem,' arXiv preprint arXiv: 1906.01227, 2019. [120]
- M. Prates, P. H. C. Avelar, H. Lemos, L. C. Lamb, and M. Y. Vardi, 'Learning to solve NP-complete problems: A graph neural network for decision TSP,' in Proc. 29th AAAI Conf. Artif. Intell. ,  vol.  33, no. 1, pp. 4731-4738, Jul. 2019. [121]
- N. Sultana, J. Chan, T. Sarwar, and A. K. Qin, 'Learning to optimise general TSP instances,' arXiv preprint arXiv: 2010.12214, 2020 [122]
- C. K. Joshi, T. Laurent, and X. Bresson, 'On learning paradigms for the  travelling  salesman  problem,'  arXiv  preprint  arXiv:  1910.07210, 2019. [123]
- M. Nazari, A. Oroojlooy, M. Takáč, and L. V. Snyder, 'Reinforcement learning for solving the vehicle routing problem,' in Proc. 32nd Int. Conf. Neural Information Processing Systems , Montréal Canada, 2018, pp. 9861-9871. [124]
- A. A.  Ibrahim,  N.  Lo,  R.  O.  Abdulaziz,  and  J.  Ishaya, 'Capacitated vehicle routing problem,' Int. J. Res. Granth. , vol. 7, no. 3, pp. 310-327, Mar. 2019. [125]
- B. Peng, J. H. Wang, and Z. Z. Zhang, 'A deep reinforcement learning algorithm using dynamic attention model for vehicle routing problems,' in Proc. 11th Int. Symp. Artificial Intelligence Algorithms and Applications . Guangzhou, China, 2019, pp. 636-650. [126]
- L. Duan, Y. Zhan, H. Y. Hu, Y. Gong, J. W. Wei, X. D. Zhang, and Y. H. Xu, ' Efficiently  solving  the  practical  vehicle  routing  problem:  A novel joint learning approach,' in Proc. 26th ACM SIGKDD Int. Conf. Knowledge Discovery &amp; Data Mining , 2020, pp. 3054-3063. [127]
- J. M. Vera and A. G. Abad, 'Deep reinforcement learning for routing a heterogeneous fleet of vehicles,' in Proc. IEEE Latin American Conf. Computational Intelligence , Guayaquil, Ecuador, 2019, pp. 1-6. [128]
- J.  K.  Falkner  and  L.  Schmidt-Thieme, ' Learning  to  solve  vehicle routing  problems  with  time  windows  through  joint  attention,' arXiv preprint arXiv: 2006.09100, 2020. [129]
- L. Xin, W. Song, Z. G. Cao, and J. Zhang, 'Step-wise deep learning models for solving routing problems,' IEEE Trans. Ind. Inf. ,  vol. 17, no. 7, pp. 4861-4871, Jul. 2020. [130]
- L. Xin, W. Song, Z. G. Cao, and, J. Zhang, 'Multi-decoder attention model with embedding glimpse for solving vehicle routing problems,' in Proc. 35th AAAI Conf. Artificial Intelligence , 2021, pp. 12042-12049. [131]
- K.  Zhang,  F.  He,  Z.  C.  Zhang,  X.  Lin,  and  M.  Li, 'Multi-vehicle routing problems with soft time windows: A multi-agent reinforcement learning approach,' Trans. Res. Part C Emerg. Technol. ,  vol.  121, p. 102861, Dec. 2020. DOI: 10.1016/j.trc.2020.102861. [132]
- J.  X.  Zhao,  M.  J.  Mao,  X.  Zhao,  and  J.  H.  Zou, ' A  hybrid  of  deep reinforcement learning and local search for the vehicle routing problems,' IEEE Trans. Intell. Transp. Syst. ,  vol. 22,  no. 11, pp. 72087218, Nov. 2021. [133]
- N.  Sultana,  J.  Chan,  A.  K.  Qin,  and  T.  Sarwar, ' Learning  vehicle routing  problems  using  policy  optimisation,'  arXiv  preprint  arXiv: 2012.13269, 2020. [134]
- I. Drori, A. Kharkar, W. R. Sickinger, B. Kates, Q. Ma, S. W. Ge, E. Dolev,  B.  Dietrich,  D.  P.  Williamson,  and  M.  Udell, ' Learning  to solve  combinatorial  optimization  problems  on  real-world  graphs  in linear  time,' in Proc.  19th  IEEE  Int.  Conf.  Machine  Learning  and Applications , Miami, USA, 2020, pp. 19-24. [135]
- G. Bono, J. S. Dibangoye, O. Simonin, L. Matignon, and F. Pereyron, 'Solving  multi-agent  routing  problems  using  deep  attention  mechanisms,' IEEE Trans. Intell. Transp. Syst. , vol. 22,  no. 12,  pp. 78047813, Dec. 2021. [136]
- B. Lin, B. Ghaddar, and J. Nathwani, 'Deep reinforcement learning for the electric vehicle routing problem with time windows,' IEEE Trans. Intell. Transp. Syst. , 2021, DOI: 10.1109/TITS.2021.3105232 [137]
- J. W.  Li,  L.  Xin,  Z.  G.  Cao,  A.  Lim,  W.  Song,  and  J.  Zhang, 'Heterogeneous attentions for solving pickup and delivery problem via deep reinforcement learning,' IEEE Trans. Intell. Transp. Syst. , vol. 23, no. 3, pp. 2306-2315, Mar. 2022. [138]
- J.  W. Li, Y. N. Ma, R. Z. Gao, Z. G. Cao, A. Lim, W. Song, and J. Zhang, ' Deep  reinforcement  learning  for  solving  the  heterogeneous capacitated  vehicle  routing  problem,' IEEE  Trans.  Cybern. , 2021, DOI: 10.1109/TCYB.2021.3111082 [139]
- J. L. Li, D. W. Fu, Q. Yuan, H. H. Zhang, K. H. Chen, S. Yang, and F. C. Yang, 'A traffic prediction enabled double rewarded value iteration network for route planning,' IEEE Trans. Veh. Technol. , vol. 68, no. 5, pp. 4170-4181, May 2019. [140]
- J. J. Q. Yu, W. Yu, and J. T. Gu, 'Online vehicle routing with neural combinatorial  optimization  and  deep  reinforcement  learning,' IEEE [141]

Trans. Intell. Transp. Syst. , vol. 20, no. 10, pp. 3806-3817, Oct. 2019.

- B. Balaji, J. Bell-Masterson, E. Bilgin, A. Damianou, P. M. Garcia, A. Jain,  R.  F.  Luo,  A.  Maggiar,  B.  Narayanaswamy,  and  C.  Ye, 'Orl: Reinforcement learning benchmarks for online stochastic optimization problems,' arXiv preprint arXiv: 1911.10641, 2019. [142]
- O.  M.  Andrychowicz,  B.  Baker,  M.  Chociej,  R.  Józefowicz,  B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider,  S.  Sidor,  J.  Tobin,  P.  Welinder,  L.  L.  Weng,  and  W. Zaremba, ' Learning  dexterous  in-hand  manipulation,' Int. J. Rob. Res. , vol. 39, no. 1, pp. 3-20, Jan. 2020. [143]
- D. Mukhutdinov, A. Filchenkov, A. Shalyto, and V. Vyatkin, 'Multiagent deep learning for simultaneous optimization for time and energy in  distributed  routing  system,' Future Gener. Comput. Syst. ,  vol. 94, pp. 587-600, May 2019. [144]
- R. RamachandranPillai and M. Arock, 'An adaptive spiking neural p system  for  solving  vehicle  routing  problems,' Arabian J. Sci. Eng. , vol. 45, no. 4, pp. 2513-2529, Apr. 2020. [145]
- Y. X. Sheng, H. W. Ma, and W. Xia, 'A pointer neural network for the vehicle routing problem with task priority and limited resources,' Inf Technol Control , vol. 49, no. 2, pp. 237-248, Sep. 2020. [146]
- T.  Luong,  H.  Pham,  and  C.  D.  Manning, ' Effective  approaches  to attention-based neural machine translation,' in Proc. Conf. Empirical Methods in Natural Language Processing , Lisbon, Portugal, 2015, pp. 1412-1421. [147]
- Z. G. Cao, H. L. Guo, W. Song, K. Z. Gao, Z. H. Chen, L. Zhang, and X. X. Zhang, ' Using reinforcement learning to minimize the probability  of  delay  occurrence  in  transportation,' IEEE Trans. Veh. Technol. , vol. 69, no. 3, pp. 2424-2436, Mar. 2020. [148]
- C. Chen, J. G. Jiang, N. Lv, and S. Y. Li, 'An intelligent path planning scheme  of  autonomous  vehicles  platoon  using  deep  reinforcement learning  on  network  edge,' IEEE Access , vol. 8, pp. 99059-99069, May 2020. [149]
- A. K. Kalakanti, S. Verma, T. Paul, and T. Yoshida, 'RL SolVeR pro: Reinforcement learning for solving vehicle routing problem,' in Proc. 1st Int. Conf. Artificial Intelligence and Data Sciences , Ipoh, Malaysia, 2019, pp. 94-99. [150]
- J.  Holler,  R.  Vuorio,  Z.  W.  Qin,  X.  C.  Tang,  Y.  Jiao,  T.  C.  Jin,  S. Singh,  C.  X.  Wang,  and  J.  P.  Ye, ' Deep  reinforcement  learning  for multi-driver vehicle dispatching and repositioning problem,' in Proc. IEEE Int. Conf. Data Mining , Beijing, China, 2019, pp. 1090-1095. [151]
- N.  Vesselinova,  R.  Steinert,  D.  F.  Perez-Ramirez,  and  M.  Boman, 'Learning  combinatorial optimization on graphs: A  survey with applications to networking,' IEEE Access ,  vol. 8,  pp. 120388-120416, Jun. 2020. [152]
- A. Subramanian, E. Uchoa, and L. S. Ochi, 'A hybrid algorithm for a class of vehicle routing problems,' Comput. Oper. Res. , vol. 40, no. 10, pp. 2519-2531, Oct. 2013. [153]
- X. Yao and Y. Liu, 'Machine learning,' in Search Methodologies ,  E. K.  Burke  and  G.  Kendall,  Eds.  Boston,  USA:  Springer,  2014,  pp. 477-517. [154]

N. Altman and M. Krzywinski, 'The curse(s) of dimensionality,' Nat. Methods , vol. 15, no. 6, pp. 399-400, Jun. 2018. [155]

M. Nickel,  K.  Murphy,  V.  Tresp,  and  E.  Gabrilovich, ' A  review  of relational machine learning for knowledge graphs,' Proc. IEEE , vol. 104, no. 1, pp. 11-33, Jan. 2016. [156]

K. T. Butler, D. W. Davies, H. Cartwright, O. Isayev, and A. Walsh, 'Machine  learning  for  molecular  and  materials  science,' Nature , vol. 559, no. 7715, pp. 547-555, Jul. 2018. [157]

- Q.  Cappart,  D.  Chetélat,  E.  B.  Khalil,  A.  Lodi,  C.  Morris,  and  P. Veličkovič, ' Combinatorial  optimization  and  reasoning  with  graph neural networks,' in Proc. Thirtieth Int. Joint Conf. Artificial Intelligence , Montreal, Canada, 2021, pp. 4348-4355. [158]
- K.  L.  Wagstaff, ' Machine  learning  that  matters,' in Proc.  29th  Int. Conf. Machine Learning , Edinburgh, UK, 2012. [159]

![Image](image_000016_ecfa82a64b1622c718c70aa7340e39f2cc01b05635853d3d19205ee9a42a5f18.png)

Bingjie  Li received  the  B.E.  degree  from  Central South University in 2019. Currently, she is working toward the M.E. degree at the School of Traffic and Transportation  Engineering,  Central  South  University. Her research interests include machine learning and path planning.

![Image](image_000017_79d2b6217529104797ed0e0c699f1550cc1da5f169e488360166811834455df5.png)

Guohua Wu received the B.S. degree in information systems and Ph.D degree in operations research from National University of Defense Technology in 2008 and  2014,  respectively.  During  2012  and  2014,  he was a visiting Ph.D student at University of Alberta, Canada. He is currently a Professor at the School of Traffic and Transportation Engineering, Central South University. His current research interests include planning and scheduling, computational intelligence  and  machine  learning.  He  has  authored more than 100 referred papers including those published in IEEE TCYB IEEE , TSMCA and IEEE TEVC .  He  serves  as  an  Associate  Editor  of Information Sciences ,  and  an  Associate  Editor  of Swarm and Evolutionary Computation Journal , an Editorial Board Member of International Journal of Bio-Inspired Computation , and Guest Editors of several journals.

![Image](image_000018_fcca4af4aeeca4699d29a5ba157db6a34b251b6d1ade7b60fca9155a2a31e59d.png)

Yongming He received  the  B.S.  degree  in  logistics engineering from Chang'an University in 2014. He is currently  pursuing  the  Ph.D.  degree  in  management science and engineering with the College of Systems Engineering,  National  University  of  Defense  Technology.  He  was  a  visiting  Ph.D.  student  with  the University of Alberta, Canada, from November 2018 to  November  2019.  His  research  interests  include operations research, artificial intelligence, intelligent decision, task scheduling, and planning.

![Image](image_000019_421d1b15b9639971c1bda6d7a6fd340fd8127c12bff4cd2ea37a56074de8b4f4.png)

Mingfeng Fan received the B.S. degree in transport equipment  and  control  engineering from  Central South University in 2019. She is currently pursuing the Ph.D. degree in traffic and transportation engineering with Central South University. Her research interests include machine learning and UAV path planning.

![Image](image_000020_fbdece37eee913b1935e76eb99804baa51e617efb5cb044e564f56c3be778c8c.png)

Witold Pedrycz (Fellow, IEEE) received the M.Sc. degree  in  computer  science,  the  Ph.D.  degree  in computer  engineering, and the D.Sci. degree in systems  science  from  the  Silesian University of Technology,  Gliwice,  Poland,  in  1977,  1980,  and 1984, respectively. He is a Professor and the Canada Research  Chair  (CRC-Computational  Intelligence) with  the  Department  of  Electrical  and  Computer Engineering, University of Alberta, Canada, and also with  the  Department  of  Electrical  and  Computer

Engineering,  Faculty  of  Engineering,  King  Abdulaziz  University,  Saudi Arabia.  He  is  also  with  the  Systems  Research  Institute,  Polish  Academy  of Sciences, Poland. In 2012, he was elected a Fellow of the Royal Society of Canada.  His  current  research  interests  include  computational  intelligence, knowledge discovery and data mining, pattern recognition, knowledge-based neural  networks,  and  software  engineering.  He  has  published  numerous papers in the above areas. He is an Editor-in-Chief of Information Sciences . He currently  serves  as  an  Associate  Editor  of IEEE Transactions on Fuzzy Systems and IEEE Transactions  on  Systems , Man and Cybernetics:  System and  is  a  Member  of  a  number  of  editorial  boards  of  other  international journals.