# Multi-Agent Reinforcement Learning for Urban Crowd Sensing with For-Hire Vehicles

## Metadata
- **Link to PDF**: [[[ding2021]_Multi-agent_reinforcement_learning_for_urban_crowd_sensing_with_for-hire_vehicles.pdf]]
- **Tags**:
  - #ML-ReinforcementLearning
  - #RL-ActorCritic
  - #RL-PolicyGradient
  - #MultiAgentRL
  - #MultiAgentRLCoordination
  - #AttentionMechanismAM
  - #GraphNeuralNetworks
  - #ExperienceReplay
  - #MDP-PartialObservations
  - #DynamicVRP
  - #VehicleRebalancing
  - #MultiAgentSystem
- **Relevant**: false  
- **Fit Score**: 2  
- **State of the Art (SoA) Concepts**:
  - Multi-agent actor-critic methods
  - Graph neural networks (specifically GAT) for routing decisions
  - Difference reward technique for cooperative learning
- **Performance Evaluation**: no  
- **Performance Evaluation Framework**: -

## Abstract
*(Extracted/Approximated from Introduction and Abstract)*  
“Recently, vehicular crowd sensing leveraging sensor-equipped for-hire vehicles (FHVs) has gained attention for city-scale data collection. This paper proposes a novel multi-agent reinforcement learning framework called GCC-MARL, which integrates graph convolution and difference rewards to align local decisions with a global objective of maximizing combined order-serving and sensing utility. The approach treats each taxi as an independent learning agent, using a centralized critic for training and distributed actors for execution. Experiments on real-world Shenzhen taxi data demonstrate improved coverage and revenue compared to baseline methods.”

## Summary
- **Problem & Objectives**  
  - Investigates real-time route selection for idle for-hire vehicles (FHVs) in a city to balance passenger-order revenue and crowd-sensing coverage.  
  - Focuses on how to distribute or “rebalance” fleets to less-served areas for better sensing without sacrificing orders.

- **Approach**  
  - Formulates the problem as a decentralized, partially observable Markov decision process (POMDP), with each taxi as an independent agent.  
  - Proposes a cooperative multi-agent reinforcement learning (MARL) framework called GCC-MARL.  
  - Uses a centralized critic and decentralized actor architecture (actor-critic).  
  - Employs graph attention networks (GAT) to process the local road network (“reachability graph”) for each agent’s routing decisions.  
  - Introduces a difference reward technique (“Wonderful Life Q-function”) to address the credit assignment problem and incentivize cooperative behavior.  
  - Encodes actions as aggregated statistics (how many vehicles choose each segment) to handle varying fleet sizes.

- **Key Techniques**  
  - **Graph Attention Networks**: Each agent’s local network neighborhood is processed to capture spatial dependencies in the road graph.  
  - **Difference Rewards**: Each agent’s advantage function filters out non-relevant changes from other agents, enhancing cooperative training.  
  - **Centralized Critic**: A single Q-network processes the global state and the aggregated action-statistics graph, enabling better policy evaluation.

- **Experiments & Findings**  
  - Conducted on large-scale real taxi data (Shenzhen, 2017), with ~553 taxis, ~50k daily orders, ~1M recorded trajectories.  
  - Demonstrates improvement in overall system utility, combining passenger orders and coverage metrics.  
  - Outperforms various baselines (rule-based, independent actor-critic, centralized critic methods without difference rewards) in both coverage (UoS) and revenue (GMV).  
  - Shows the importance of 1-hop or limited GAT layers (more than 1–2 layers can degrade results due to over-smoothing).

- **Strengths & Contributions**  
  - Integrates cooperative MARL with graph neural networks for large-scale urban routing problems.  
  - Provides an effective solution for multi-agent credit assignment in a dynamic environment.  
  - Demonstrates a working system on real data, highlighting practical feasibility.

- **Limitations & Relevance**  
  - Focuses on dynamic, uncertain passenger arrivals, which differs from deterministic VRP assumptions (order demand is not fixed or known in advance).  
  - The system aims at balancing competing objectives (order revenue vs. coverage), not purely classical routing cost minimization.  
  - Does not use standard VRP benchmarks or deterministic constraints, making it less aligned with classical VRP theory.  

- **Overall**  
  - This work is significant for multi-agent RL in ride-hailing or dynamic fleet management settings, showing how GNN-based MARL can handle city-scale route selection.  
  - For a strictly deterministic VRP perspective, however, it falls outside the standard scope due to its dynamic, stochastic problem setup.  