# DEEP Q-LEARNING FOR SAME-DAY DELIVERY WITH VEHICLES AND DRONES

## Metadata
- **Link to PDF**: [[[chen2022]_Deep_Q-learning_for_same-day_delivery_with_vehicles_and_drones.pdf]]
- **Tags**:
  - #DynamicVRP
  - #MultiVehicleRouting
  - #TimeWindowConstraints
  - #VehicleDronesCollaboration
  - #RL-ValueBased
  - #RL-Qlearning
  - #RL-DQN
  - #ExperienceReplay
  - #HeuristicOrExactBlends
  - #NeuralCombinatorialOptimization
- **Relevant**: false  
- *Reason*: The paper addresses a **dynamic VRP** with stochastic, online customer arrivals (same-day delivery requests revealed over time). It falls outside the thesis scope of purely deterministic VRPs.  
- **Fit Score**: 0  
- **State of the Art (SoA) Concepts**:
  - Q-learning for dynamic routing with heterogeneous fleets (vehicles + drones)  
  - Deep neural network approximations of action-value functions  
  - Real-time/online routing decisions under time window constraints  
- **Performance Evaluation**: no  
- **Performance Evaluation Framework**: "-"  

## Abstract
“In this paper, we consider same-day delivery with vehicles and drones. Customers make delivery requests over the course of the day, and the dispatcher dynamically dispatches vehicles and drones to deliver the goods to customers before their delivery deadline. Vehicles can deliver multiple packages in one route but travel relatively slowly due to the urban traffic. Drones travel faster, but they have limited capacity and require charging or battery swaps. To exploit the different strengths of the fleets, we propose a deep Q-learning approach. Our method learns the value of assigning a new customer to either drones or vehicles as well as the option to not offer service at all. In a systematic computational analysis, we show the superiority of our policy compared to benchmark policies and the effectiveness of our deep Q-learning approach. We also show that our policy can maintain effectiveness when the fleet size changes moderately. Experiments on data drawn from varied spatial/temporal distributions demonstrate that our trained policies can cope with changes in the input data.”

## Summary
- **Problem & Motivation**  
  - The paper addresses a **Same-Day Delivery Problem** (SDD) in which customers place orders throughout the day with tight delivery deadlines.  
  - A fleet of **vehicles** (capacity effectively unlimited, but slower speed) and a fleet of **drones** (faster speed but limited capacity) operate from a central depot.  
  - The requests are **revealed dynamically**, and the dispatcher must decide immediately whether to accept a request and, if so, assign a drone or vehicle.  
  - The objective is to maximize the expected number of serviced customers within their deadlines.

- **ML Techniques Used**  
  - The authors develop a **deep Q-learning** approach, a **value-based RL** method, to decide for each new request:  
    1. Whether to accept or reject it.  
    2. If accepted, whether a drone or a vehicle should serve it.  
  - A **deep neural network** approximates the action-value function (Q function), with experience replay used during training.  
  - Features include time of request, vehicle/drone availability, and insertion costs (for vehicles) or distance metrics (for drones).  

- **Methodology**  
  - The paper formalizes the decision process as a Markov Decision Process (MDP) with states representing time, fleet availability, and planned routes.  
  - The Q-learning module is fed by a heuristic that checks feasibility of inserting a request into the existing vehicle schedule or drone schedule.  
    - If it is feasible with vehicles, a best insertion position is identified to minimize route prolongation.  
    - If drones are feasible, they are assigned in first-in-first-out manner, respecting battery constraints and recharging times.  
  - The neural network is trained offline on simulated daily request sequences and then used online to make immediate (real-time) assignment decisions.  
  - The authors compare their approach to benchmark policies from prior research (e.g., parametric threshold-based assignment rules) and measure improvements in total served requests.

- **Key Results & Findings**  
  - The proposed deep Q-learning solution outperforms baseline threshold-based policies across a range of fleet sizes (e.g., 2–4 vehicles and 5–15 drones).  
  - Notably, the largest gains occur when resources are more constrained (fewer drones/vehicles).  
  - The approach remains moderately robust if fleet parameters (like number of vehicles) change from those used during training; small divergences from training conditions do not drastically reduce performance.  
  - Testing on spatially and temporally varied customer-request distributions shows the policy can still adapt well, though it is somewhat sensitive to large changes in demand patterns.

- **Strengths, Weaknesses, Limitations**  
  - **Strengths**:  
    - Demonstrates a Q-learning architecture for a **heterogeneous fleet** under immediate response constraints.  
    - Achieves faster, near real-time decisions after offline training.  
    - Incorporates acceptance/rejection decisions and a specialized feasibility-heuristic to constrain the action space.  
  - **Weaknesses/Limitations**:  
    - Primarily designed for **online/dynamic** contexts; not applicable to pure deterministic VRP with all data known in advance.  
    - Relies on carefully selected features; generalization might degrade if real-world constraints differ drastically from training.  
    - No reference to standard VRP benchmarking sets; problem data is custom, focusing on same-day and dynamic request arrivals.

- **Relevance to Thesis**  
  - The method is innovative for dynamic, online VRP scenarios with heterogeneous fleets.  
  - However, the core problem is **non-deterministic** (requests revealed over time, uncertain future) and thus outside the scope of purely deterministic VRPs.  
  - Despite this, it underscores how **deep RL** can handle acceptance and assignment decisions in time-critical contexts.  