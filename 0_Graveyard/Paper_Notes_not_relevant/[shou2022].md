# Multi-Agent Reinforcement Learning for Markov Routing Games: A New Modeling Paradigm For Dynamic Traffic Assignment

## Metadata
- **Link to PDF**: [[[shou2022]_Multi-agent_reinforcement_learning_for_Markov_routing_games-A_new_modeling_paradigm_for_dynamic_traffic_assignment.pdf]]
- **Tags**:
  - #ML-ReinforcementLearning
  - #RL-Qlearning
  - #RL-DQN
  - #RL-ValueBased
  - #MeanFieldRL
  - #MultiAgentRL
  - #MultiAgentSystem
  - #GameTheory
  - #MDP
  - #RealWorldData
- **Relevant**: false
- **Fit Score**: 2  
- **State of the Art (SoA) Concepts**:
  - Markov Routing Games  
  - Multi-Agent Reinforcement Learning with Mean-Field Approximation  
  - Dynamic Traffic Assignment models (PQ, SQ, LTM, CTM)  
- **Performance Evaluation**: no
- **Performance Evaluation Framework**: -

## Abstract
This paper aims to develop a paradigm that models the learning behavior of intelligent agents (including but not limited to autonomous vehicles, connected and automated vehicles, or human-driven vehicles with intelligent navigation systems where human drivers follow the navigation instructions completely) with a utility-optimizing goal and the system's equilibrating processes in a routing game among atomic selfish agents. Such a paradigm can assist policymakers in devising optimal operational and planning countermeasures under both normal and abnormal circumstances. To this end, we develop a Markov routing game (MRG) in which each agent learns and updates her own en-route path choice policy while interacting with others in transportation networks. To efficiently solve MRG, we formulate it as multiagent reinforcement learning (MARL) and devise a mean field multi-agent deep Q learning (MF-MADQL) approach that captures the competition among agents. The linkage between the classical DUE paradigm and our proposed Markov routing game (MRG) is discussed. We show that the routing behavior of intelligent agents is shown to converge to the classical notion of predictive dynamic user equilibrium (DUE) when traffic environments are simulated using dynamic loading models (DNL). In other words, the MRG depicts DUEs assuming perfect information and deterministic environments propagated by DNL models. Four examples are solved to illustrate the algorithm efficiency and consistency between DUE and the MRG equilibrium, on a simple network without and with spillback, the Ortuzar Willumsen (OW) Network, and a real-world network near Columbia University's campus in Manhattan of New York City.

## Summary
- **Context & Motivation**  
  - Investigates a **dynamic traffic assignment** problem in which multiple agents (vehicles) make en-route routing choices on a road network.  
  - Proposes a **Markov Routing Game (MRG)** framework where each agent learns sequentially to minimize individual travel cost while competing with other agents for road capacity.  

- **Key Contributions**  
  1. **MRG Formulation**: Introduces a multi-agent game-theoretic paradigm modeling each traveler as a Markov decision-maker.  
  2. **Mean-Field Multi-Agent Reinforcement Learning**: Develops a Mean Field Multi-Agent Deep Q-Learning (MF-MADQL) algorithm to address high-dimensional state and action spaces.  
  3. **Link to Classical DTA**: Shows that solutions can coincide with the **Dynamic User Equilibrium (DUE)** notion when traffic is deterministic and each agent has perfect information.  

- **Method & Approach**  
  - Each agent’s routing decisions are updated via **value-based RL** (deep Q-learning) where the environment’s transitions are governed by known or simulated dynamic network loading (DNL) models.  
  - A novel “mean action” captures the local congestion effect by summarizing the flow on the chosen link without tracking every other agent individually.  
  - Multiple DNL models (e.g., LTM, PQ, SQ, CTM) are used to simulate realistic traffic dynamics and queue propagation, including spillback effects.  

- **Experiments & Findings**  
  - Demonstrates **consistency** between the learned routing policies and the classical DUE solutions on several example networks:
    1. Simple networks (with and without spillback)
    2. An expanded OW (Ortuzar-Willumsen) network
    3. A real-world Manhattan subnetwork (69 nodes, 166 links) simulated in SUMO.  
  - Shows the multi-agent Q-learning method can approximate user-equilibrium routing choices given unknown or partial traffic dynamics.  

- **Strengths**  
  - Provides a comprehensive multi-agent reinforcement learning framework that ties classical traffic equilibrium concepts with advanced RL.  
  - Demonstrates scalability through mean-field approximations, avoiding an exponential blow-up in multi-agent state/action spaces.  
  - Applies to large-scale networks and includes a real test network.  

- **Limitations & Gaps**  
  - Focuses specifically on **traffic assignment** among drivers, not on distribution/logistics VRP tasks or capacity constraints at the vehicle level.  
  - Requires repeated training in a simulation environment and thus might be computationally demanding relative to classical equilibrium-solving methods.  
  - Does not address stochastic or uncertain demand; the environment is treated as deterministic or known from data.  

- **Relevance to Deterministic VRP**  
  - Although it applies advanced RL in a routing context, it does **not** tackle the vehicle routing problem with capacity constraints, time windows, or fleet optimization.  
  - Instead, it models **selfish route choice** in a traffic network, focusing on dynamic user equilibrium rather than VRP cost structures.  

- **Conclusion & Outlook**  
  - Proposes a robust multi-agent RL approach for **dynamic routing** in congested traffic networks.  
  - Finds that the RL-based solution converges to user-equilibrium-like patterns.  
  - Potentially extensible to other forms of dynamic routing or driver behavior modeling, but not targeted at the VRP domain where vehicle-level constraints and scheduling are central.  
